\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\pr}{\mathbb{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Machine Learning}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}

\rus{Дмитрий Игоревич Игнатов}

Descision tree as classification tools.

Is a tree with nodes, labelled by features, conditions or rules. The leaves are the classes.

The learning sample may be unbalanced.

There is no point in identical conditions within the same branch: at nodes $u,v$ in the tree-order.

Main definitions.
The connection of decision tree with boolean functions.

Can a tree be represented in a disjunctive normal form?

Conjuncts are identified with the root-leaf paths, with a \emph{True} leaf.

Decision tree construction.
\begin{itemize}
	\item Pick a feature $Q$.
	\item For each value $i$ of $Q$.
	\begin{itemize}
		\item recursively construct the decision tree base of the learning sub-sample with $Q=i$.
	\end{itemize}
\end{itemize}

The question is how the next attribute should be chosen.

Suppose the we need to define a measure $\nu$ of uncertainty of events:\begin{itemize}
	\item If $A$ is certain to occur ($\pr(A)=1$), then $\nu(A)=0$;
	\item If $A$ is more likely than $B$, then is reasonable to expect the uncertainty of $B$ be higher than that of $A$. Thus $\nu$ should be anti-monotonous;
	\item if $A\perp B$, then the uncertainty of one event is unaffected by the uncertainty of the other. Thus it is reasonable to expect that their simultaneous uncertainty must be pooled: $\nu(A\cap B)= \nu(A)+\nu(B)$.
\end{itemize}

It is convenient to define $\nu$ as a composition of some $h$ with $\pr$: $\nu = h\circ \pr$.

Thus we arrive at a concept of entropy: if $Q<<\mu$ and $Q=\int q d\mu$, then $H = \int \log \frac{1}{q} q d\mu = - \int \log q dQ$.

In the discrete case the integral degenerates into $H = \sum_k \frac{1}{p_k} p_k$.

For a binary variable:
\[H(\theta) = - \theta \log \theta - (1-\theta) \log (1-\theta)\]
The entropy as a function of $\theta$ resembles a bell with zeros at $0,1$. Use l'H\^opital's rule.

\textbf{Idea}: Select the successive feature $Q$ to be as informative as possible.


Suppose $A$ elemnts with a property $S$ are classified by a feature $Q$ with $q$ distinct values then the \textbf{information gain} is
\[\text{Gain} = H(A,S) = \sum_{k=1}^q \frac{\abs{A_k}}{\abs{A}} H(A_k,S)\]
where $A_k$ is the partition by $Q$: $A_k = \obj{\induc{\omega \in A} Q(\omega)=k }$.

The impurity of a class might be defined as
\[\dot{p} = \frac{p_+}{p_++p_-} = \frac{N_+}{N_+ + N_-}\]
in the learning sample with $+$ and $-$ examples.

the impurity measure is expected to be symmetric with respect to the classes $+$ and $-$.

Consider using $p_+(1-p_-)$ and $p_-(1-p_+)$ pooled together for symmetry.

Ross Queenlain ID3 algorithm (C4.5).
\begin{itemize}
	\item Create a root;
	\item If $S$ hold for all $A$, then set the node to $1$ and leave;
	\item If $S$ fails to hold for all $A$, then set the node to $0$ and leave;
	\item If $\mathcal{Q} = \emptyset$ then \begin{itemize}
		\item if $S$ it true for a half or more examples in $A$, then set the root to $1$ and leave;
		\item if $S$ it true for a half or more examples in $A$, then set the root to $1$ and leave;
	\end{itemize}
	\item IF $\mathcal{Q}\neq\emptyset$ then choose feature $Q\in \mathcal{Q}$ with the highest information gain $\text{Gain}(A, Q)$.
	\item ... MISSING LOC;
\end{itemize}

Gini inequality coefficient.
Gini index

A feature with distinct value for each example has the greatest information gain. Indeed, the entropy of a set consisting of an individual example with respect to the target variable is zero since $0\log 0 = 1 \log 1 = 0$.
\[\text{SplitInfo}(A,S) = H(A,S)\]

Branch trimming:
\[\text{GainRatio}(A,S) = \frac{\text{Gain}(A,S)}{\text{SplitInfo}(A,S)}\]


% Если зенит просто как случайная величина выигрывает/проигрывает с вероятностью $p$ то можно и построить такой ```случайный классификатор''.

One-node (one-rule) degenerate decision trees are called \eng{decision stumps}.

\begin{align*}
	\text{Gini}(D,T) &= 1 - \sum_{k=1}^T \brac{\frac{D_k}{D}}^2 \\
	H(D,T) &= - \sum_{k=1}^T \frac{D_k}{D} \log \frac{D_k}{D} \\
	\text{Gain}(D,T,A) &= H(D,T) - \sum_{k=1}^T \frac{D_k}{D} H(D_k,T)
\end{align*}



% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Associative rules mining.

A ocntext is an object-attribute table: $\brac{G,M,I}$, with $I\subseteq G\times M$.
An Associative rule is a a pair $A\to B$ with $A,B\subseteq M$ (and sometimes $A\cap B = \emptyset$).

\subsection{A brief history of FCA} % (fold)
\label{sub:a_brief_history_of_fca}

\begin{itemize}
	\item Aristotle. An extnet (the objects) and an intent (the set of common attributes);
	\item Galois. I his reserach he used the what now is know as a Galois connection: a pair of maps $\psi:P\to Q$ and $\phi:Q\to P$ between ordered sets $(P,\leq)$ and $(Q,\leq)$ with the \textbf{GALOIS} property: for any $a\in P$ and $b\in Q$ it
	\[a\leq \phi(b) \Leftrightarrow b\leq \psi(a)\]
	\item Lattice structure; Birkhgoff proposed to compare incomparable elements of a poset $(P,\leq)$ by using the least upper and the greatest lower bounds $a\vee b$ and $a\wedge b$;
\end{itemize}

% subsection a_brief_history_of_fca (end)

\subsection{A recap of the FCA} % (fold)
\label{sub:a_recap_of_the_fca}

The intent of $A\subseteq G$ is the set of attributes
\[A' \defn \obj{\induc{m\in M}\,\forall g\in A (g,m)\in I} = \bigcap_{a\in A} a'\]
Similarly the extent of $B\subseteq M$ is the collection of objects
\[B' \defn \obj{\induc{g\in G}\,\forall m\in B (g,m)\in I} = \bigcap_{b\in B} b'\]

Also the ${(\cdot)}'$ operator has the following property:
\[\brac{\bigcup_{j\in J}A_j}' = \bigcap_{j\in J}A_j'\]

A formal concept is a pair $(A,B)$ such that $A'=B$ and $B'=A$ -- stability. 

% subsection a_recap_of_the_fca (end)

An associative rule is an intent (a formal intent, itemset). The support of an associative rule $A\to B$ is
\[\text{supp}(A\to B) = \frac{\abs{\brac{A\cup B}'}}{\abs{G}}\]
The confidence of a rule is 
\[\text{conf}(A\to B) = \frac{\abs{\brac{A\cup B}'}}{\abs{A'}}\]

The setting of the problem
Find all associative rules of the context with support and confidence higher than some pre-set threshold value.

An implication of the context is an associative rule $A\to B$ with $A'\subseteq B'$, which implies that $\text{conf}(A\to B) = 1$.

The problem is that $A\to B$ is an implication even if $A'=B'=\emptyset$.
For example $bc\to ad$ in the context $\abs{\begin{matrix}\times & \times & \cdot & \cdot\\ \cdot & \cdot & \times & \times\end{matrix}}$

\subsection{The Apriori algorithm} % (fold)
\label{sub:the_apriori_algorithm}
The algorithm is based on the following property of support:
\noindent\textbf{anti-monotonicity}\hfill\\
If $A,B\subseteq M$ and $A\subseteq B$ then $\text{supp}(B) \leq \text{supp}(A)$

\textbf{Apriori}
\begin{description}
	\item[input] A context $\brac{G,M,I}$, the minimum confidence $\alpha$ and support $\beta$ thresholds;
	\item[output] All frequent intents (itemsets);
	\item $C_i\leftarrow \obj{\text{1-itemsets}}$;
	\item[while] $C_i\neq \emptyset$;\begin{itemize}
		\item SupportCount($C_i$);
		\item $F_i \leftarrow \obj{\induc{f\in C_i}\,\text{supp}(f)\geq \beta}$;
		\item $C_{i+1}\leftarrow \text{AprioriGen}\brac{F_i}$.
	\end{itemize}
	\item[Finalization] $I_F \leftarrow \bigcup_i F_i$;
\end{description}

Where the service procedure is given by:
\textbf{AprioriGen}
\begin{description}
	\item[input] $F$ -- frequent itemsets of size $i$;
	\item[output] $C_{i+1}$ -- the potential candidate itemsets;
	\item[SQL-step]\hfill\\
		insert into $C_{i+1}$ select $\brac{p_k}_{k=1}^i$, $q_i$ from $F_ip$, $F_iq$ where $\brac{p_k}_{k=1}^{i-1} = \brac{q_k}_{k=1}^{i-1}$ and $p_i\leq q_i$;
	\item[for each] $c\in C_{i+1}$;\begin{description}
		\item $S\leftarrow (i-1)\text{-element subsets of }c$;
		\item[for each] $s\in S$;
		\item if $s\notin F_i$, then $C_{i+1}\leftarrow C_{i+1}\setminus \obj{s}$.
	\end{description}
\end{description}

Let $F$ be a frequent itemset. A rule is satisfactory if 
\[\text{conf}(f\to F\setminus \obj{f}) = \frac{\text{supp}(F)}{\text{supp}(f)} \geq \alpha\]

The confidence of $f\to F\setminus f$ is minimal when $\text{supp}(f)$ is maximal.

The the confidence is minimal when the Antecedent has only one attribute. Supersets of this attribute have lesser support and thus higher confidence.

The associative rules are extracted a frequent itemset $F$ recursively:
\begin{itemize}
	\item starting from $1$-itemset of $F$ antecedent which satisfies the $\alpha$ and $\beta$ thresholds;
	\item check its supersets in $F$; If the confidence requirement is met, then the association rule $f\to F\setminus f$ is kept.
\end{itemize}

In data mining $A$ is closed if $\text{supp}(A)\leq \alpha$ and there exists no $A\subseteq B$ with $\text{supp}(B)=\text{supp}(A)$.



% subsection the_apriori_algorithm (end)

% Associative rule document mining.

% section lecture_5 (end)


\end{document}