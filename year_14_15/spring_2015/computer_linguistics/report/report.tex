\documentclass{beamer}
\usepackage[utf8x]{inputenc}
\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}
\selectlanguage{russian}
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

%\usepackage{fullpage}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, xfrac}
\usepackage{mathtools}

\usepackage{tikz}
\usetikzlibrary{shapes}
\colorlet{light blue}{blue!50}

\newcommand{\obj}[1]{\left\{ #1 \right \}}
\newcommand{\clo}[1]{\left [ #1 \right ]}
\newcommand{\clop}[1]{\left [ #1 \right )}
\newcommand{\ploc}[1]{\left ( #1 \right ]}

\newcommand{\brac}[1]{\left ( #1 \right )}
\newcommand{\crab}[1]{\left ] #1 \right [}
\newcommand{\induc}[1]{\left . #1 \right \vert}
\newcommand{\abs}[1]{\left | #1 \right |}
\newcommand{\nrm}[1]{\left\| #1 \right \|}
\newcommand{\brkt}[1]{\left\langle #1 \right\rangle}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Real}{\mathbb{R}}

\selectlanguage{russian}

\begin{document}
\selectlanguage{russian}
\title{Байесовские модели в компьютерной лингвистике}
\author{Назаров Иван}
\date

%% Title slide
\frame{\titlepage}

%% Table of Contents
% \begin{frame}
%   \frametitle{\rus{Содержание}}
%   \tableofcontents
% \end{frame}

\selectlanguage{russian}
\section{\rus{Байесовская вероятность}} % (fold)
\label{sec:probability}
\begin{frame}
  \begin{block}

    Закон больших чисел: предел относительной частоты явления равен вероятности
  \end{block}
  \begin{block}

    Требуется возможность повторять наблюдения бесконечно
  \end{block}

  \begin{block}{Принципиально неповторяемые события}

    Вероятность -- характеристика веры в наступление события \eng{degree of belief}
    Априорная информация $\oplus$ наблюдения $\Leftarrow$ теорема Байеса $\implies$ апостериорная вероятность
    \[\mathbb{P}\brac{\induc{\theta} \Dcal} \propto L\brac{\induc{\Dcal}\theta} \pi\brac{\theta}\]

  \end{block}
\end{frame}

\begin{frame}\selectlanguage{russian}\frametitle{Типы задач}
  Всегда постулируется вероятностная модель
  \begin{block}{Обратные}

    по наблюдениям определить вероятностные свойства скрытых состояний (классификация)
  \end{block}
  \begin{block}{Прямые}

    определить вероятностные характеристики наблюдений (прогнозирование)
  \end{block}
\end{frame}

\begin{frame}\selectlanguage{russian}\frametitle{Байесовские задачи в компьютерной лингвистике}

  \begin{block}

  \begin{itemize}
    \item Классификация текстов, тэгирование слов частями речи;
    \item Машинный перевод, сопоставление текстов;
    \item Разрешение омонимии;
    \item Извлечение информации.
  \end{itemize}
  \end{block}
\end{frame}

% section probability (end)


\section{Байесовские сети} % (fold)
\label{sec:bayesian_networks}

\begin{frame}
  \begin{block}
  
    Вероятностная модель, основанная на направленном ацикличном графе причинно-следственных связей.
  \end{block}
  \begin{block}{Описание}

    Набор случайных величин $\brac{X_v}_{v\in V}$ и ориентированный граф $G=(V,E)$, узлам которого соответствуют наблюдаемые или скрытые переменные или состояния. Рёбра -- отношения причинности или влияния.

    При этом требуется возможность следующей факторизации совместной плотности вероятности
    \[p\brac{\brac{X_v}_{v\in V}} = \prod_{v\in V} p\brac{\induc{X_v} \brac{X_w}_{w\in\pi^+(v)}}\]
    где $\pi^+(v)$ -- множество непосредственных родителей $v$ в $G$.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Локальная Марковость}

    Для любого $v\in V$ и любого $S\subseteq V\setminus\brac{\sigma(v)\cup\obj{v}}$ выполняется
    \[p\brac{\induc{X_v}\brac{X_w}_{w\in \pi^+(v)},\,\brac{X_k}_{k\in S} } = p\brac{\induc{X_v}\brac{X_w}_{w\in \pi^+(v)} }\]
    где множество потомков узла $v\in V$ равно
    \[\sigma(v) = \obj{\induc{w\in V} \exists\,\text{path}\,\pi\subseteq E,\, v\overset{\pi}{\leadsto}w}\]
    
  \end{block}

\end{frame}

\begin{frame}{$n$-граммная модель текста}
  \begin{block}{Основная идея}

     чем дальше вхождения единиц текста отстоят друг от друга, тем они более независимы
  \end{block}

  \begin{block}

    Пусть $\brac{w_i}_{i=0}^{l+1}$ -- базовые единицы текста в порядке их следования, $w_0=w_{l+1}=\$$ -- символы начала и конца текста.

    Совокупная вероятность $w_k$ единицы определяется $n-1$ предыдущей единицей и не зависит от ``префикса'' текста $\brac{w_i}_{i=0}^{k-n}$:
    \begin{align*}
      p\brac{\induc{w}M} &= p\brac{\induc{w_1}\$;M} \cdot p\brac{\induc{w_2}\$,w_1;M} \cdot \ldots \\
        &\times p\brac{\induc{w_n},w_1,\ldots,w_{n-1};M} \\
        &\times \prod_{i=n+1}^l p\brac{\induc{w_i}w_{i-n+1}, w_{i-n+2}, \ldots, w_{i-1};M} \\ 
        &\times p\brac{\induc{\$}w_{l-n+2}, \ldots, w_l;M}
    \end{align*}
  \end{block}
\end{frame}

\begin{frame}{$n$-граммная модель текста}
  \begin{block}{Параметры}

    Вероятности
    \[p\brac{\induc{x_n}x_1, x_2, \ldots, x_{n-1}}\] 
    где $x_i\in \obj{\$}\cup \Sigma$ -- алфавит единиц текста, расширенный маркером конца строки.
    Количество параметров модели $\propto \abs{\Sigma}^n$.
  \end{block}
\end{frame}

% section bayesian_networks (end)

\section{Иллюстрация байесовского подхода} % (fold)
\label{sec:bayes_demonstration}
\begin{frame}\selectlanguage{russian}\frametitle{Пример байесовского подхода}
  \begin{block}

    В качестве примера применения байесовского подхода, рассмотрим подробно ``наивный'' классификатор текстов: основное допущение в независимости появления единиц внутри текста.
  \end{block}
  \begin{block}{Текст}

    Текст $d$ -- упорядоченный набор базовых единиц из пространства $W$.
    Например, $n$-граммы символов или слов.
  \end{block}
  \begin{block}{Термины}

    Пусть $h:W\to \Tcal$ сопоставляет каждой единице некоторый ``термин'' из пространства $\Tcal$. Например, хэш от $n$-граммы или лемма от словоформы.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Корпус}

    Пусть $\Dcal$ -- корпус в обучающей выборке, тексты которого разбиты на классы $\brac{D_c}_{c\in K}$, и $T$ -- термины во всех текстах:
    \[T = \bigcup_{c\in K}T_c;\,T_c = \bigcup_{d\in D_c} T_d\]
  \end{block}

  \begin{block}{Классификация}

    Задача определить класс $c\in K$ для текста $d$ по частотным характеристикам его терминов $T_d = \obj{\induc{h(w)}\,w\in d}$.
    Максимизируется апостериорная вероятность 
   \[\hat{c} = \underset{c\in K}{\text{argmax}}\,p\brac{\induc{c}\,d} = \underset{c\in K}{\text{argmax}}\,\ln p\brac{\induc{d}\,c} + \ln \pi\brac{c} \]
    Фактически это выбор наиправдоподобнейшей модели текста $d$ -- (\eng{\emph{model selection}}).
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Модель текста}

    Предполагается независимость появления терминов в тексте (с точностью до константы):
    \[p\brac{\induc{d}\,c} \propto \prod_{t\in \Tcal}\theta_{ct}^{n_{dt}}\]
    где $\brac{\theta_{ct}}_{t\in \Tcal}$ -- вероятностные характеристики текстов класса $c\in K$, ``модель'' текста согласно этому классу, $n_{dt}$ -- количество вхождений термина $t$ в текст $d$
    \[n_{dt} = \abs{\obj{\induc{w\in d} h(w) = t}}\]
  \end{block}

\end{frame}

\begin{frame}
  \begin{block}{Вероятность класса}

    Введём набор распределений $\brac{\pi_c}_{c\in K}$ вероятностных характеристик терминов в классах.
    Согласно теореме Байеса, на основании наблюдений текста $d$, апостериорная плотность распределения вектора вероятностных характеристик $\theta_c = \brac{\theta_{ct}}_{t\in \Tcal}$ класса $c\in K$ равна:
      \[p\brac{\induc{\theta_c}\,d} \propto p\brac{\induc{d}\,\theta_c}\pi_c(\theta_c)\]
    где $\pi_c$ -- априорная плотность распределения $\theta_c$.
  \end{block}
  \begin{block}{Сопряжённые распределения}

    Для анализа, обучения и классификации удобно найти \emph{собственное} семейство многомерных распределений оператора преобразования априорного в апостериорное распределение.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{\eng{Noninformative prior}}

    В силу скудности априорных знаний, нет оснований приписывать \textbf{особенную} вероятностную модель $\theta = \brac{\theta_{ct}}_{t\in T}$ классу $c$.
    В качестве такого неинформативного априорного распределения $\pi_0=\pi(\theta)$ годится равномерное распределение на $S_{\#T-1}^\circ$ открытом $\#T-1$-мерном симплексе в $\clo{0,1}^{\#T}$.
    \begin{align*}
      p\brac{\induc{\theta_c}\,d} & \propto p\brac{\induc{d}\,\theta_c}\pi_0(\theta_c) \\
        & \propto \prod_{t\in T}\theta_{ct}^{n_{dt}} \cdot \frac{1}{\text{const}_{\#T}}
    \end{align*}
    Случай $\#T=2$ сильно напоминает Бета распределение, которое по сути определено на одномерном симплексе в $\clo{0,1}^2$.
    Нужно многомерное обобщение Бета распределения на $S_{\#T-1}^\circ$.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Распределение Дирихле}

    Случайный вектор $x=\brac{X_k}_{k=1}^n$ имеет распределение Дирихле если его совместная плотность равна
    \[f\brac{x;\alpha} = \frac{\Gamma\brac{\sum_{k=1}^n\alpha_k}}{\prod_{k=1}^n \Gamma(\alpha_k)} \prod_{k=1}^n x_k^{\alpha_k-1} 1_{x\in S_{n-1}^\circ}\]
    где $\alpha = \brac{\alpha_k}_{k=1}^n$ -- вектор параметров концентрации, и $S_{n-1}^\circ$ -- открытый $n-1$ мерный симплекс в $\clo{0,1}^n$.
  \end{block}
  \begin{block}{Сопряжённость}

    Если $\pi_c(\theta_c) = \text{Dir}\brac{\theta_c;\alpha_c}$, $\alpha_c$ -- гиперпараметр класса $c$, то при поступлении текста $d$ из $c$ выполняется
    \[\text{Dir}\brac{\theta_c; \alpha_c+n_d} \propto \prod_{t\in T}\theta_{ct}^{n_{dt}} \text{Dir}\brac{\theta_c; \alpha_c}\]
    Апостериорные вероятностные характеристики класса $c$ задаются распределением Дирихле с параметром $\alpha_c + n_d$.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Правдоподобие термина в классе}

    Вероятность появления термина $t$ в классе $c$ с гиперпараметром $\brac{\alpha_t}_{t\in T}$ равна
    \begin{align*}
      p\brac{\induc{t} c} &= \frac{\Gamma\brac{\sum_{s\in T}\alpha_s}}{\prod_{s\in T} \Gamma(\alpha_s)} \int\limits_{S_{n-1}^\circ} \theta_t \prod_{s\in T} \theta_s^{\alpha_s-1} d\theta \\
      &= \frac{\Gamma\brac{\sum_{s\in T}\alpha_s}}{\Gamma(\alpha_t) \prod_{s\neq t} \Gamma(\alpha_s)} \frac{\Gamma(\alpha_t+1) \prod_{s\neq t} \Gamma(\alpha_s)}{\Gamma\brac{\sum_{s\in T}\alpha_s + 1}}\\
      &= \frac{\alpha_t}{\sum_{s\in T}\alpha_s}\\
    \end{align*}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Обучение}

    Обучение классификатора сводится к определению вероятностных характеристик терминов в текстах классов.
    Если наблюдается набор текстов $D_c$, то $\alpha_c \to \alpha_c + \sum_{d\in D_c}n_d$.
    Поскольку нет оснований полагать неравномерность распределения вероятностных характеристик, то $\alpha_c = \mathbf{1}$ для каждого класса $c\in K$.
  \end{block}
  \begin{block}{Правдоподобие термина}

    Пусть $f_{ct} = \sum_{d\in D_c} n_{dt}$ -- количество вхождений термина $t$ в класс $c$.
    Тогда апостериорная вероятность термина $t$ в классе $c$ равна
    \[p\brac{\induc{t}c} = \frac{ f_{ct} + 1}{ \sum_{s\in T}f_{cs} + \abs{T}}\]
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Априорная вероятность класса}

    В корпусе $\Dcal$ текстов, и каждый принадлежит некоторому классу $c\in K$. Если корпус репрезентативен, то вероятность $\mathbb{P}(c)$ что текст принадлежит классу $c$, точно приближается частотностью
    \[\eta_c = \frac{\abs{\Dcal_c}}{\abs{\Dcal}}\]
    Очень важна сбалансированность и репрезентативность корпуса текстов, поскольку в противном случае повышается риска ``застревания'' классификатора в некотором классе.
  \end{block}
  \begin{block}{Классификация}

    Итак, к какому классу следует отнести текст?
    \[\hat{c} = \underset{c\in K}{\text{argmax}}\, \sum_{t\in T} \ln \frac{f_{ct} + 1}{\sum_{s\in T}f_{cs} + \abs{T}} + \ln\frac{\abs{\Dcal_c}}{\abs{\Dcal}}\]
  \end{block}
\end{frame}

% section bayes_demonstration (end)

\begin{frame}{Процесс Дирихле}
  \begin{block}

    распределение на функциях конечных дискретных распределений.
    \begin{description}
      \item[нет ограничений на сложность словаря]\hfill\\
        закладывается ненулевая вероятность появления неизвестного ранее термина
    \end{description}
  \end{block}
  \begin{block}{Определение}

    Случайное распределение $G$ является реализацией процесса $\text{DP}\brac{\alpha,H}$, где $H$ -- базовое распределение на пространстве параметров $\Theta$, и $\alpha$ -- параметр концентрации, если для любого измеримого конечного разбиения $\brac{A_t}_{t\in T}$ пространства $\Theta$ выполнено:
    \[\brac{G(A_t)}_{t\in T}\sim \text{Dir}\brac{ \brac{\alpha H(A_t)}_{t\in T} }\] 

  \end{block}
\end{frame}

\begin{frame}{$\text{DP}\brac{\alpha,H}$}
  \begin{block}

    Распределение $H$ является ``средним'' распределением, ``вокруг'' которого порождаются случайные распределения.
    Распределения почти наверное дискретны, однако концентрация вероятности в атомах снижается с ростом $\alpha$.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Примеры процесса}

    \begin{itemize}
      \item разбиение отрезка $\clo{0,1}$: раскалываем отрезок $I_{n-1}$ в пропорции $\beta_n$, присваиваем $\eta_n$ длину произвольного ``осколка'', обозначаем оставшийся $I_n$. Для некоторой реализации $\brac{\theta_k^*}_{k\geq 1}\sim H$ \[G = \sum_{k\geq1} \eta_k \delta_{\theta_k^*}\]
      \item наполнение корзины цветными шарами: \begin{itemize}
        \item случайным цветом $\theta_1\sim H$ помечаем шар и кладём его в корзину;
        \item на шаге $n+1$ с вероятностью $\frac{\alpha}{\alpha+n}$ вытягиваем цвет $\theta_{n+1}\sim H$, помечаем им шар и кладём его в корзину;
        \item с вероятностью $\frac{n}{\alpha+n}$ выбираем шар из корзины, его цветом помечаем новый шар и кладём оба в корзину.
      \end{itemize} 
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Униграммы [\eng{Goldwater et al.; 2008}]}
  \begin{block}{Модель}

    независимое порождение строк, состоящих из независимых вхождений слов или конца строки. 
  \end{block}

  \begin{block}

    Слово вытягивается по следующему закону ($\text{DP}\brac{\alpha_0,P_0}$)
    \begin{description}
      \item[новое слово]\hfill\\
        новое слово $w$ вытягивается из $P_0$;
        \item[иначе]\hfill\\
        слово $w$ вытягивается из накопленного лексикона согласно его частотным характеристикам.
    \end{description}    
  \end{block}
\end{frame}
\begin{frame}{Униграммы}
  \begin{block}

    Условная вероятность слова $l$ на $i$ месте в предложении равна:
    \[P\brac{\induc{w_i = t}w_{i-1},\ldots,w_1} = \frac{n_t}{i-1+\alpha_0} + \frac{\alpha_0 P_0(w_i = t)}{i-1+\alpha_0}\]
    Базовое распределение $P_0$ -- униграммная модель на фонемах:
    \[P_0\brac{\induc{w_i = x_1\ldots x_M}} = p_\$ \brac{1-p_\$}^{M-1} \prod_{j=1}^M P(x_j)\]
    где $P(x_j)$ -- равномерное распределение на фонемах, $p_\$$ -- вероятность конца слова.
  \end{block}
\end{frame}

\begin{frame}{Источники}
  \begin{block}

    \begin{itemize}
      \item Автоматическая обработка текстов на естественном языке и компьютерная лингвистика : учеб. пособие / Большакова~Е.И., Клышинский~Э.С., Ландэ~Д.В., Носков~А.А., Пескова~О.В., Ягунова~Е.В. — М.: МИЭМ, 2011. — 272~с. стр.~176
      \item \eng{Goldwater, Sharon, Thomas L. Griffiths, and Mark Johnson. 2008. A Bayesian framework for word segmentation: Exploring the effects of context. In submission. Available at \url{http://homepages.inf.ed.ac.uk/sgwater/papers/journalwordseg-hdp.pdf}.}
      \item \eng{Sharif-Razavian, Narges, and Andreas Zollmann. ``An overview of nonparametric bayesian models and applications to natural language processing.'' Science (2008): 71-93.}
    \end{itemize}
  \end{block}
\end{frame}

\end{document}

