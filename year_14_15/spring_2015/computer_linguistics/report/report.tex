\documentclass{beamer}
\usepackage[utf8x]{inputenc}
\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}
\selectlanguage{russian}
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

%\usepackage{fullpage}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, xfrac}
\usepackage{mathtools}

\usepackage{tikz}
\usetikzlibrary{shapes}
\colorlet{light blue}{blue!50}

\newcommand{\obj}[1]{\left\{ #1 \right \}}
\newcommand{\clo}[1]{\left [ #1 \right ]}
\newcommand{\clop}[1]{\left [ #1 \right )}
\newcommand{\ploc}[1]{\left ( #1 \right ]}

\newcommand{\brac}[1]{\left ( #1 \right )}
\newcommand{\crab}[1]{\left ] #1 \right [}
\newcommand{\induc}[1]{\left . #1 \right \vert}
\newcommand{\abs}[1]{\left | #1 \right |}
\newcommand{\nrm}[1]{\left\| #1 \right \|}
\newcommand{\brkt}[1]{\left\langle #1 \right\rangle}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Real}{\mathbb{R}}

\selectlanguage{russian}

\begin{document}
\selectlanguage{russian}
\title{Байесовские модели в компьютерной лингвистике}
\author{Назаров Иван}
\date

%% Title slide
\frame{\titlepage}

%% Table of Contents
% \begin{frame}
%   \frametitle{\rus{Содержание}}
%   \tableofcontents
% \end{frame}

\selectlanguage{russian}
\section{\rus{Байесовская вероятность}} % (fold)
\label{sec:probability}
\begin{frame}
  \begin{block}

    Закон больших чисел: предел относительной частоты явления равен вероятности
  \end{block}
  \begin{block}

    Требуется возможность повторять наблюдения бесконечно
  \end{block}

  \begin{block}{Принципиально неповторяемые события}

    Вероятность -- характеристика веры в наступление события \eng{degree of belief}
    Априорная информация $\oplus$ наблюдения $\Leftarrow$ теорема Байеса $\implies$ апостериорная вероятность
    \[\mathbb{P}\brac{\induc{\theta} \Dcal} \propto L\brac{\induc{\Dcal}\theta} \pi\brac{\theta}\]

  \end{block}
\end{frame}

\begin{frame}\selectlanguage{russian}\frametitle{Типы задач}
  Всегда постулируется вероятностная модель
  \begin{block}{Обратные}

    по наблюдениям определить вероятностные свойства скрытых состояний (классификация)
  \end{block}
  \begin{block}{Прямые}

    определить вероятностные характеристики наблюдений (прогнозирование)
  \end{block}
\end{frame}

\begin{frame}\selectlanguage{russian}\frametitle{Байесовские задачи в компьютерной лингвистике}

  \begin{block}

  \begin{itemize}
    \item Классификация текстов
    \item Определение частей речи
    \item Разрешение омонимии
    \item Машинный перевод
    \item Извлечение информации
  \end{itemize}
  \end{block}
\end{frame}

% section probability (end)

\section{Иллюстрация байесовского подхода} % (fold)
\label{sec:bayes_demonstration}
\begin{frame}\selectlanguage{russian}\frametitle{Пример байесовского подхода}
  \begin{block}

    В качестве примера применения байесовского подхода, рассмотрим подробно ``наивный'' классификатор текстов: основное допущение в независимости появления единиц внутри текста.
  \end{block}
  \begin{block}{Текст}

    Текст $d$ -- упорядоченный набор базовых единиц из пространства $W$.
    Например, $n$-граммы символов или слов.
  \end{block}
  \begin{block}{Термины}

    Пусть $h:W\to \Tcal$ сопоставляет каждой единице некоторый ``термин'' из пространства $\Tcal$. Например, хэш от $n$-граммы или лемма от словоформы.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Корпус}

    Пусть $\Dcal$ -- корпус в обучающей выборке, тексты которого разбиты на классы $\brac{D_c}_{c\in K}$, и $T$ -- термины во всех текстах:
    \[T = \bigcup_{c\in K}T_c;\,T_c = \bigcup_{d\in D_c} T_d\]
  \end{block}

  \begin{block}{Классификация}

    Задача определить класс $c\in K$ для текста $d$ по частотным характеристикам его терминов $T_d = \obj{\induc{h(w)}\,w\in d}$.
    Максимизируется апостериорная вероятность 
   \[\hat{c} = \underset{c\in K}{\text{argmax}} p\brac{\induc{c}\,d} = \underset{c\in K}{\text{argmax}} \ln p\brac{\induc{d}\,c} + \ln \pi\brac{c} \]
    Фактически это выбор наиправдоподобнейшей модели текста $d$ -- (\eng{\emph{model selection}}).
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Модель текста}

    Предполагается независимость появления терминов в тексте (с точностью до константы):
    \[p\brac{\induc{d}\,c} \propto \prod_{t\in \Tcal}\theta_{ct}^{n_{dt}}\]
    где $\brac{\theta_{ct}}_{t\in \Tcal}$ -- вероятностные характеристики текстов класса $c\in K$, ``модель'' текста согласно этому классу, $n_{dt}$ -- количество вхождений термина $t$ в текст $d$
    \[n_{dt} = \abs{\obj{\induc{w\in d} h(w) = t}}\]
  \end{block}

\end{frame}

\begin{frame}
  \begin{block}{Вероятность класса}

    Введём набор распределений $\brac{\pi_c}_{c\in K}$ вероятностных характеристик терминов в классах.
    Согласно теореме Байеса, на основании наблюдений текста $d$, апостериорная плотность распределения вектора вероятностных характеристик $\theta_c = \brac{\theta_{ct}}_{t\in \Tcal}$ класса $c\in K$ равна:
      \[p\brac{\induc{\theta_c}\,d} \propto p\brac{\induc{d}\,\theta_c}\pi_c(\theta_c)\]
    где $\pi_c$ -- априорная плотность распределения $\theta_c$.
  \end{block}
  \begin{block}{Сопряжённые распределения}

    Для анализа, обучения и классификации удобно найти \emph{собственное} семейство многомерных распределений оператора преобразования априорного в апостериорное распределение.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{\eng{Noninformative prior}}

    В силу скудности априорных знаний, нет оснований приписывать \textbf{особенную} вероятностную модель $\theta = \brac{\theta_{ct}}_{t\in T}$ классу $c$.
    В качестве такого неинформативного априорного распределения $\pi_0=\pi(\theta)$ годится равномерное распределение на $S_{\#T-1}^\circ$ открытом $\#T-1$-мерном симплексе в $\clo{0,1}^{\#T}$.
    \begin{align*}
      p\brac{\induc{\theta_c}\,d} & \propto p\brac{\induc{d}\,\theta_c}\pi_0(\theta_c) \\
        & \propto \prod_{t\in T}\theta_{ct}^{n_{dt}} \cdot \frac{1}{\text{const}_{\#T}}
    \end{align*}
    Случай $\#T=2$ сильно напоминает Бета распределение, которое по сути определено на одномерном симплексе в $\clo{0,1}^2$.
    Нужно многомерное обобщение Бета распределения на $S_{\#T-1}^\circ$.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Распределение Дирихле}

    Случайный вектор $x=\brac{X_k}_{k=1}^n$ имеет распределение Дирихле если его совместная плотность равна
    \[f\brac{x;\alpha} = \frac{\Gamma\brac{\sum_{k=1}^n\alpha_k}}{\prod_{k=1}^n \Gamma(\alpha_k)} \prod_{k=1}^n x_k^{\alpha_k-1} 1_{x\in S_{n-1}^\circ}\]
    где $\alpha = \brac{\alpha_k}_{k=1}^n$ -- вектор параметров концентрации, и $S_{n-1}^\circ$ -- открытый $n-1$ мерный симплекс в $\clo{0,1}^n$.
  \end{block}
  \begin{block}{Сопряжённость}

    Если $\pi_c(\theta_c) = \text{Dir}\brac{\theta_c;\alpha_c}$, $\alpha_c$ -- гиперпараметр класса $c$, то при поступлении текста $d$ из $c$ выполняется
    \[\text{Dir}\brac{\theta_c; \alpha_c+n_d} \propto \prod_{t\in T}\theta_{ct}^{n_{dt}} \text{Dir}\brac{\theta_c; \alpha_c}\]
    Апостериорные вероятностные характеристики класса $c$ задаются распределением Дирихле с параметром $\alpha_c + n_d$.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Правдоподобие термина в классе}

    Вероятность появления термина $t$ в классе $c$ с гиперпараметром $\brac{\alpha_t}_{t\in T}$ равна
    \begin{align*}
      p\brac{\induc{t} c} &= \frac{\Gamma\brac{\sum_{s\in T}\alpha_s}}{\prod_{s\in T} \Gamma(\alpha_s)} \int\limits_{S_{n-1}^\circ} \theta_t \prod_{s\in T} \theta_s^{\alpha_s-1} d\theta \\
      &= \frac{\Gamma\brac{\sum_{s\in T}\alpha_s}}{\Gamma(\alpha_t) \prod_{s\neq t} \Gamma(\alpha_s)} \frac{\Gamma(\alpha_t+1) \prod_{s\neq t} \Gamma(\alpha_s)}{\Gamma\brac{\sum_{s\in T}\alpha_s + 1}}\\
      &= \frac{\alpha_t+1}{\sum_{s\in T}\alpha_s}\\
    \end{align*}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Обучение}

    Обучение классификатора сводится к определению вероятностных характеристик терминов в текстах классов.
    Если наблюдается набор текстов $D_c$, то $\alpha_c \to \alpha_c + \sum_{d\in D_c}n_d$.
    Поскольку нет оснований полагать неравномерность распределения вероятностных характеристик, то $\alpha_c = \mathbf{1}$ для каждого класса $c\in K$.
  \end{block}
  \begin{block}{Правдоподобие термина}

    Пусть $f_{ct} = \sum_{d\in D_c} n_{dt}$ -- количество вхождений термина $t$ в класс $c$.
    Тогда апостериорная вероятность термина $t$ в классе $c$ равна
    \[p\brac{\induc{t}c} = \frac{ f_{ct} + 1}{ \sum_{s\in T}f_{cs} + \abs{T}}\]
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Априорная вероятность класса}

    В корпусе $\Dcal$ текстов, и каждый принадлежит некоторому классу $c\in K$. Если корпус репрезентативен, то вероятность $\mathbb{P}(c)$ что текст принадлежит классу $c$, точно приближается частотностью
    \[\eta_c = \frac{\abs{\Dcal_c}}{\abs{\Dcal}}\]
    Очень важна сбалансированность и репрезентативность корпуса текстов, поскольку в противном случае повышается риска ``застревания'' классификатора в некотором классе.
  \end{block}
  \begin{block}{Классификация}

    Итак, к какому классу следует отнести текст?
    \[\hat{c} = \underset{c\in K}{\text{argmax}} \sum_{t\in T} \ln \frac{f_{ct} + 1}{\sum_{s\in T}f_{cs} + \abs{T}} + \ln\frac{\abs{\Dcal_c}}{\abs{\Dcal}}\]
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Источник}
    Автоматическая обработка текстов на естественном языке и компьютерная лингвистика : учеб. пособие / Большакова~Е.И., Клышинский~Э.С., Ландэ~Д.В., Носков~А.А., Пескова~О.В., Ягунова~Е.В. — М.: МИЭМ, 2011. — 272~с. 
  \end{block}
\end{frame}

% section bayes_demonstration (end)

\section{section name} % (fold)
\label{sec:section_name}

% section section_name (end)
\begin{frame}{Процесс Дирихле}
  \begin{block}

    распределение заданное на функциях конечных дискретных рапсределений
    Применительно к К \begin{description}
      \item[нет ограничений на сложность словаря]\hfill\\
        закладывается ненулевая вероятность появления неизвестного ранее термина 
    \end{description}
  \end{block}
\end{frame}

\begin{frame}{Определение}
  \begin{block}

    Случайное распределение $G$ является реализацией процесса $\text{DP}\brac{\alpha,H}$, где $H$ -- базовое распределение на пространстве параметров $\Theta$, и $\alpha$ -- папарметр концентрации, если для любого измеримого конечного разбиения $\brac{A_t}_{t\in T}$ пространства $\Theta$ выполнено:
    \[\brac{G(A_t)}_{t\in T}\sim \text{Dir}\brac{ \brac{\alpha H(A_t)}_{t\in T} }\] 
  \end{block}
\end{frame}


\begin{frame}
  \begin{block}{Примеры}

    \begin{itemize}
      \item разбиение отрезка $\clo{0,1}$: раскалываем отрезок $I_{n-1}$ в пропроции $\beta_n$, присваиваем $\eta_n$ длину произвольного ``осколка'', обозначаем оставщийся $I_n$: \[G = \sum_{k\geq1} \eta_k \delta_{\theta_k^*}\]
      \item наполнение корзины цветными шарами: \begin{itemize}
        \item случайным цветом $\theta_1\sim H$ помечаем шар и кладём его в корзину;
        \item на шаге $n+1$ с вероятностью $\frac{\alpha}{\alpha+n}$ вытягиваем цвет $\theta_{n+1}\sim H$, помечаем им шар и кладём его в корзину;
        \item с вероятностью $\frac{n}{\alpha+n}$ выбираем шар из корзины, его цветом помечаем новый шар и кладём оба в корзину;
        \item 
      \end{itemize} 
    \end{itemize}
  \end{block}
\end{frame}


\section{Байесовские сети} % (fold)
\label{sec:bayesian_networks}

\begin{frame}
  \begin{block}
  
    Вероятностная модель, основанная на направленном ацикличном графе причинно-следственных связей.
  \end{block}
  \begin{block}{Описание}

    Набор случайных величин $\brac{X_v}_{v\in V}$ и ориентированный граф $G=(V,E)$, узлам которого соответствовуют набюдаемые или скрытые переменные или состояния. Рёбра -- отношения причинности или влияния.

    При этом требуется возможность следующей факторизации совместной плотности вероятности
    \[p\brac{\brac{X_v}_{v\in V}} = \prod_{v\in V} p\brac{\induc{X_v} \brac{X_w}_{w\in\pi^+(v)}}\]
    где $\pi^+(v)$ -- множество непосредственных родителей $v$ в $G$.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Локальная Марковость}

    Для любого $v\in V$ и любого $S\subseteq V\setminus\brac{\sigma(v)\cup\obj{v}}$ выполняется
    \[p\brac{\induc{X_v}\brac{X_w}_{w\in \pi^+(v)},\,\brac{X_k}_{k\in S} } = p\brac{\induc{X_v}\brac{X_w}_{w\in \pi^+(v)} }\]
    где множество потомков узла $v\in V$ равно
    \[\sigma(v) = \obj{\induc{w\in V} \exists\,\text{path}\,\pi\subseteq E,\, v\overset{\pi}{\leadsto}w}\]
    
  \end{block}

\end{frame}

\section{Приложени} % (fold)
\label{sec:applications}

\begin{frame}{$n$-граммная модель текста}
  \begin{block}{Основная идея}

     чем дальше вхождения единц текста отстоят друг от друга, тем они более независимы
  \end{block}

  \begin{block}

    Пусть $\brac{w_i}_{i=0}^{l+1}$ -- базовые едининцы текста в порядке их следования, $w_0=w_{l+1}=\$$ -- символы начала и конца текста.

    Совокупная веротяность $w_k$ единицы определяется $n-1$ предыдущей единицей и не зависит от ``префикса'' текста $\brac{w_i}_{i=0}^{k-n}$:
    \begin{align*}
      p\brac{\induc{w}M} &= p\brac{\induc{w_1}\$;M} \cdot p\brac{\induc{w_2}\$,w_1;M} \cdot \ldots \\
        &\times p\brac{\induc{w_n},w_1,\ldots,w_{n-1};M} \\
        &\times \prod_{i=n+1}^l p\brac{\induc{w_i}w_{i-n+1}, w_{i-n+2}, \ldots, w_{i-1};M} \\ 
        &\times p\brac{\induc{\$}w_{l-n+2}, \ldots, w_l;M}
    \end{align*}
  \end{block}
\end{frame}

\begin{frame}{$n$-граммная модель текста}
  \begin{block}{Параметры}

    Вероятности
    \[p\brac{\induc{x_n}x_1, x_2, \ldots, x_{n-1}}\] 
    где $x_i\in \obj{\$}\cup \Sigma$ -- алфавит единиц текста, расширенный маркером конца строки.
    Количество параметров модели $\propto \abs{\Sigma}^n$.
  \end{block}
\end{frame}

\begin{frame}{Униграммы [Goldwater et al. (2008)]}
  \begin{block}{Модель}

    независимое порождение строк, состоящих из независимых входжений слов или конца строки. 
  \end{block}

  \begin{block}

    Слово вытягивается по следующему закону ($\text{DP}\brac{\alpha_0,P_0}$)
    \begin{description}
      \item[новое слово]\hfill\\
        новое слово $w$ вытягивается из $P_0$;
        \item[иначе]\hfill\\
        слово $w$ вытягивается из накопленного лексикона согласно его частотным характеристикам.
    \end{description}    
  \end{block}
\end{frame}
\begin{frame}{Униграммы}
  \begin{block}

    Условная вероятность слова $l$ на $i$ месте в предложении равна:
    \[P\brac{\induc{w_i = t}w_{i-1},\ldots,w_1} = \frac{n_t}{i-1+\alpha_0} + \frac{\alpha_0 P_0(w_i = t)}{i-1+\alpha_0}\]
    Базовое распределение $P_0$ -- униграммная модель на фонемах:
    \[P_0\brac{\induc{w_i = x_1\ldots x_M}} = p_\$ \brac{1-p_\$}^{M-1} \prod_{j=1}^M P(x_j)\]
    где $P(x_j)$ -- равномерное распределение на фонемах, $p_\$$ -- вероятность конца слова.
  \end{block}
\end{frame}

% section applications (end)

%% \begin{frame}[fragile]{$n$-граммная модель}
%% 
%%   \begin{block}
%% 
%%     % http://texographie.fr/2014/09/arbres-probabilite-forest/
%%     %% Рисуем часть дерева горниц от уровня {n+1} до {n}
%%     \begin{figure}[htb]\begin{center}
%%     %% Задаём стили вершин дерева и меток рёбер.
%%       \tikzstyle{tree} = [isosceles triangle, shape border rotate = 90, % isosceles triangle stretching = True,
%%         fill = none, thick, draw, minimum height = 2em, minimum width = 1cm]
%%       \tikzstyle{door} = [minimum width = 1em, minimum height = 1em, yshift = -1.5em]
%%       \tikzstyle{passable} = [midway, fill = white] % , circle, draw]
%% 
%%       \begin{tikzpicture}[level/.style={sibling distance = 15mm/#1, level distance = 25mm }]
%% 
%%       \node [door] (root) {$\pi_{n+1}$} [child anchor = north]
%%         child { node [tree] (s1) {$\pi_n$} { node[door] {$1$} }
%%            edge from parent node [passable] {$p$} }
%%         child { node [tree] (s2) {$\pi_n$} { node[door] {$2$} }
%%            edge from parent node [passable] {$p$} }
%%         child { node[draw=none] (hidden) {} edge from parent[draw=none] }
%%         child { node [tree] (sk) {$\pi_n$} { node[door] {$k$} }
%%            edge from parent node [passable] {$p$} };
%%       \path (s2) -- (sk) node [midway, above] {$\cdots$};
%%       \end{tikzpicture}
%% 
%%       \caption{Граф части байесовской сети $n$-граммной модели}
%%     \label{fig:tree_structure}
%%     \end{center}\end{figure}
%%   \end{block}
%% \end{frame}

% section bayesian_networks (end)

\end{document}

