\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[russian, english]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Computer lingusitics}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\selectlanguage{russian}
\section{Lecture \#1} % (fold)
\label{sec:lecture_1}
\eng{2015-01-12: Introduction}
\begin{enumerate}
	\item Практические мелкие задачи -- поверхностный синтаксический анализ
	\item Работоспособное приложение
	\item Теория
\end{enumerate}
\eng{Natural Language Processing}
\eng{Computaional linguistics}
Общая лингвистика \begin{itemize}
	\item Синтаксис, синтактика
	\item Семантика
	\item Прагматика -- естественный язык со своими особенностями развивался из соображений удобства решения пракитических задач гоминидов.
\end{itemize}	
Теория формальных языков, Иерархия грамматик Хомского (\eng{Noam Chomsky})

\eng{Quatitative (statistical) linguistics}

Проблема существования языковой универсали (инвариант):
к сожалению среди всех языков универсалью может лищь считаться существование гласных и согласных.
Среди европейских языков -- существование частей речи.

Семиотика -- теория знаковых систем.
Треугольник Фреге
\begin{description}
	\item[\eng{Signifier}] Смысловой символ
	\item[\eng{Signified}] Представление в сознании
	\item[\eng{Referent}] Целевой предмет или явление
\end{description}

Базовые единицы:
\begin{itemize}
	\item фонемы/графемы
	\item лексемы
\end{itemize}

Уровни:
\begin{itemize}
	\item Синтаксичечкий -- предложение
		словосочетания $\to$ сверхфразовые единства
	\item Графематический -- графемы
	\item Морфологический -- слова
	\item Семантический -- элементарная единица ``сема''
	\item Дискурсивный -- \eng{Coherent connected set of sentences}
\end{itemize}

Невозможность взаимо однозначного отображения лексемы в смысл
\begin{description}
	\item[Полисемия] многозначность языковой единицы
	\item[Синонимия] совпадение единиц по смыслу
	\item[Омонимия] совпадение единиц по форме, существенное различие по смыслу.
	Бывает лексическая и морфологическая.
\end{description}

\begin{itemize}
	\item Семантические сети
	\item Синтез нового текста -- языковая компетенция
	\item векторная модлеь текста (\eng{bag of words})
\end{itemize}

% section lecture_1 (end)

\selectlanguage{russian}
\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

Вычислительная (квантитативная) лингвистика опирается на предположение, что то, что верно для выборки текстов, верно для всей совокупности текстов и даже для всего естественного языка в целом (возможно имеется ввиду репрезентативная выборка).

Для каждой статистической единицы текста подсчитывается число употреблений в тексте (группе текстов) $T$:
для каждого объекта $e\in \mathcal{E}$ абсолютная частота
\[a_e \defn \#\obj{\text{вхождение сущности } e \text{ в текст } T}\]

Относительная частота отражает частотные характеристики внутри одного текста (группы текстов) $T$.
Для объекта $e\in \mathcal{E}$ \[r_e \defn \frac{a_e}{\#\text{различные сущности в } T}\]

Возможные \textbf{статистические сущности} перечислены ниже:
\begin{description}
	\item[Лемма (Слово)] \hfill\\
	Слово в широком смысле -- единица морфологического анализа, единица толкового словаря, каноническая, основная, начальная форма словоформы (сохранение части речи, аспекта, времеи и т.п.).
	\item[Словоформа] \hfill\\
	Слово в узком смысле -- одинаковая последовательность фонем. Совокупность словоформ одной лексемы -- слвоизменительная парадигма.
	\item[Словоупотребление] \hfill\\
	Вхождение словоформы в текст, единица текста.
\end{description}

При подсчёте статистики символов нужно всегда понимать, показательная ли она.
Частоты символов ($n$-грамм):
\begin{description}
	\item[относительная] \hfill\\
	частота на уникальный символ (внутри одного текста);
	\item[абсолютная] \hfill\\
	частота на все символы (между текстами).
\end{description}

Общая форма гистограммы абсолютных частот букв сохраняется от текста к тексту -- характеристика языка. Может использоваться для:
\begin{description}
	\item[Определение кодировки текста]\hfill\\
	при верном определении количество недопустимых биграмм минимально;
	\item[Определение языка текста] \hfill\\
	Единица \eng{Schilling}
	\item[Определение схожести текстов]
	\item[Дешифровка текстов] \hfill\\
	В случае простоых перестановочных шифров.
\end{description}


% \eng{Brown's corpus}

\begin{description}
	\item[Закон Ципфа (\eng{Zipf's law})] \hfill\\
	Текст $\to$ Частотный список слов $\to$ Частота слова обратно пропорциональна рангу слова $\to$ Распределение с тяжёлым хвостом $\to$ соотношение Парето:
	``Большая часть текста формируется минимальными количеством слов.''
	Эмпирический закон, имеет недостатки:
	\begin{itemize}
		\item Художественный текст лексически богаче, чем технический.
		\item Нарушается на фрагментах текста $\implies$ закон не языка, а текста.
		\item проверка текста на естественность (зная тему, можно сравнить с эталоном).
	\end{itemize}


	\item[Формула Кондона] \hfill\\
	Для относительных и абсолютных частот при переходе к \eng{log-log} шкале прослеживается линенйная зависимость. Исходя из этого степенной закон распределения Ципфа обощается до одыкновенного \eng{Power law}:
	\[a_e \defn \frac{C}{\text{rank}_e^\gamma}\,\text{ и }\,r_e \defn \frac{C}{N \text{rank}_e^\gamma}\]
	где $N$ -- общее количество словоупотреблений в тексте,
	$\gamma>0$ -- коэффициент лексического богатстсва текста, зависящий от тектста, автора, стиля, направленности и т.п.
	Эмпирически выясняется, что мем меньше $\gamma$ тем богаче язык по словоформам.

	\item[\eng{Zipf-Mandelbr\:ot law}] \hfill\\
	Кодирование сообщений. Чем чаще слово тем короче код. Можно наблюдать в естественном языке: служебные слова, например. (Хаффман). В процессе развития естественный язык самооптимизировался.

	Для малых рангов (более частых слов) закон Ципфа нарушается. Вводится поправка для высокочастотных статистических единиц:
	\[a_e \defn \frac{k N}{\brac{\rho + \text{rank}_e}^\gamma}\]
	\begin{itemize}
		\item Плохо работает на очень редких словах (тяжёлый хвост).
		\item Константы зависят от стиля и длины
		\item Постоянство $\gamma$ не сохраняется.
		\item Даёт лишь грубое приближение к статистической структуре текста.
	\end{itemize}

	\item[\eng{Herdan-Heaps law}] \hfill\\
	Эмпирический закон, согласно которому количество различных слов (?) увеличивается с ростом объёма текста с убывающей отдачей:
	\[V(N)\defn K N^\beta\]
	То есть с ростом объёма текста не происходит насыщения словаря уникальных слов, из которого состоит текст, однако скорость выявления словаря убывает. 
\end{description}

Стоп-список для несмысловых слов.
Выявление наиболее значимых слов -- они имеют среднюю частоту.
Средняя длина слова также может играть роль в определении определение стиля/темы/автора.

$n$\eng{-gramm} модель.
Рассмотрим $W = \brac{w_i}_{i=1}^n$ -- некоторая $n$-грамма.
Предполагается, что вероятность сущности текста зависит от предыдущих $n-1$ сущностей.
\[ \Pr(W) = \Pr\brac{w_1,w_2,\ldots,w_n} = \prod_{k=1}^n \Pr\brac{\induc{w_k}\,\brac{w_i}_{i=1}^{k-1}}\]
Вероятность всего предложения вычисляется как произведение вероятности всех входящих в него $n$-грамм.

Разложение совместной вероятности в предположении направленности порядка слов позволяет строить модели, способные синтезировать текст:

На практике объём корпуса ограничен (и есть закон Хердана-Хиппса), то не все $n$-граммы представлены.
Поэтому можно ввести упрощение вероятности $n$-граммы до марковской цепи порядка $k\leq n-1$:
\[\Pr\brac{w_1,w_2,\ldots,w_n} = \prod_{j=1}^n \Pr\brac{\induc{w_j}\,\brac{w_{j-i}}_{i=1}^k}\]

Изложенная модель проста и легко строится по любому корпусу.
Однако требует колоссального объёма информации, поскольку \eng{scope} корпуса ограничен,
то нет возможности определить допустима или нет отсутствующая $n$-грамма (её вероятность $0$).

Сглаживание -- повышение вероятности одних $n$-грамм за счёт других.
Пример для биграмм : \[\Pr\brac{\induc{w_2}\, w_1} = \frac{r_{w_1w_2}+\alpha}{r_{w_1}+\alpha V}\]
где $V$ -- количество единиц в тексте.

Коэффициент неопределённости (?)

% Домашка см лекция~2 слайд~50

% section lecture_2 (end)

\end{document}

