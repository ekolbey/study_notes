\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[russian, english]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Computer lingusitics}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\selectlanguage{russian}
\section{Lecture \#1} % (fold)
\label{sec:lecture_1}
\eng{2015-01-12: Introduction}
\begin{enumerate}
	\item Практические мелкие задачи -- поверхностный синтаксический анализ
	\item Работоспособное приложение
	\item Теория
\end{enumerate}
\eng{Natural Language Processing}
\eng{Computaional linguistics}
Общая лингвистика \begin{itemize}
	\item Синтаксис, синтактика
	\item Семантика
	\item Прагматика -- естественный язык со своими особенностями развивался из соображений удобства решения пракитических задач гоминидов.
\end{itemize}	
Теория формальных языков, Иерархия грамматик Хомского (\eng{Noam Chomsky})

\eng{Quatitative (statistical) linguistics}

Проблема существования языковой универсали (инвариант):
к сожалению среди всех языков универсалью может лищь считаться существование гласных и согласных.
Среди европейских языков -- существование частей речи.

Семиотика -- теория знаковых систем.
Треугольник Фреге
\begin{description}
	\item[\eng{Signifier}] Смысловой символ
	\item[\eng{Signified}] Представление в сознании
	\item[\eng{Referent}] Целевой предмет или явление
\end{description}

Базовые единицы:
\begin{itemize}
	\item фонемы/графемы
	\item лексемы
\end{itemize}

Уровни:
\begin{itemize}
	\item Синтаксичечкий -- предложение
		словосочетания $\to$ сверхфразовые единства
	\item Графематический -- графемы
	\item Морфологический -- слова
	\item Семантический -- элементарная единица ``сема''
	\item Дискурсивный -- \eng{Coherent connected set of sentences}
\end{itemize}

Невозможность взаимо однозначного отображения лексемы в смысл
\begin{description}
	\item[Полисемия] многозначность языковой единицы
	\item[Синонимия] совпадение единиц по смыслу
	\item[Омонимия] совпадение единиц по форме, существенное различие по смыслу.
	Бывает лексическая и морфологическая.
\end{description}

\begin{itemize}
	\item Семантические сети
	\item Синтез нового текста -- языковая компетенция
	\item векторная модлеь текста (\eng{bag of words})
\end{itemize}

% section lecture_1 (end)

\selectlanguage{russian}
\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

Вычислительная (квантитативная) лингвистика опирается на предположение, что то, что верно для выборки текстов, верно для всей совокупности текстов и даже для всего естественного языка в целом (возможно имеется ввиду репрезентативная выборка).

Для каждой статистической единицы текста подсчитывается число употреблений в тексте (группе текстов) $T$:
для каждого объекта $e\in \mathcal{E}$ абсолютная частота
\[a_e \defn \#\obj{\text{вхождение сущности } e \text{ в текст } T}\]

Относительная частота отражает частотные характеристики внутри одного текста (группы текстов) $T$.
Для объекта $e\in \mathcal{E}$ \[r_e \defn \frac{a_e}{\#\text{различные сущности в } T}\]

Возможные \textbf{статистические сущности} перечислены ниже:
\begin{description}
	\item[Лемма (Слово)] \hfill\\
	Слово в широком смысле -- единица морфологического анализа, единица толкового словаря, каноническая, основная, начальная форма словоформы (сохранение части речи, аспекта, времеи и т.п.).
	\item[Словоформа] \hfill\\
	Слово в узком смысле -- одинаковая последовательность фонем. Совокупность словоформ одной лексемы -- слвоизменительная парадигма.
	\item[Словоупотребление] \hfill\\
	Вхождение словоформы в текст, единица текста.
\end{description}

При подсчёте статистики символов нужно всегда понимать, показательная ли она.
Частоты символов ($n$-грамм):
\begin{description}
	\item[относительная] \hfill\\
	частота на уникальный символ (внутри одного текста);
	\item[абсолютная] \hfill\\
	частота на все символы (между текстами).
\end{description}

Общая форма гистограммы абсолютных частот букв сохраняется от текста к тексту -- характеристика языка. Может использоваться для:
\begin{description}
	\item[Определение кодировки текста]\hfill\\
	при верном определении количество недопустимых биграмм минимально;
	\item[Определение языка текста] \hfill\\
	Единица \eng{Schilling}
	\item[Определение схожести текстов]
	\item[Дешифровка текстов] \hfill\\
	В случае простоых перестановочных шифров.
\end{description}


% \eng{Brown's corpus}

\begin{description}
	\item[Закон Ципфа (\eng{Zipf's law})] \hfill\\
	Текст $\to$ Частотный список слов $\to$ Частота слова обратно пропорциональна рангу слова $\to$ Распределение с тяжёлым хвостом $\to$ соотношение Парето:
	``Большая часть текста формируется минимальными количеством слов.''
	Эмпирический закон, имеет недостатки:
	\begin{itemize}
		\item Художественный текст лексически богаче, чем технический.
		\item Нарушается на фрагментах текста $\implies$ закон не языка, а текста.
		\item проверка текста на естественность (зная тему, можно сравнить с эталоном).
	\end{itemize}


	\item[Формула Кондона] \hfill\\
	Для относительных и абсолютных частот при переходе к \eng{log-log} шкале прослеживается линенйная зависимость. Исходя из этого степенной закон распределения Ципфа обощается до одыкновенного \eng{Power law}:
	\[a_e \defn \frac{C}{\text{rank}_e^\gamma}\,\text{ и }\,r_e \defn \frac{C}{N \text{rank}_e^\gamma}\]
	где $N$ -- общее количество словоупотреблений в тексте,
	$\gamma>0$ -- коэффициент лексического богатстсва текста, зависящий от тектста, автора, стиля, направленности и т.п.
	Эмпирически выясняется, что мем меньше $\gamma$ тем богаче язык по словоформам.

	\item[\eng{Zipf-Mandelbr\:ot law}] \hfill\\
	Кодирование сообщений. Чем чаще слово тем короче код. Можно наблюдать в естественном языке: служебные слова, например. (Хаффман). В процессе развития естественный язык самооптимизировался.

	Для малых рангов (более частых слов) закон Ципфа нарушается. Вводится поправка для высокочастотных статистических единиц:
	\[a_e \defn \frac{k N}{\brac{\rho + \text{rank}_e}^\gamma}\]
	\begin{itemize}
		\item Плохо работает на очень редких словах (тяжёлый хвост).
		\item Константы зависят от стиля и длины
		\item Постоянство $\gamma$ не сохраняется.
		\item Даёт лишь грубое приближение к статистической структуре текста.
	\end{itemize}

	\item[\eng{Herdan-Heaps law}] \hfill\\
	Эмпирический закон, согласно которому количество различных слов (?) увеличивается с ростом объёма текста с убывающей отдачей:
	\[V(N)\defn K N^\beta\]
	То есть с ростом объёма текста не происходит насыщения словаря уникальных слов, из которого состоит текст, однако скорость выявления словаря убывает. 
\end{description}

Стоп-список для несмысловых слов.
Выявление наиболее значимых слов -- они имеют среднюю частоту.
Средняя длина слова также может играть роль в определении определение стиля/темы/автора.

$n$\eng{-gramm} модель.
Рассмотрим $W = \brac{w_i}_{i=1}^n$ -- некоторая $n$-грамма.
Предполагается, что вероятность сущности текста зависит от предыдущих $n-1$ сущностей.
\[ \Pr(W) = \Pr\brac{w_1,w_2,\ldots,w_n} = \prod_{k=1}^n \Pr\brac{\induc{w_k}\,\brac{w_i}_{i=1}^{k-1}}\]
Вероятность всего предложения вычисляется как произведение вероятности всех входящих в него $n$-грамм.

Разложение совместной вероятности в предположении направленности порядка слов позволяет строить модели, способные синтезировать текст:

На практике объём корпуса ограничен (и есть закон Хердана-Хиппса), то не все $n$-граммы представлены.
Поэтому можно ввести упрощение вероятности $n$-граммы до марковской цепи порядка $k\leq n-1$:
\[\Pr\brac{w_1,w_2,\ldots,w_n} = \prod_{j=1}^n \Pr\brac{\induc{w_j}\,\brac{w_{j-i}}_{i=1}^k}\]

Изложенная модель проста и легко строится по любому корпусу.
Однако требует колоссального объёма информации, поскольку \eng{scope} корпуса ограничен,
то нет возможности определить допустима или нет отсутствующая $n$-грамма (её вероятность $0$).

Сглаживание -- повышение вероятности одних $n$-грамм за счёт других.
Пример для биграмм : \[\Pr\brac{\induc{w_2}\, w_1} = \frac{r_{w_1w_2}+\alpha}{r_{w_1}+\alpha V}\]
где $V$ -- количество единиц в тексте.

Коэффициент неопределённости (?)

% Домашка см лекция~2 слайд~50

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

Корпусная лингвистика занимается теорией и практикой создания размеченных языковых корпусов.

Наличие разметки исследуемых целевых языковых явлений

Корпус и коллекция текстов отличаются тем, что в коллекции нет качеств охвата, полноты разметки, а главное цели собранных языковых явлений.

Лингвистические задачи: язык вид речи, стиль жанр, употребление слова, буквы, словосочетания, части речи и тп для анализа состояния языка пространственно (диалекты -- география) и/или во времени (изменчивость).

Корпус -- соответствие лингвистической задаче, должен быть репрезентативен, сбалансированный и без перекосов. Полнота охвата изучаемого явления. Массив данных с разметкой

Типы корпусов: \begin{itemize}
	\item Язык и параллельность многоязычных текстов
	\item Стиль жанр: публицистический, литературный и тп
	\item Полные или фрагментированные тексты
	\item Вид данных: речь, письмо
	\item Размеченные/неразмеченный
	\item Динамичность относительно пополнения
\end{itemize}
% RuWac, Генеральный корпус русского языка

Проблемы с корпусами:
\begin{itemize}
	\item Определить критерий отбора текстов
	\item решить проблемы с авторскими правами
	\item балансировка и репрезентативность
	\item приведение к единому формату текста и разметки
	\item сам процесс разметки
\end{itemize}

Поэтому корпусы создаются очень медленно и, зачастую, некачественно.
Корпусы не всегда масштабируемы на весь естественный язык: что норма в них, не всегда норма в языке.
Плохо отражает редкие явления.

%% В приложениях используются методы численной оптимизации.

Интернет не корпус, нет лингвистически релевантной разметки.
Плохо структурирован для задач лингвистических исследований.
Очистка от рекламы, меню контекстных/неконтекстных ссылок.

Корпуса всё-таки нужны, поскольку они могут служить эталонами.
Используются для построения статистических языковых моделей.

Берём слово для исследования частоты использования:
% многозначное или значимое изменение со временем
% написать отчёт о сходстве/различии
% Супруги -- парная конная упряжь

Исследование морфологических анализаторов русского языка, стемминг (откидывание словоизменительной части).
% solarix.ru
%% НКРЯ

% Доклад на 2015-02-09:
%% Байесовская вероятность и её использование


Морфологический (графематический) анализ очень важен!
% два направления: анализ текста и синтез текста


Графематический
Морфологический
	Приведение слова к нормальному виду слова (лемма), выделение основы слова (стемминг).
Постморфологический

Синтаксический
Семантический и прагматический

уровень символы и знаки текста
Графема -- минимальные неделимые единицы текста
Посимвольная обработка: выделение единиц \eng{tokenization}

\eng{Token} -- цепочка знаков от разделителя, до разделителя (пробелы, знаки препинания), соответствует лексеме в языках программирования.

\eng{Tokenization} -- Вычленение значимых единиц.

\begin{itemize}
	\item Слова могут быть написаны с ``разрядкой''
	\item преобразование числительных в числа
	\item восстановление правильного регистра (\eng{truecase})
	\item различия дефиса, тире и переноса: либо собрать слово, либо разделить на морфы
	\item нормализация сокращений
	\item свёртка составных предлогов или союзов, устойчивых фразеологизмов
	\item выделение полного имени (ФИО)
\end{itemize}
Для всего этого нужны словари сущностей.

Сложности
\begin{itemize}
	\item Неуниверсальности:
	\begin{itemize}
		\item Маркеры конца предложения (точка, троеточие, вопросительный или восклицательный знак)
		\item Маркеры начала предложения
	\end{itemize}
	\item пропуски знаков препинания (нужны контекст маркеры)
	\item оформление цитат, прямой речи
\end{itemize}

По размеченным текстам (в корпусе)
Инженерный подход \eng{rule-based}
\begin{itemize}
	\item словари имён, сокращений словосочетаний
	\item эвристические правила
\end{itemize}

Точность сегментации зависит от тематики, жанра.
Графематика почти никогда не идёт отдельно от морфологии

Инженерный подход: \url{http://aot.ru} -- вход текст, выход текст с дескрипторами, (основные, контекстные и макросинтактические).

% MyStem: нет морфологии если поток символов не разбит на единицы

Анализаторы основываются на регулярных грамматиках (\eng{Chomsky type 3}).

% section lecture_3 (end)


\end{document}

