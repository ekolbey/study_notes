\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ex}[0]{{\mathbb{E}}}
\newcommand{\pr}[0]{{\mathbb{P}}}
\newcommand{\Var}[0]{{\text{Var}}}
\newcommand{\RSS}{{\text{RSS}}}
\newcommand{\TR}{{\text{Tr}}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\rank}{\mathop{\text{rank}}\nolimits}



\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Document title}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section*{Preliminaries} % (fold)
\label{sec:preliminaries}

\url{http://cs.hse.ru/ai/mmdm}

The folowing topics would be covered in the course:
\begin{itemize}
	\item Decision theory wit linear regression
	\item Shrinkage methods (Ridge regression, Lasso, elastic net)
	\item Polynomial regression, splines
	\item Model selection techniques, validation methods
	\item Classification problems (the Support Vector Machine, decision trees, bagging and boosting)
	\item Neural networks (flexible regression and classification)
\end{itemize}

Main books:
\begin{description}
	\item[Tibshirani2013] \hfill\\
		James, Witten, Hastie and Tibshirani (2013): ``An Introduction to Statistical Learning with Applications in R''
	\item[Bishop2006] \hfill\\
		Bishop (2006): ``Pattern Recognition and Machine Learning''
	\item[Tibshirani2013] \hfill\\
		Hastie, Tibshirani, Friedman (2013): ``The Elements of Statistical Learning''
\end{description}

% section* preliminaries (end)

\section{Lecture \# 1} % (fold)
\label{sec:lecture_1}

Linear regression lies in the class of supervised learning problems.
Such problems work with paired data of arbitrary nature: univariate, multivariate, categorical, numerical, abstract (like graphs in chemical classification problems) et c.
In general there is a learning (training) sample $\brac{x_i, T_i}_{i=1}^N$ of a pairs of an input variable $X$ and the associated output target $T$.

The goal is to construct a function $y(\cdot)$ of the input that predicts, forecasts, estimates or otherwise returns the target, associated with the given input.

The target variable $T$ is always considered to be a random variable at least due to measurement or other irreducible uncertainty, that is present in any system.
The input variable $X$, depending on the approach, may or may not be random.

In the case when it is an rv, the data is characterized by the join probability density (distribution) function $p_{X,T}\brac{x,t}$.
The joint pdf can be factorized into the density of the response conditional on a particular input and the density (probability) of the input:\[p_{X,T}\brac{x,t} = p_{\induc{T}X}\brac{\induc{t}x} p_X(x) \]
The analysis of the problem may now fork:
\begin{description}
	\item[Generative model] \hfill \\
		both the data and the response given data are modelled;
	\item[Discriminative model] \hfill \\
		model just the conditional response.
\end{description}

Statistical decision theory helps decide on a functional form of $y(\cdot)$:
choose $y(\cdot)$ so a to minimize the expected loss functional $\Ex_{X,T} L\brac{T,y(X)}$
over the bivariate distribution of the input data.
This way the incorrect estimation of $t$ by $y(x)$ given $x$ is penalized.

Among the possible loss functions the most frequently used is the quadratic
loss function $L(t,y) \defn \brac{t-y}^2$, which has its roots in the $L^p$ norm
\[\nrm{f}_p = \brac{\int \abs{f}^p d\mu}^\frac{1}{p}\] for $p = 2$.

Another possible loss function is the so called Minkowski cost, defined as $L(t,y) \defn \abs{t-y}$,
which is the $L^p$ norm with $p=1$.

Sidenote: conditional expectation as the projection of $f$ onto a subspace of $g$ measurable functions in the Hilbert space of square integrable functions with the usual $\int f\bar{g} d\mu$ inner product.

% Remember the measure theory? with its sigma-finite measure \mu with respect to which another measure \nu is absolutely continuous, and there is f\in L^1(\mu) such that \nu=\int f d\mu and \int gd\nu = \int fg d\mu.

% Which approach to use?
The joint randomness of $(X,T)$ leads to the following problem in the case of $L^2$ loss function:
\[\Ex_{X,T} \brac{T-y(X)}^2 \to \min_y\]
Using the tower property of the conditional expectation the optimal $y$ is the conditional expectation $\phi(X)\defn \Ex\brac{\induc{T}X}$.
Indeed, since $\Ex_{\induc{T}X} \brac{\brac{T-\phi(X)} } = 0$ , the mean squared error is
\begin{align*}
	\Ex_{X,T} \brac{T-y(X)}^2 = \Ex_{X} \brac{\Ex_{\induc{T}X} \brac{T-y(X)}^2 } \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{ \brac{T-\phi(X)}^2+\brac{\phi(X)-y(X)}^2} + 2 \Ex_{\induc{T}X} \brac{\brac{T-\phi(X)}\brac{\phi(X)-y(X)} }} \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{T-\phi(X)}^2+\brac{\phi(X)-y(X)}^2 } + 2 \Ex_X \brac{\brac{\phi(X)-y(X)} \Ex_{\induc{T}X} \brac{T-\phi(X)} } \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{T-\phi(X)}^2 + \Ex_{\induc{T}X} \brac{\phi(X)-y(X)}^2 }
\end{align*}

The latter conditional expectation is vanishes whenever $y=\phi$, whilst the first one is independent of $y$ and can be considered as the irreducible variance due to structural variance.
The mean prediction error is the irreducible variance and the approximation error:
\[\text{error} \defn \Ex_X \Var\brac{\induc{T}X} + \Ex_x \brac{\brac{\phi(X)-y(X)}^2} \]

However it is never possible to achieve $y(x) = \Ex\brac{\induc{T}X=x}$.
Therefore some insight is needed on $\Ex_{\induc{T}X} \brac{\phi(X)-y(X)}^2$.
\begin{description}
	\item[Frequentist] $\pr\brac{\induc{\Dcal}\Theta}$ -- uncertainty is due to dataset
	\item[Bayesian] $\pr\brac{\induc{\Dcal}\Theta} \pi(\Theta)$ -- uncertainty is due parameters, where $\pi(\Theta)$ is the prior distribution.
	The posterior distribution is proportional to
		\[\pr\brac{\induc{\Theta}\Dcal} \sim \pr\brac{\induc{\Dcal}\Theta} \pi(\Theta)\]
\end{description}

Error is for a particular training dataset, on which the $y$ is estimated. In effect the functional form of $y$ depends on the data set $D$:
\[\Ex_\Dcal\brac{\text{Error}_\Dcal} = \text{Noise} + \Ex_\Dcal \Ex_X \brac{ y(x,D)-\phi(x) }^2\]
where noise is defined as $\Ex_X \Var\brac{\induc{T}X}$.

Now put $f(x) \defn \Ex_\Dcal y(x,D)$.
\begin{align*}
	\Ex_\Dcal\brac{ y(x,D)-\phi(x)\pm f(x) }^2 &
		= \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \Ex_\Dcal\brac{ f(x) - \phi(x) }^2 \\ &+ 2 \Ex_\Dcal\brac{ y(x,D) - f(x) }\brac{ f(x) - \phi(x) } \\
		& = \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \brac{ f(x) - \phi(x) }^2 \\&+ 2\brac{ f(x) - \phi(x) } \Ex_\Dcal\brac{ y(x,D) - f(x) } \\
		& = \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \brac{ f(x) - \phi(x) }^2 \\
\end{align*}

Therefore the expected value of the loss is 
\[\Ex_\Dcal\brac{\text{Error}_\Dcal} = \text{Noise} + \text{Variance} + \text{Bias}^2\]
with the variance is due to the fact that the instance of $y$ is not static with varying dataset $D$,
and the bias is the measure of average closeness of the approximation $y(\cdot)$ to the real conditional expectation $\Ex\brac{\induc{T}X}$.
This is the decomposition of the loss into systemic noise, bias and variance.

% Non-parametric approach + low dimension: smoothing local data (kNN -- $k$ nearest neighbours).

Parametric approach would require a parametric expression of $y(\cdot)$ as an approximation of $\Ex\brac{\induc{T}X}$ at every $x$ (or at most almost all).

Suppose $\Lcal^2$ is the space in which the best approximation of $\phi(X)$ is searched for.
Projecting $\Ex\brac{\induc{T}X}$ onto $\Lcal^2$ yields $\phi(X) = h^\perp(X) + h(X)$, where $h^\perp \perp \Lcal^2$ (i.e. $\brkt{h^\perp,g} = 0$ for all $g\in \Lcal$) and $h\in \Lcal^2$.

% Here should be a picture of the projection: the conditional expectation is projected onto the subspace of square integrable functions.

% \Ex_\Dcal y(X,D) -- is an element of the 

Since $f-h\in\Lcal^2$ it must be true that
\[\Ex_X \brac{ f(X) - h(X) } h^\perp(X) = \brkt{f-h, h^\perp} = 0 \]
whence the following decomposition must hold:
\begin{align*}
	\text{Bias}^2 &= \Ex_X \brac{ \Ex_\Dcal y(X,D) - \phi(X) }^2 
		= \Ex_X \brac{ f(X) - h^\perp(X) - h(X) }^2 \\
		& = \Ex_X \brac{ f(X) - h }^2 + \Ex_X \brac{ h^\perp(X) }^2 + \Ex_X \brac{ f(X) - h(X) } h^\perp(X) \\
		& = \Ex_X \brac{ f(X) - h(X) }^2 + \Ex_X \brac{ h^\perp(X) }^2
\end{align*}
This means that the squared bias is the sum of how close the model is to the projection of $\Ex\brac{\induc{T}X}$ onto $\Lcal^2$, and how well the projection approximates the conditional expectation itself.

In linear regression problems the space $\Lcal^2$ is taken to be the set of all linear functions of $X$.

\subsection*{Linear regression} % (fold)
\label{sub:linear_regression}

Suppose $X$ is a $p$-dimensional vector of features and that we attempt to approximate the $\Ex\brac{\induc{T}X}$ with $y(x) \defn \beta_0 + \sum_{k=1}^p \beta_k x_k$.

In effect we suppose the following econometric model
\[T = \beta_0 + \sum_{k=1}^p \beta_k X_p + \epsilon;\,\epsilon\sim\Ncal(0,\sigma^2)\]
Therefore conditional upon $X$ the target $T$ is distributed like \[\induc{T}X\sim \Ncal(y(X),\sigma^2)\]

In this setting the following questions arise: \begin{itemize}
	\item How $\beta_{1\times(p+1)}$ is estimated?
	\item Which coefficients $\beta_k$ are significant?
	\item What is their interpretation?
\end{itemize}

In the matrix form, the linear regression problem is stated as follows:
\[\underset{n\times 1}{T} = \underset{n\times (1+p)}{X}\underset{(1+p)\times 1}{\beta} + \underset{n\times 1}{\epsilon}\]
where the vector $X$ of the predictors of the training set has been altered by adding a constant term -- the intercept. The predictors now look like this
\[X = \brac{ \one \defn \underset{n\times 1}{1}, \underset{n\times 1}{x_1}, \ldots, \underset{n\times 1}{x_p} }\]

Two approachs to estimating $\beta$ is possible: the \textbf{L}east \textbf{S}quares\footnotemark  and the \textbf{M}aximum \textbf{L}ikelihood.
Effectively the LS approach to parameter estimation is non-probabilistic.
There are different flavours of least squares: the ordinary, the generalised (with a weighting matrix), two-stage least squares (2SLS), to name a few, and others.
The 2SLS is an extension of the GLS that attempts to battle heteroskedasticity:
first run OLS to get the squared residuals, then GLS with weights that standardise the residuals.

The goal of LS is to solve the following problem: minimize the \textbf{R}esidual \textbf{S}um of \textbf{S}squares
\[\RSS \defn \brac{T-X\beta}'\brac{T-X\beta} \to \min_\beta\]
The first order conditions on the potential minimizer are in the matrix form:
\begin{align*}
	\frac{\partial}{\partial \beta}\RSS &= \frac{\partial}{\partial \beta}\brac{T'T-\beta'X'T - T'X\beta + \beta'X'X\beta}\\
	&= \frac{\partial}{\partial \beta} T'T - \frac{\partial}{\partial \beta} \beta'X'T - \frac{\partial}{\partial \beta} T'X\beta + \frac{\partial}{\partial \beta} \TR\brac{\beta'X'X\beta}\\
	&= - 2 \frac{\partial}{\partial \beta} T'X\beta + \beta'\brac{X'X + \brac{X'X}'}\\
	&= - 2 T'X + 2 \beta'X'X = 0
\end{align*}
If the matrix $X$ is full rank, then the matrix $X'X$ is invertible, and is positive-semidefinite.
Therefore $\frac{\partial^2}{\partial \beta\partial \beta}\RSS = 2 X'X \geq 0$, whence the Least Squares solution $\hat{\beta} \defn \brac{X'X}^{-1} X'T$ is indeed the minimizer of the RSS.

The best linear approximation to $\Ex\brac{\induc{T}X}$ is the projection of $T$ onto the space of linear functionals of $X$.

The matrix $\hat{H} \defn X\brac{X'X}^{-1}X'$ is called the \textbf{hat matrix} and is in fact a projector onto the linear subspace spanned by the column vectors of $X$.

Every projector matrix has the following property: if $v$ is in the spanned linear subspace, then $\hat{H}v = v$.
Indeed, since $v\in \clo{X}$, there must exist $\underset{(p+1)\times 1}{\alpha}\in \Real^{(p+1)}$ such that $v = X\alpha$.
Therefore
\[X\brac{X'X}^{-1}X'v = X\brac{X'X}^{-1}X'X\alpha = X\alpha = v\]

If the columns of $X$ are orthogonal, then $X'X$ is in fact a diagonal matrix with the squared Euclidean norms of columns of $X$ on the diagonal.
In this case each of $\brac{\beta_k}_{k=0}^n$ is the coefficient in the projection of $T$ onto th respective column of $X$.

% subsection* linear_regression (end)

\subsection*{Gram-Schmidt orthogonalisation} % (fold)
\label{sub:gram_schmidt_orthogonalisation}

Suppose there is a set of linearly independent vectors $\brac{f_i}_{i=1}^m$ in some Euclidean space $V$.
\begin{description}
	\item[Initialisation] \hfill \\
		Set $e_1 \defn \frac{1}{\nrm{f_1}} f_1$;
	\item[General step $k=1,\ldots,m-1$] \hfill \\
		Let \[u_{k+1} \defn f_{k+1} - \sum_{i=1}^k \frac{\brkt{e_i, f_{k+1}}}{\brkt{e_i,e_i}} e_i\]
		and put $e_{k+1}\defn \frac{1}{\nrm{u_{k+1}}} u_{k+1}$.
\end{description}
An effective algorithm is given in \emph{Golub \& Van Loan 1996} section 5.2.8.

% subsection* gram_schmidt_orthogonalisation (end)

%% I don't recall what the section below is for...
In a bivariate case $X = (\one,x)$ and
\[\hat{\beta} = \brac{\begin{matrix} \one'\one & \one'x\\ x'\one & x'x \end{matrix}}^{-1} \brac{\begin{matrix} \one't \\ x't \end{matrix}}
= \frac{1}{n^2 \brac{\overline{x^2} - \bar{x}^2}} \brac{\begin{matrix} \one'\one \one't - \one'x x't \\ - x'\one \one't + x'x x't \end{matrix}}
\]
since $\one'\one = n$ and $x'\one = \one'x = n\bar{x}$.

%% p-predictors

Set $\hat{z}_0 \defn \one$. For each $j=1,\ldots,p$ regressing $x_k$ onto $\brac{\hat{z}_j}_{j=0}^{k-1}$ to get the LS coefficients $\brac{\alpha_{jk}}_{j=0}^{k-1}$.
Construct $\hat{z}_k$ as the residuals of $x_k$ from projecting onto $\brac{\hat{z}_j}_{j=0}^{k-1}$ :
\[\hat{z}_k = x_k - \sum_{j=0}^{k-1} \alpha_{jk} \hat{z}_j\]
This is similar to the Gram-Schmidt procedure described previously.

Regressing $t$ on $z_k$ yields the coefficient $\hat{\beta}_k$ which is not the OLS coefficient, but shows new information gained from the $k^\text{th}$ predictor.

Discriminative approach.
Since $\hat{\beta}$ is a linear transformation of $T$ and $\hat{\beta} = \beta + \brac{X'X}^{-1}X'\epsilon$, \textbf{provided the model is specified correctly}, it is therefore true that
\[\hat{\beta}\sim\Ncal\brac{\beta,\brac{X'X}^{-1} \sigma^2}\]

In fact if $\epsilon\sim \Ncal\brac{0,\Sigma}$ -- i.e the noise has some other covariance structure, then $\Var(\hat{\beta}) = \brac{X'X}^{-1}X'\Sigma X\brac{X'X}^{-1}$ -- this is useful in GLS, mentioned above, especially if $\Sigma$ is decomposed into $C'C$ with $C$ -- non-singular lower triangular matrix (Cholesky).

Indeed, if we transform (weigh) the observations according to matrix $W$ with $W'W = \Sigma^{-1}$, then the LS model becomes $WT=WX\beta + W\epsilon$, whence
\[\hat{\beta} = \brac{X'W'WX}^{-1}X'W'WT = \brac{X'\Sigma^{-1}X}^{-1}X'\Sigma^{-1}T\]
whence $\hat{\beta}\sim\Ncal\brac{\beta, \brac{X'\Sigma^{-1}X}^{-1}}$.

\noindent\textbf{Cochran's Theorem} \hfill\\
Suppose $\brac{\xi_i}_{i=1}^n \sim\Ncal(0, 1)$ and that
\[\sum_{i=1}^m \xi_i^2 = \sum_{j=1}^k Q_j\]
where $Q_j = \xi' A_j \xi$ is a positive-semidefinite quadratic form for each $j=1,\ldots,k$.
If $\sum_{j=1}^k r_j = n$, with $r_j = \rank{Q_j}$, then \begin{enumerate}
	\item The random variables $Q_j$ are independent;
	\item each $Q_j\sim \chi^2_{r_j}$.
\end{enumerate}

Returning to the simple model, what is the estimator of $\sigma^2$?

It turns out that $\frac{\RSS}{\sigma^2}\sim \chi^2_{n-p-1}$.
Indeed, provided the model is specified correctly, \begin{align*}
	\RSS &= \brac{T-X\hat{\beta}}'\brac{T-X\hat{\beta}} \\
	& = \brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon}'\brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon} \\
	& = \epsilon' \brac{I - X\brac{X'X}^{-1}X'}'\brac{I - X\brac{X'X}^{-1}X'}\epsilon \\
	& = \epsilon' \brac{ I - X\brac{X'X}^{-1}X' } \epsilon = \epsilon' \brac{ I - \hat{H} } \epsilon
\end{align*}
Since the trace of an idempotent matrix equals its rank, $\rank(I - \hat{H}) = \TR(I) - \TR(\hat{H}) = n- (p+1)$, whence by Cochran's theorem $\frac{1}{\sigma}\RSS\sim\chi^2_{n-(p+1)}$.
Furthermore, $\hat{\sigma}^2 \perp \hat{\beta}$.

\subsection{Hypothesis testing} % (fold)
\label{sub:hypothesis_testing}

If $V_j \defn \brac{\brac{X'X}^{-1}}_{jj}$ -- the $j^\text{th}$ diagonal element of $\brac{X'X}^{-1}$,
whence $\Var\hat{\beta}_j\defn \sigma^2 V_j$.

To test $H_0:\beta_j=0$ versus $H_1:\beta_j\neq 0$, one should use the following statistic, which under the null is distributed as follows:
\[\frac{\sfrac{\hat{\beta}_j}{\sqrt{\sigma^2 V_j}}}{\sqrt{ \frac{\sfrac{n-p-1}{\sigma^2} \hat{\sigma}^2 }{ n-p-1 } }} \sim \frac{\Ncal(0, 1)}{\sqrt{\chi^2_{n-p-1}}} = t_{n-p-1}\]

The $\alpha$-level confidence interval for $\beta_j$ is defined as
\[\clo{ \hat{\beta}_j - \hat{\sigma} \sqrt{ V_j },  \hat{\beta}_j + \hat{\sigma} \sqrt{ V_j } }\]

For the false rejection rate of the t-test $\alpha = 0.05$, on average $5\%$ of statistically significant results would be false positives.

To test for simultaneous insignificance of $\brac{\beta_j}_{j=1}^p = 0$ one should use the F-test
(more generally an F-test for linear restriction). The following takes place under the null hypothesis that the simpler model is correct :
\[F = \frac{\sfrac{\RSS_0-\RSS_1}{p_1-p_0}}{\sfrac{\RSS_1}{n-p_1-1}}\sim F(p_1-p_0, n-p_1-1)\]

To gauge the model accuracy one can use both the RSS as a measure of lack of fit and the $R^2$ as a measure of goodness-of-fit.

If $\one\in \clo{X}$ then
\[ \text{TSS} = T' \brac{I - \one\brac{\one'\one}^{-1}\one'} T = T' \brac{I - X\brac{X'X}^{-1}X'} T + T' \brac{ X\brac{X'X}^{-1}X' - \one\brac{\one'\one}^{-1}\one'} T = \RSS + \text{ESS}\]

the goodness-of-fit measure $R^2$ is the ratio of the explained variance to the total variance
\[R^2 \defn \frac{\text{ESS}}{\text{TSS}} = 1 - \frac{\RSS}{\text{TSS}} \]
For a simple case of a bivariate regression
\[R^2 = \rho^2 = \frac{\abs{\text{corr}(x,t)}^2}{\Var(x)\Var(t)} = \beta^2 \frac{\Var(x)}{\Var(t)}\]


% subsection hypothesis_testing (end)

% section lecture_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}
% 2015-01-20

The uncertainty on the model for a new input $x_0$ 

Prediction of the value of $T$ at a new input $x_0$ has two sources of uncertainty: \begin{enumerate}
	\item Uncertainty of the first type -- due to the estimation of $y(\cdot)$ ($\hat{\beta}$) in $\Lcal^2_X$;
	\item Uncertainty of the second type -- due to the additive noise term. Even if the true $\beta$ were known, the true values of the response $T$ at $x_0$ would still be unknown.
\end{enumerate}
Predictive intervals incorporate the noise as well as uncertainty due to imprecise estimation of the coefficients.

Variability due to estimation of an approximate model.

\subsection{Shrinkage models} % (fold)
\label{sub:shrinkage_models}

In the ordinary linear squares method we minimized the following fitness function:
\[\text{min} \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2\]

Ridge regression
Minimize the following penalized Residual Sum of Squares as:
\[\RSS\defn \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2 + \lambda \sum_{j=1}^k \beta_j^2\]
the $\lambda>0$ is called the tuning parameter.
It controls the amount of shrinkage:
if $\lambda = 0$ then there is absolutely no shrinkage, and the optimal beta coincide with the OLS.
But if $\lambda\to \infty$ then the optimal coefficient vector tends to zero. 

However with shrinkage, the scale of each predictor $\brac{x_j}_{j=0}^k\in \Real^n$ matter!

Thus the first step is to standardize the variables before fitting the Ridge regression.

Next observation: the intercept $\beta_0$ is indeed excluded from the shrinkage term.

Suppose the $t$ and $X$ are standardized (mean and centre), which means that $\beta_0 \equiv 0$
Then in the matrix form the following is minimized:
\[\brac{t-X\beta}'\brac{t- X\beta} + \lambda \beta'\beta \to \text{min}\]

The first order conditions are:
\begin{align*}
	\frac{\partial \RSS}{\partial \beta } &= -2 X\brac{t-X\beta}+2\lambda \beta\\
	% \frac{\partial \RSS}{\partial \lambda } = \beta'\beta = 0
\end{align*}

The optimal coefficients are given by
\[\hat{\beta}_\text{ridge}\defn \brac{X'X + \lambda I}^{-1}X't\]

% revise matrix derivatives.

% subsection shrinkage_models (end)

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

Limiting the size of the coefficients. Modified RSS with a tuning parameter $\lambda$.
\[\RSS\defn \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2 + \lambda \sum_{j=1}^k \beta_j^2\]
important: \begin{itemize}
	\item Consider dimensionless inputs (standardized by standard deviation) -- comparable $\beta_j$.
	\[x_{ij}\]
	\item If the input vectors are centred, then the estimation is separable.
\end{itemize}
Consider \begin{align*}
	\RSS &= \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j \brac{x_{ij}-\bar{x}_j} - \sum_{j=1}^k \beta_j \bar{x}_j}  + \lambda \sum_{j=1}^k \beta_j^2 \\
	\RSS' &= \sum_{i=1}^n \brac{t_i - \beta_0^c - \sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}  + \lambda \sum_{j=1}^k {\beta_j^c}^2
\end{align*}
Notice that $\beta_j^c = \beta_j$ for all $j=1,\ldots,n$ and $\RSS=\RSS'$.
\begin{align*}
	\RSS' &= \lambda \sum_{j=1}^k {\beta_j^c}^2 + \sum_{i=1}^n \brac{t_i - \beta_0^c}^2 \\
	& + \sum_{i=1}^n \brac{\sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}^2 \\
	& - 2 \sum_{i=1}^n \brac{t_i - \beta_0^c} \sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}
\end{align*}
Notice that the last term is zero. Indeed \begin{align*}
	\sum_{i=1}^n \brac{t_i - \beta_0^c} \brac{\sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}
	& = \sum_{j=1}^k \beta_j^c \sum_{i=1}^n \brac{t_i - \beta_0^c} \brac{x_{ij}-\bar{x}_j}
\end{align*}
The problem of ridge regression in the matrix form, where $X$ the $n\times k$
is matrix of centred standardized independent variables with no intercept:
\[\RSS = \brac{t-X\beta}'\brac{t-X\beta} + \lambda \beta'\beta\]

The solution is
\[\hat{\beta}_{\text{ridge}}\defn \brac{X'X + \lambda I}^{-1} X'T\]

An equivalent optimization problem:
\begin{align*}
	\hat{\beta}_{\text{ridge}} = \text{argmin}_{\beta} \sum_{i=1}^n \brac{t_i - \sum_{j=1}^k \beta_j x_{ij}^c}
	\text{subject to } \sum_{j=1}^k \beta_j^2 \leq \eta
\end{align*}
$\eta$ -- the budget, $\lambda$ -- the complementary slackness coefficient.

There is a certain universality of the statement of the regression problem in this form:
it is possible to plug in different constraints ($L^1$ constraints -- LASSO).
LASSO is automatically performing feature selection due to $L^1$ norm.

% How to tune $\lambda$? Wait until model selection and cross-validation.

How does the shrinkage work?
Consider two cases of columns of $X$ \begin{itemize}
	\item orthogonal
	\item correlated
\end{itemize}

\subsection*{Orthogonal columns} % (fold)
\label{sub:orthogonal_columns}

Suppose $X$ are orthogonal, in which case $X'X = I$. Therefore
the OLS solution is $\hat{\beta}_{\text{OLS}} = {X'X}^{-1}X'T = X'T$.
The ridge regression solution is given by 
\[\hat{\beta}_{\text{ridge}} = \brac{X'X + \lambda I}^{-1} X'T = \frac{1}{1+\lambda} \hat{\beta}_{\text{OLS}}\]
Hence the shrinkage!

The lasso solution:
\[\text{argmin}_\beta \brac{T - X\beta}'\brac{T - X\beta} + \lambda \sum_{j=1}^k \abs{\beta_j}\]
\begin{align*}
	\brac{T - X\beta}'\brac{T - X\beta} + \lambda \sum_{j=1}^k \abs{\beta_j}
	& = T'T - 2 \beta'X'T + \beta'X'X\beta + \lambda \sum_{j=1}^k \abs{\beta_j}\\
	& = T'T - 2 \beta'\hat{\beta}_{\text{OLS}} + \beta'X'X\beta + \lambda \sum_{j=1}^k \abs{\beta_j}
\end{align*}
The minimization problem is equivalent to the following (in this orthogonal case):
\[\text{argmin}_\beta \sum_{i=1}^k -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}\]
The problem is additively separable. Thus \begin{align*}
	\text{argmin}_{\beta_i\geq 0} -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}\\
	\text{argmin}_{\beta_i\geq 0} -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}
\end{align*}
The FOC for the first problem is given by:
\[\frac{\partial}{\partial \beta_i} = - \beta_{\text{OLS}\,i} + \beta_i + \frac{\lambda}{2}\]
whence the optimal value is given by:
\[\hat{\beta}_{\text{LASSO}\,i} \defn \brac{ \beta_{\text{OLS}\,i} - \frac{\lambda}{2} }^+\]

Due to soft thresholding in the case of the LASSO there is feature selection.

% subsection* orthogonal_columns (end)

\subsection*{The best subset selection} % (fold)
\label{sub:the_best_subset_selection}
Suppose we have a set of $p$ predictors. Chose the subset with the least
residual sum of squares.

For each $k = 1,\ldots, p$ pick the model with the lowest RSS.
\begin{align*}
\min_{\beta} \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}}^2 \\
\text{subject to} \sum_{j=1}^p 1_\obj{\beta_j\neq 0} \leq k
\end{align*}

This is equivalent to the following problem:
\[\min_{\beta} \sum_{i=1}^n \brac{ - 2 \beta_j \hat{\beta}_{\text{OLS}\,j} + \beta_j^2 }\]
subject to keeping at most $k$ coefficients nonzero.
\[\hat{\beta}_{\text{BSS}\,i} \defn \beta_{\text{OLS}\,i} 1_{\clop{\hat{\beta}, \infty}}\brac{ \abs{\beta_{\text{OLS}\,i}} }\]
where $\hat{\beta}$ is the $n-k$ th order statistic. This given hard thresholding.

Gram-Schmidt orthogonalization is equivalent to the QR decomposition.

The best idea is to use SVD decomposition of $X = U\Lambda V'$, with $U$ and $V$ orthogonal matrices.

% subsection* the_best_subset_selection (end)

\subsection*{SVD} % (fold)
\label{sub:svd}

Suppose there is a matrix $X$ with dimensions $n\times p$ with rank $r$.
SVD decomposition claims that there exist square orthogonal matrices $U_{n\times n}$ and $V_{p\times p}$
and a diagonal matrix $\Lambda_{n\times p}$ of rank $r$, such that $X = U \Lambda V'$.
Orthogonality of $U$: $U'U = I_{n\times n}$.

Orthogonality greatly simplifies the projectors onto the column space of $X$: $\hat{T} = U U' T$.

% subsection* svd (end)

\subsection*{Ridge regression and the SVD} % (fold)
\label{sub:ridge_regression_and_the_svd}

Begin with the expression of the solution:
\[\hat{\beta}_{\text{ridge}} \defn \brac{X'X + \lambda I}^{-1} X'T\]

Since $X'X = V\Lambda U' U \Lambda V' = V\Lambda^2 V'$, it is true that 
\begin{align*}
	\hat{\beta}_{\text{ridge}} & = \brac{X'X + \lambda I}^{-1} X'T\\
	& = \brac{V\Lambda^2 V' + \lambda VV'}^{-1} X'T\\
	& = V \brac{\Lambda^2 + \lambda I}^{-1} V' V\Lambda U' T \\
	& = V \brac{\Lambda^2 + \lambda I}^{-1} \Lambda U' T
\end{align*}

Therefore the ridge regression shrinks the coefficients over the tiniest principal directions.

The sample correlation matrix
\[\frac{1}{n} X'X = \frac{1}{n} V'\Lambda^2 V\]
The principal components are in $V$
the eigenvalue-eigenvector pair for the sample correlation is given by the $\brac{\frac{\lambda_i^2}{n}, V_\clo{i}}$.

The $i$ th principal component is given by $Z_i = X V_\clo{i}$ and $\text{var}(z_i) = \lambda_i^2$

The ridge regression shrinks the coefficients corresponding to the principal directions in the column space of $X$ having the smallest variance.

% subsection* ridge_regression_and_the_svd (end)

\subsection*{The effective degree of freedom} % (fold)
\label{sub:the_effective_degree_of_freedom}

It is a function of the tuning parameter $\lambda$ which governs the complexity of the model.

Complexity id the trace of the hat matrix (the projector) of the ridge regression:
\[\text{df}(\lambda) \defn \text{tr}\brac{X\brac{X'X + \lambda I}^{-1}X'}\]
Using the SVD of $X$ gives us:
\[\text{tr}\brac{X\brac{X'X + \lambda I}^{-1}X'} = \sum_{j=1}^p \frac{\lambda_i^2}{\lambda_i^2 + \lambda^2}\]

If $\lambda = 0$ then $\text{df}(\lambda) = p$, and $\lambda\to +\infty$, then $\text{df}(\lambda) \to 0$.

Correlation between this and this (?)
\begin{align*}
	\text{Var}(T) = \sigma^2 I
\end{align*}
Covariance of $\hat{T}$ and $T$.
% \[\ex(T, \hat{T}) = T' T\] 

% subsection* the_effective_degree_of_freedom (end)

Take a matrix of predictors $X$, its standardized version \[X^\sigma \defn \frac{X}{\sigma}\]
and \[X^c \defn \frac{X-m}{\sigma}\]


% section lecture_3 (end)

\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}
Departing from the linearity of the Linear regression.

Transform $\brac{f_m}_{m=1}^M:\Real^p \to \Real$ since there are initially $p$ predictors.
Sample space is augmented and now has $M$ predictors. The regression function now becomes:
\[f(X) = \sum_{m=1}^M \theta_m f_m(x)\]

This model generalises the original linear regressions since $f_m(x) = \pi_m(x)$ 

Polynomial regression problem: what happens in the tails. It is not very trustworthy near the endpoints.
In this respect polynomial regression is not a good idea -- use local regression by partitioning the range
into regions with different model in each area: $f_m(X) = 1_{\clo{l_m, u_m}}(x_k)$

Piecewise polynomials and splines

Suppose that $x$ is unidimensional -- only one feature.
%%% The following really resembles the dummy variables in ecnometrics.

Divide $x$ in several regions with different thresholds (knots since they link the regions).
To each region fit the constant. In order to cover the whole range of $x$ in this case one has to have three basis functions:

$1_{\ploc{-\infty, \xi_1}}$, $1_{\ploc{\xi_1, \xi_2}}$ and $1_{\ploc{\xi_2, \infty}}$.

Consider linear function on each region: fitting something piecewise linear. In this aces we nee 6 basis functions, since we do not only model the piecewise intercept but also the piecewise slope.

Enforcing continuity at two knots then four basis functions are only needed.

$h_1 = 1$ and $h_2 = x$ (no need for the indicator here, since it would introduce discontinuity), $h_3 = \brac{x-\xi_1}^+$ and $h_4 = \brac{x-\xi_2}^+$ -- fix their intercepts.

Overlapping region are refined into a partition, which is then estimated and the test on linear constraints of coefficients on the estimated coefficients.

To add smoothness at the knots, one has to impose continuity of the derivatives at the knots.

Three regions and two knots, require continuity up to the second derivative at the first knot.
The remaining degree of ``freedom'' at this knot is the convexity of the fitted line.
The set of basis function is $h_1 = 1$, $h_2 = x$, $h_3 = x^2$, $h_4 = x^3$, $h_5 = \brac{\brac{x-\xi_1}^+}^3$ and $h_6 = \brac{\brac{x-\xi_2}^+}^3$.


In general Splines are piecewise polynomial regression with smoothness at the binding knots:
\begin{align*}
	h_j(x) = x^{j-1}\quad i=1,\ldots, M\\
	h_{m+k}(x) = \big((x-\xi_i)_+\big)^M \quad k=1,\ldots,K
\end{align*}

One polynomial -- global and smooth.
More polynomials with a local regression are not necessarily smooth.
Splines are local and smooth!

What to do with the endpoints?
Problems with the values out the common region of $x$.

Natural cubic splines: fit a linear model in the ends to achieve robustness and get more trustworthy prediction

Order-4-spline (cubic spline) $\to$ 6 basis functions ($K+4$ functions).
Natural cubic spline $\to$ 2 basis functions ($K$ basis functions):
\[G(x) = \sum_{j=1}^K \theta_j N_j(x)\]
where $\brac{N_j}_{j=1}^K$ is the set of basis functions for the natural cubic spline.

Goal: show that it is enough to know the value of the NCS at $K$ knots to know it everywhere.

$K$ -- the number of knots $\brac{\xi_k}_{k=1}^K$. Consider a regression problem on some interval $\clo{a,b}$ with $a<\xi_1\<\ldots<\xi_K<b$.

Let $g$ be the NCS -- piecewise function, cubic in the main region, linear at the ends.
Let $g_k = g(\xi_k)$ for all $k=1,\ldots K$.

Since NCS is cubic, the second derivative must be linear in $x$ between any two consecutive knots.

Let $\gamma_k=g''(\xi_k)$. Due to linearity at the ends $\gamma_K = \gamma_1 = 0$.

$g''(x) - \alpha x + \beta$ since $g''$ is linear.
...
thus we get
\begin{matrix}
	\gamma_k = \alpha_k \xi_k + \beta_k \\
	\gamma_{k+1} = \alpha_k \xi_{k+1} + \beta_k
\end{matrix}


g(x) = \frac{(x-\xi_k)g_{k+1} + (\xi_k-x)g_k}{\xi_{k+1}-\xi_k} - \frac{1}{6}(x-\xi_k)(\xi_{k+1}-x)
g(x) = g_1 - (\xi_1-x)g'(\xi_1) & \text{if}\,x\leq \xi_1 \\
g(x) = g_K - (x-\xi_K)g'(\xi_K) & \text{if}\,x\geq \xi_K \\


There must be some relation between the coefficients since continuity across the knots is required.
g'(x) = \frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}
	-\frac{1}{6} \brac{ \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} (x-\xi_k)(\xi_{k+1}-x) }
	+ \obj{
		\brac{1+\frac{x-\xi_k}{\xi_{k+1}-\xi_k}}\gamma_{k+1}
		+ \brac{1+\frac{\xi_{k+1}-x}{\xi_{k+1}-\xi_k}}\gamma_k
	} \brac{\xi_{k+1} + \xi_k - 2 x}

g'(\xi_k+) = \frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} - \frac{1}{6} \brac{\xi_{k+1}-\xi_k}\brac{\gamma_{k+1}+2\gamam_k}
g'(\xi_{k+1}-) =\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} + \frac{1}{6} \brac{\xi_{k+1}-\xi_k}\brac{2\gamma_{k+1}+\gamam_k}
% g'(\xi_k-) =\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}

Smoothness condition: for $g$ to be an NCS $g'(\xi_k+)=g'(\xi_{k+1}-)$.

Which yields:

\[ \frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} - \frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}\]


All of the above in the matrix notation:
\[\underset{(K-2)\times K}{Q}'\underset{K\times1}{g} = \underset{(K-2)\times(K-2)}{R}\underset{(K-2)\times1}{\gamma}\]

the matrix $R$ is invertible whence its inverse exists.

% the structure of $R$ and $Q$ is on the sheet.

\noindent\textbf{Theorem}\hfill \\
The vectors $g$ and $\gamma$ fully specify the NCS if and only if $Q'g = R'\gamma$.

When this is satisfied, then 
\[\int_a^b \abs{g''(s)}^2 ds = \gamma' R \gamma = g' Q R^{-1} R R^{-1} Q g = g' K g\]

The nice observation is that this integral can be used as a penalty term in the regression problem.

\begin{align*}
	\int_a^b \abs{g''(s)}^2 ds &= \induc{\brac{g''(s)g'(s)}}_a^b - \int_a^b g'(s) g'''(s) ds \\
% Since on $(a, \xi_1]$ and $[\xi_K, b)$ the function $g$ is linear.
	&= - \int_{\xi_1}^{\xi_K} g'(s) g'''(s) ds = - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} g'(s) g'''(s) ds\\
% Due to piecewise linearity
	&= - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} g'(s) \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} ds\\
%  But $g'''$ is constant over each interval
	&= - \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} \int_{\xi_k}^{\xi_{k+1}} g'(s) ds\\
%  Since \int_{\xi_k}^{\xi_{k+1}} g'(s) ds = g(\xi_{k+1}) - g(\xi_k)
	&= - \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} \brac{\gamma_{k+1}-\gamma_k}\\
	&= - \sum_{k=2}^{K-2} \brac{\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} - \frac{g_k-g_{k-1}}{\xi_{k+1}-\xi_k}} \gamma_k \\
	& = \gamma' Q' g = \gamma' R \gamma
\end{align*}

Putting things in other words:
the NCS has the minimum value of the smoothness integral among all smooth curves interpolating the data.

This means that formally we have the following:
with at leas two knots and $g$ being the NCS interpolating $z_1,\ldots,z_K$ on knots $\xi_1,\ldots,\xi_K$ within a bounded interval.

Then for any other interpolating function on the same mesh $\bar{g}$, the total variation of $\bar{g}''$ is necessarily grater than that of the NCS.


The function $h \defn g - \bar{g}$ has root at the knots $\xi_k$
Consider the difference of these integrals. 
\begin{align*}
	\int_a^b h'' g'' ds
%% Integrate by part using the nice properties of g 
	& = \brac{h'(s)g''(s)}_a^b - \int_a^b h'(s) g'''(s) ds \\
%% since g''' is zero outside \clo{\xi_1, \xi_K} and g'' is zero at the endpoints of the interval \clo{a,b}
	& = - \int_{\xi_1}^{\xi_K} h'(s) g'''(s) ds \\
	& = - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} h'(s) g'''(s) ds \\
	& = - \sum_{k=1}^{K-1}  g'''(\xi_k+) \int_{\xi_k}^{\xi_{k+1}} h'(s) ds \\
	& = - \sum_{k=1}^{K-1}  g'''(\xi_k+) \brac{h(\xi_{k+1})-h(\xi_k)} = 0
\end{align*}


\int_a^b \abs{\bar{g}''}^2 ds &= 
\int_a^b \abs{g'' + h''}^2 ds = \int_a^b \abs{g''}^2 + \abs{h''}^2 ds + \int_a^b 2h''g'' ds= \int_a^b \abs{g''}^2 + \abs{h''}^2 ds \geq \int_a^b \abs{g''}^2 ds

\begin{itemize}
	\item the NCS on $\clo{a,b}$ is completely determined by the levels and derivatives at the interpolation knots;
	\item The NCS brings the value of the $\int_a^b \abs{g''}ds$ to its minimum in the class of all smooth curves interpolating the data at the same knots;
	\item The NCS with $K$ knots has $K$ basis functions.
\end{itemize}


Cubic spline with $K$ knots
\[f(x) = \sum_{k=1}^3 \beta_k x^k + \sum_{k=1}^K \theta_k \brac{\brac{x-\xi_k}^+}^3\]

% See the handwritten notes 


\subsection{Smoothing spline} % (fold)
\label{sub:smoothing_spline}

The goal of fitting a smoothing spline is among all functions $g(x)$ with two continuous derivatives, to find the one that minimizes the following penalized sum of squares
\[\RSS(g,\lambda) = \sum_{i=1}^n \brac{t_i - g(x_i)}^2 + \lambda \int \abs{g''(s)}^2 ds\]
Trying to make $g$ fit the data s much as possible and yet penalizing if it varies too much. This is basically a problem of calculus of variations. The penalty term sums up all non-linearities in the function $g$ and is higher the more oscillations the $g$ has.

$\lambda\to \infty$ then the optimal solution is linear $g$
$\lambda = 0$ then the optimal solution is some ``wild'' function, matching all the points, with no structure known beforehand.


This seems to be a problem of functional analysis.

consider the function $g(\xi_k) = \bar{g}(\xi_k)$ for all $k$.
Consider as $\bar{g}$ an NCS at knots given by $\brac{\xi_k}_{k=1}^K$.
Since an NCS has the lest variation integral, the solution of the problem cannot be anything but the NCS.
\[\int \abs{g''(s)}^2 ds\geq \int \abs{\bar{g}''(s)}^2 ds\]

Thus the optimal solution to this infinite dimensional problem is a quite finite dimensional NCS.
% Surprise
Therefore the problem reduces to the following quadratic optimization problem:
\[\RSS(g,\lambda) = {(t - g)}'{(t - g)} + \lambda g'K g \to \min_g\]
\[\RSS(g,\lambda) = t't - g't - t'g + g'( I + \lambda K )g  \to \min_g\]
using the matrix derivatives, the solution turns out to be
\[\frac{\partial}{\partial g} \quad:\quad 2( I + \lambda K )g - 2 t = 0\]
which reduces to
\[\hat{g} = \brac{I + \lambda K}^{-1} t\]

The matrix $\brac{I + \lambda K}^{-1}$ is called the \textbf{smoother matrix} $S_\lambda$.

Where does the smoothing take place? 

Define the effective degree freedom as, again, the trace of $S_\lambda$.
\[\text{df}(\lambda_ = \tr\brac{S_\lambda}\]

Use the SVD of $S_\lambda$ to extract the eigenvalues and eigenvectors.

Since $K \defn Q R^{-1} Q'$, whence $K$ is symmetric. The unit matrix is symmetric, hence $S_\lambda$ is symmetric and its inverse is symmetric.
% Thus it is an Hermitian matrix, whence the eigenvalues must be real.
\[S_\lambda = \sum_{i=1}^n \lambda_i u_i u_i' = U\Lambda U'\]

Since  $S_\lambda$ is positive-semidefinite, all eigenvalues are non-negative.

\begin{itemize}
	\item The eigenvalues of $K$ do not depend on the parameter $\lambda$;
	\item Both $S_\lambda$ and $K$ have the same eigenvectors.
\end{itemize}

If $\eta$ -- the eigenvalues of $K$, then $\text{det}\brac{K-\eta I} = 0$.
Thus \[\text{det}\brac{\frac{1}{\lambda}\brac{\brac{I+\lambda K} -(1 + \lambda \eta) I} } = 0\]

Thus $1 + \lambda \eta$ are the eigenvalues of $\brac{I+\lambda K}$, and $\frac{1}{1 + \lambda \eta}$ is the eigenvalue of $S_\lambda$.
Thus $\lambda_j = \frac{1}{1+\lambda \eta_{n-j+1}}$ is a decreasing function of $\lambda$.

Consider a $u_j$ the eigenvector of $S_\lambda$ corresponding to $\lambda_j$.
Then $S_\lambda u_j = \lambda_j u_j$ and
\[u_j = \lambda_j \brac{S_\lambda}^{-1} u_j
= \lambda_j \brac{I+\lambda K} u_j
= \lambda_j u_j + \lambda_j \lambda K u_j\]
Whence
\[K u_j = \frac{1-\lambda_j}{\lambda \lambda_j} u_j = \eta_{n-j+1} u_j\]
therefore the eigenvectors of $S_\lambda$ are the eigenvectors of $K$ (with other eigenvalues).


Thus due to the spectral decomposition:
\[g = S_\lambda t = \sum_{j=1}^n \lambda_j u_j u_j' t = \sum_{j=1}^n \lambda_j \brkt{u_j, t} u_j \]

Since each eigenvector $u_j$ is independent of the $\lambda$, the projections on are invariant under $\lambda$. Therefore models are comparable.

\[\sum_{j=1}^n \frac{1}{1+\lambda\eta_{n-j+1}} \brkt{u_j, t} u_j\]
some smoothing take place

The higher the $j$ the more smoothing take place:
	the less principal component is, the more smoothed it is.

\[g = \lambda_1\brkt{u_1, t} u_1 + \lambda_2\brkt{u_2, t} u_2 + \sum_{j=3}^n \lambda_j \brkt{u_j, t} u_j \]
The nature of the matrix $K$ makes the first $\lambda_1$ and $\lamba_2$ equal to $1$.

What is the relationship between $x$ and $u_1$?

The smoothing spline does not shrink linear components.

Higher $u_j$ correspond to more and more oscillating functions of $x$.

% Reproducing Hilbert spaces



\begin{itemize}
	\item
\end{itemize}

% subsection smoothing_spline (end)
% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

\[\RSS = \sum_{i=1}^n \brac{t_i - g(x_i)}^2 + \lambda \int \abs{g''(s)}^2ds\]
The solution to this problem is the Natural cubic spline , which is finite dimensional reduces the problem to
\[\brac{t-g}'\brac{t-g} + \lambda g'K g\]
where $K$ is a symmetric and positive  semi-definite matrix
the fitted $\hat{g}$ are determined by the 
\[S_\lambda = \brac{I+\lambda H}^{-1}\]

The smoother matrix $S_\lambda$ is positive semi-definite and has eigenvalues $\lambda_k$. Why does $\lambda_1$ and $\lambda_2$ are equal to $1$?

%% See handwritten notes

What happens when the soother is applied again to the smoothed output?
In the case of a linear regression we get the same output.

Re-applying the smoother to $\hat{g}$ yields a more linear fit, since the first pair of eigenvalues are unit, thus preserving the linear components, while other higher degree polynomial components diminish.

Indeed, \begin{align*}
	\hat{g}_2 & = \sum_{k=1}^n \lambda_k \brkt{\hat{g}, u_k} u_k \\
	& = \sum_{k=1}^n \lambda_k \brkt{\sum_{j=1}^n \lambda_j \brkt{g, u_j} u_j, u_k} u_k \\
	& = \sum_{k=1}^n \sum_{j=1}^n \lambda_j \lambda_k \brkt{g, u_j} \brkt{u_j, u_k} u_k \\
	& = \sum_{k=1}^n \sum_{j=1}^n \lambda_j \lambda_k \brkt{g, u_j} \delta_{kj} u_k \\
	& = \sum_{k=1}^n \lambda_k^2 \brkt{g, u_k} u_k \\
	& = \sum_{k=1}^n \lambda_k^2 \brkt{g, u_k} u_k
\end{align*}

The projection matrix is idempotent, hence for any eigenvector-eigenvalue pair $(\lamba, u)$ it is true that $\lamba u = Hu = H^2u = H\lamba u = \lamba Hu = \lamba^2 u$. Thus $\lamba$ is either $\pm1$ or $0$.

The solution to the weighted problem is the same Natural Cubic spline.
\[\sum_{i=1}^n \omega_i \brac{t_i - g(x_i)}^2 + \lamba \int \abs{g''(s)}^2 ds \tom \min\]
Since again, we can take an NCS with the same first sum but with lesser penalty term.

The solution is $\hat{g} = \brac{W+\lamba K}^{-1} Wt$.

\subsection{Model selection} % (fold)
\label{sub:model_selection}

%% See the handwritten notes

Four adjustments to the likelihood, which incorporate the complexity of the model.

Elements of information theory

Suppose $x$ is a discrete random variable.
The main question is how much information one gets when one observes a realisation of $X$.

THe amount of information is quantified by the amount of ``'surprise'':\begin{itemize}
	\item very likely -- not surprising;
	\item very unlikely -- very surprising.
\end{itemize}

$H$ the measure of information has the following properties: \begin{description}
	\item[Monotonicity] $H$ is a monotonic functional of the probability measure function;
	\item[Independence] If events are independent or unrelated, then the information gained from both must be poled together: in the case of densities this means that $H(X,Y) = H(X)+H(Y)$ when $p_{X,Y}(x,y) = p_X(x) p_Y(y)$;
	\item[]
\end{description}

Average amount of information for a discrete random variable
\[H(X) = \Ex\brac{H(X)} = - \sum_x p(x) \log_2 p(x)\]
for a continuous:
\[H(X) = \int \log_2\frac{1}{p(x)} p(x) dx\]

Suppose the $f$ is unknown distribution modelled using $\hat{f}$.

The Kullback-Leibler \textbf{divergence} between $f$ and $\hat{f}$: the additional information need to describe the random variable with $f$ using $\hat{f}$: \[\int f(x) \log_2\frac{1}{\hat{f}(x)} dx - \int f(x) \log_2\frac{1}{f(x)} dx \leq 0 \]

Use Jensen's inequality $\Ex g(X) \geq g\brac{\Ex X}$ for any convex $g$.
\begin{align*}
	-\int f(x) \log_2\frac{\hat{f}(x)}{f(x)} dx & \geq -\log_2\int f(x) \frac{\hat{f}(x)}{f(x)}  dx = 
\end{align*}

First criterion of model selection (Akaike; 1973).

Suppose $\brac{Y_i}_{i=1}^n$ is some data generate according to some unknown density $g\brac{\induc{y}\theta_0}$. Let there be a parametric family of models
\[M_K \defn \obj{\induc{f\brac{\induc{y}\theta_k}}\,\theta_k\in \Theta}\]
where $\Theta\subseteq \Real^K$ -- the $K$-dimensional parametric family.

Let $\hat{\theta}_k$ be the MLE estimator of the parameter in $\Theta$, and $\hat{f}_k = f\brac{\induc{y}\theta_k}$.

Assume that the models are nested, and are distinguished by their dimension $K$.

How far the $\hat{f}_k$ is from $g$.

The idea is to look a t the Kullback-Leibler divergence between $g$ and $f_k$, and get the $f_k$ with the smallest distance:
\[\text{KL}(g||f_k) = \int g(y) \log g(y) dy - \int g(y) \log f_k(y) dy\]
is the sum of the inherent entropy of $Y$ and the cross entropy.

The problem is to minimize this function, and it is equivalent to minimizing the Kullback-Leibler \textbf{discrepancy}:
\[d(\theta_k) = -2\int g(y) \log f_k(y) dy = - 2 \Ex_g\brac{\log f_k(Y)}\]

Instead	let's look at
\[d(\hat{\theta}_k) = - 2\induc{\Ex_g\brac{\log f(\induc{y}\theta)}}_{\theta=\theta_k(y)} \]

Since in practice $g$ is unknown, Akaike suggested to use the log likelihood as a proxy for $d(\hat{\theta}_k)$, which is a biased estimator of it.

In fact, using the law of large numbers
\[\frac{1}{N} \sum_{i=1}^n \log f(Y_i|\hat{\theta}_k) \overset{a.s}{\to} \Ex_g\brac{\log f(\induc{Y}\hat{\theta}_k)}\]

Therefore
\[-\frac{2}{N} \sum_{i=1}^n \log f(Y_i|\hat{\theta}_k) \approx d(\hat{\theta}_k)\] and Akaike has something great in mind.

In order to compute the bias, it is necessary look an the average Kullback-Leibler distance with respect to the distribution of the MLE estimate $\hat{\theta}_k$
\[\Ex_{\hat{\theta}_k} d(\hat{\theta}_k) - \Ex_g\brac{- \log f\brac{\induc{Y}\hat{\theta}_k(Y)}}\]

The AIC is given by (up to the terms of the first order):
\[-2\log \hat{f}\brac{\induc{y}\,\hat{\theta}_k(y)} + 2k\]

%% See the handwritten notes!

% subsection model_selection (end)

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

Akaike criterion combines parameter estimation and model selection

By looking at the Kullback-Leibler divergence between the estimated and the true

\[d(\theta_k) = - 2 \Ex_g \ln f\brac{\induc{Y}\theta_k}\]

$g(y\vert \theta_0)$

\[d(\hat{theta}_k) = - 2 \induc{\Ex \ln f\brac{\induc{Y}\theta_k}}_{\theta_k = \hat{\theta}_k(y)}\]

\[\Ex_{\hat{\theta}_k} d(\hat{theta}_k) = - 2 \induc{\Ex_z \Ex_Y \ln f\brac{\induc{Y}\theta_k}}_{\theta_k = \hat{\theta}_k(z)}\]

The idea is to use the sample twice (law of Large Numbers)

What is the bias here?

Law of Large numbers
\[-2 \ln f\brac{\induc{y}\hat{\theta}_k(y)} \to \Ex \]

Modified (Corrected) AIC

Akaike's information criterion in which the contribution of the error term is computed.
This way it is possible to relax the assumption of large sample size $n$.

Look at what happens in the normal linear regression setting.

AIC is is more general, but the bias of the estimate can be really large
$\text{AIC}_c$ requires the model class assumption -- restrictive, but offers greater precision.

\subsection{Derivation} % (fold)
\label{sub:derivation}
Assumptions \begin{description}
	\item[the true model]\hfill\\
	\[y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon\]
	where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$
	\item[the candidate model] \hfill\\
	\[y = \underset{n\times p}{X}\underset{p\times 1}{\beta_0} + \epsilon\]
	where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$.
	\item[Nested models] assume that $0\leq p_0 \leq p$ and $\beta_0$ has $(p-p_0)$ zeros at the end. The models are nested $f\brac{\induc{y}\theta_0}\in \Mcal_k$
\end{description}

The LS estimate of $\Mcal_k$:\begin{align*}
	\hat{\beta}_k &= \brac{X'X}^{-1} X'Y\\
	\hat{\sigma}^2_k &= \frac{1}{n}\brac{Y-X\hat{\beta}_k}'\brac{Y-X\hat{\beta}_k}
\end{align*}


the log likelihood is 
\[\ln f\brac{\induc{y}\theta} = -\frac{n}{2} \ln 2\pi -\frac{n}{2}\ln \sigma^2_k - \frac{1}{2\sigma^2_k} \brac{Y-X\beta_k}'\brac{Y-X\beta_k}\]

For $\theta_k$ this can be simplified to 
\[\ln f\brac{\induc{y}\hat{\theta}_k(y)} = -\frac{n}{2} \ln 2\pi -\frac{n}{2}\ln \hat{\sigma}^2_k - \frac{1}{2\hat{\sigma}^2_k} n \hat{\sigma}^2_k \]
\[\ln f\brac{\induc{y}\hat{\theta}_k(y)} = -\frac{n}{2} \brac{ 1 + \ln 2\pi} -\frac{n}{2}\ln \hat{\sigma}^2_k\]

However
\[-2\ln f\brac{\induc{y}\theta_k} = n \ln 2\pi +n\ln \sigma^2_k + \frac{1}{\sigma^2_k} \brac{Y-X\beta_k}'\brac{Y-X\beta_k}\]

Since 
\begin{align*}
	\brac{Y-X\beta_k}'\brac{Y-X\beta_k} &= \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + \\
		& + \brac{\beta_0 - \beta_k}'X'\epsilon + \epsilon'X\brac{\beta_0 - \beta_k}
		& + \epsilon'\epsilon
\end{align*}

Therefore 
\[\Ex-2\ln f\brac{\induc{y}\theta_k} = n \ln 2\pi + n\ln \sigma^2_k + \frac{1}{\sigma^2_k} \brac{ \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + n\sigma_0^2 }\]

Now 
\[d\brac{\hat{\theta}_k} = n \ln 2\pi + n\ln \hat{\sigma}^2_k + \frac{1}{\hat{\sigma}^2_k}\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k} + \frac{n\sigma_0^2}{\hat{\sigma}^2_k}\]

taking expectations
\[\Ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\Ex \ln \hat{\sigma}^2_k + n^2\Ex \frac{\sigma_0^2}{n\hat{\sigma}^2_k} + n \Ex \frac{1}{n\hat{\sigma}^2_k}\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}\]

Since $\frac{n\hat{\sigma}^2_k}{\sigma_0^2} \sim \chi^2_{n-p}$ and 
\[\brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}{\sigma_0^2} \sim \chi^2_p\]
and are independent

Hence
\[\Ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\Ex \ln \hat{\sigma}^2_k + \frac{n^2}{n-p-2} + n \Ex \frac{1}{n\hat{\sigma}^2_k} \Ex \brac{\beta_0-\beta_k}'X'X\brac{\beta_0-\beta_k}\]
and thus 
\[\Ex d\brac{\hat{\theta}_k} = n \ln 2\pi + n\Ex \ln \hat{\sigma}^2_k + \frac{n^2}{n-p-2} + \frac{n p}{n-p-2} \]

Whence 
\[\Ex d\brac{\hat{\theta}_k} = n \brac{1+\ln 2\pi} + n\Ex \ln \hat{\sigma}^2_k + \frac{2n(p+1)}{n-p-2}\]
\[\Ex d\brac{\hat{\theta}_k} = \Ex -2 \ln f\brac{\induc{y}\hat{\theta}_k} + \frac{2n(p+1)}{n-p-2}\]

Since 
\[\frac{2n(p+1)}{n-p-2} = \frac{2nk}{n-k-1} = 2k + \frac{2k(k+1)}{n-k-1}\]

the original AIC is
\[\text{AIC} = -2 \ln f\brac{\induc{y}\hat{\theta}_k(y)} + 2k\]

The corrected AIC is 
\[\text{AIC}_c = \text{AIC} + \frac{2k(k+1)}{n-k-1}\]

% subsection derivation (end)

\subsection{Mallow's $C_p$} % (fold)
\label{sub:mallows_cp}

The $C_p$ stands for the conceptual predictive statistic in the context of the multivariate regression.

Assumptions \begin{description}
	\item[the true model]\hfill\\
	The true model is $y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma_0^2 \underset{n\times 1}{E}}$;
	\item[the candidate model] \hfill\\
	The candidate is $y = \underset{n\times p}{X}\underset{p\times 1}{\beta} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$. THe MLE of $\theta_k$ is $\hat{\theta}_k$.
	\item[the density] $f\brac{\induc{y}\hat{\theta}_k} = f_k$.
\end{description}

The idea is to look athow the RSS behaves

\[\RSS = \sum_{i=1}^n \brac{y_i - x_i \hat{\beta}}^2 = \brac{Y-X\hat{\beta}}'\brac{Y-X\hat{\beta}} = Y'\brac{E-H}Y\]
where $H$ is the projection matrix $X\brac{X'X}^{-1}X'$.

The expected value of the RSS is given by
\begin{align*}
	\Ex Y'\brac{E-H}Y &= \Ex\tr{Y'\brac{E-H}Y} \\
		&= \beta_0'X_0' \brac{E-H} X_0\beta_0 + \tr\brac{\brac{E-H} \sigma^2_0 I}\\
		&= \beta_0'X_0' \brac{E-H} X_0\beta_0 + (n-p)\sigma^2_0
\end{align*}

Show that if $z\sim \Dcal_m\brac{\mu,\Sigma}$ and $A$ is a non-random $m\times m$ matrix, then
\[\Ex\brac{z'Az} = \mu'A\mu + \tr\brac{A\Sigma}\] 

Define the $C_p^0$ as
\[C_p^0 = \frac{\RSS}{\sigma^2_0} - n + 2p\]

The expectation is 
\[\Ex C_p^0 = p + \frac{1}{\sigma^2_0} \beta_0'X_0' \brac{E-H} X_0\beta_0\]

If the true model has bee selected then $\Ex C_p^0 = p$, otherwise $\Ex C_p^0 > p$.

Put $C_p$ to $\frac{\RSS}{\hat{\sigma}^2} - n + 2p$, with the hope, that it leads to the same expectation of $C_p^0$.

But which estimator of the reference $\sigma^2$ to use? Use the smallest -- for the full model
\[\hat{\sigma}^2_*\frac{1}{n-p} \sum_{i=1}^n\brac{y_i - X_i\hat{\beta}}^2\]
where $p$ is the number of parameters in the full model.

% subsection mallows_cp (end)

\subsection{Comparison between the $C_p$ and $\text{AIC}_c$} % (fold)
\label{sub:comparison_between_cp_and_aicc}

These criteria are asymptotically equivalent.

The AIC for (the biased estimator)
\[\text{AIC} = - \ln f\brac{\induc{y}\hat{\theta}_k} + 2k \\
	&= n \ln \hat{\sigma}^2 + n\brac{1 + \ln 2\pi} + 2(p+1)\]

Return the model with the minimum values of the AIC. For a fixed $n$ it is the same a looking at 
\[n\ln \hat{\sigma}^2 - n\ln \hat{\sigma}^2_* + 2 p \]

But this minimization is equivalent to minimization of the following 
\[ n\ln \frac{\hat{\sigma}^2}{\hat{\sigma}^2_*} + 2p\]

For large $n$ the ratio of $\hat{\sigma}^2$ to $\hat{\sigma}^2_*$ is approximately $1$ due to consistency.
Hence it is possible to expand the logarithm around $1$: $\ln(1+x)\approx x + o(x)$.

Hence
\[\approx b\brac{\frac{\hat{\sigma}^2}{\hat{\sigma}^2_*} - 1} + 2p = C_p\]

\textbf{Exercise}\\
Prove the expected Gauss discrepancy
\[\Ex C_p^0 = \Ex \frac{1}{\sigma^2_0}\brac{X_0\beta_0 -X \beta}'\brac{X_0\beta_0 -X \beta}\]
Indeed, take a look at the Gauss discrepancy:
\begin{align*}
	\brac{X_0\beta_0 -X \beta}'\brac{X_0\beta_0 -X \beta} &= \brac{X_0\beta_0 - X \brac{X'X}^{-1}X'X_0\beta_0 - X \brac{X'X}^{-1}X'\epsilon }'\brac{ \ldots }\\
	&= \brac{ (I - H )X_0\beta_0 - H \epsilon }'\brac{ (I - H )X_0\beta_0 - H \epsilon }\\
	&= \beta_0'X_0'(I-H)X_0\beta_0 - \epsilon'H(I-H)X_0\beta_0 - \beta_0'X_0'(I-H)H \epsilon + \epsilon'H\epsilon\\
	&= \beta_0'X_0'(I-H)X_0\beta_0 + \epsilon'H\epsilon
\end{align*}
Since $I-H$ and $H$ are projectors onto orthogonal subspaces of $\Real^n$.

Taking expectations yields
\[\Ex\brac{\beta_0'X_0'(I-H)X_0\beta_0 + \epsilon'H\epsilon} = \beta_0'X_0'(I-H)X_0\beta_0 + \Ex\brac{\epsilon'H\epsilon}\]
while at the same time due to linearity of $\Ex$ and $\tr$:

\[\Ex(\epsilon'H\epsilon) &= \tr\Ex(\epsilon'H\epsilon) = \Ex\tr(\epsilon'H\epsilon) \\
&= \Ex\tr(H\epsilon\epsilon) = \tr(H\Ex\epsilon\epsilon)\]

% subsection comparison_between_cp_and_aicc (end)

\subsection{The Bayesian information criterion} % (fold)
\label{sub:the_bayesian_information_criterion}

Also known as the \textbf{B}ayesian \textbf{I}nformation \textbf{C}riterion introduced by Schwartz as the competitor of the AIC.

This looks at the problem from the Bayesian point of view.

Assumptions \begin{description}
	\item[Data] $\brac{y_k}_{k=1}^n$;
	\itme[Parametric] The distribution function is fully specified by the vector of parameters $\theta_k\in \Theta_k$.
	\item[the true model]\hfill\\
	The true model is $y = \underset{n\times p_0}{X}\underset{p_0\times 1}{\beta_0} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma_0^2 \underset{n\times 1}{E}}$;
	\item[the candidate model] \hfill\\
	The candidate is $y = \underset{n\times p}{X}\underset{p\times 1}{\beta} + \epsilon$, where $\epsilon \sim \Ncal_n\brac{0,\sigma^2 \underset{n\times 1}{E}}$.
	There are $k=p+1$ parameters to estimate: $\theta_0 = \brac{\beta_0,\sigma_0^2}$ and $\theta_k = \brac{\beta_k,\sigma_k^2}$. THe MLE of $\theta_k$ is $\hat{\theta}_k$.
	\item[the density] $f\brac{\induc{y}\hat{\theta}_k} = f_k$.
\end{description}

\begin{itemize}
	\item There is prior distribution $\pi_k$ on the classes of models $\Mcal_k$;
	\item Each model class has a prior distribution $G_k\brac{\theta_k}$ on the true parameter $\theta_k$;
	\item 
\end{itemize}
The bayes formula yields the following posterior distribution of $\Mcal_k$ given $\mathbf{y}$:
\[p\brac{\induc{\Mcal_k}\mathbf{y}} \propto p\brac{\induc{\mathbf{y}}\Mcal_k} \pi_k\]
up to the factor $\frac{1}{f(\mathbf{y})}$.

Also
\[p\brac{\induc{\mathbf{y}}\Mcal_k}
= \int p\brac{\induc{\mathbf{y}, \theta_k}\Mcal_k} d\theta_k
= \int p\brac{\induc{\mathbf{y}}\theta_k, \Mcal_k} p\brac{\induc{\theta_k}\Mcal_k}d\theta_k
= \int \induc{L}_{\Mcal_k}(\theta_k;\mathbf{y}) g_k(\theta_k) d\theta_k\]

\subsection{Laplace approximation} % (fold)
\label{sub:laplace_approximation}

Idea is to find a Gaussian approximation to a probability density defined over a set of continuous variables.

Univariate case
\[p(z) = \frac{1}{N_z}f(z)\]
where $N_z$ is the normalizing constant, and $f(z)$ is unimodal.
The idea is to approximate $f(z)$ with gaussians $q(z)$.

Consider the Taylor expansion of the logarithm of $f(z)$ around its mode $z_0$, given by $f'(z_0) = 0$.
\[\ln f(z) \approx \ln f(z_0) + \induc{\frac{f'(z)}{f(z)}}_{z=z_0} (z-z_0) - \frac{1}{2} \induc{\frac{d^2}{dz^2} \ln f(z)}_{z=z_0} \brac{z-z_0}^2 \]
the reminder term decays to zero faster than $\brac{z-z_0}^2$. Thus $f(z)\approx e^{ -\frac{1}{2} A {(z-z_0)}^2 }$.

Whence around $z_0$:
\[q(z) = \brac{\frac{A}{2\pi}}^\frac{1}{2}e^{ -\frac{1}{2} A {(z-z_0)}^2 }\]

In the multidimensional case for $A = \induc{-\frac{d^2}{dz'dz}\ln f(z)}_{z=z_0}$
\[q(\mathbf{z}) = \frac{\abs{A}^\frac{1}{2}}{{(2\pi)}^\frac{1}{2}} e^{-\frac{1}{2} (z-z_0)'A(z-z_0)}\]

Therefore
\[N_z = \int f(z) dz = f(z_0) \frac{{(2\pi)}^\frac{1}{2}}{\abs{A}^\frac{1}{2}}\]

% subsection laplace_approximation (end)

The integrand
\[f(\theta_k) = p\brac{\induc{y}\theta_k}g_k(\theta_k)\]

Using the Laplace approximation we can treat the integrand as a normal density.

Let's compute the Laplace approximation.

Let $\theta_k^* = \text{argmax}_{\theta_k\in \Theta_k}\,f(\theta_k)$ and put $A_{\theta_k^*}$ as
\[\induc{- \frac{d^2}{d\theta_kd\theta_k'} \ln f(\theta_k)}_{\theta_k=\theta_k^*}\]

Could be ignore the contribution coming from $g_k$?

The log of
\[\ln p\brac{\induc{\mathbf{y}}\theta_k} = \sum_{i=1}^n p\brac{\induc{y_i}\theta_k}\]
which hints at the fact that $P\brac{\induc{\mathbf{y}}\theta_k}$ dominates the $\ln g_k(\theta_k)$.

Thus we can take $\theta_k^* = \hat{\theta}_k$ -- the MLE and the observed Fisher information matrix
\[I_{\hat{\theta}_k} = \induc{- \frac{\partial^2}{\partial\theta_k \partial\theta_k'}\ln p\brac{\induc{\mathbf{y}} \theta_k}} \]

In fact it is possible to have better Laplace approximations of $\int e^{n\phi(z)}dz = \text{approx} + o(n^{-\frac{1}{2}})$.

The posterior distribution is given by 
\[p\brac{\induc{\mathbf{y}}\Mcal_k} = \int f(\theta_k) d \theta_k = \int p\brac{\induc{\mathbf{y}}\theta_k} g_k(\theta_k) d \theta_k \approx p\brac{\induc{\mathbf{y}}\hat{\theta}_k} g_k(\hat{\theta}_k) \frac{\brac{2\pi}^\frac{k}{2}}{\abs{I_{\hat{\theta}_k}}^\frac{1}{2}}\]
the exponent $\frac{k}{2}$ comes from the dimension of $\theta_k$.

Therefore
\[\ln p\brac{\induc{\mathbf{y}}\Mcal_k} = \ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \frac{1}{2}\ln \abs{I_{\hat{\theta}_k}} \]

For independent observations the full information matrix is just the one-observation information matrix times the number of observations.
Thus 
\[\abs{I_{\hat{\theta}_k}} = n^k \abs{I_1}\]

Hence 
\[\ln p\brac{\induc{\mathbf{y}}\Mcal_k} = \ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \frac{k}{2}\ln n - \ln \abs{I_1} \]

But the terms 
\[ + \ln g_k(\theta_k) + \frac{k}{2} \ln 2 \pi - \ln \abs{I_1}\]
are bound with regardless of the values of $n$ and moreover are independent of it!

Therefore for asymptotic model comparison we can use
\[\text{BIC} = -2\ln f\brac{\induc{\mathbf{y}}\hat{\theta}_k} + k\ln n \]

The BIC tends to favour simpler models, because the penalty term is larger as $n\to \infty$.
In Bayesian modelling one needs to accumulate larger evidence in order to make estimation.
Uncertainty comes from the estimation and from the uncertainty of the underlying parameter.


The BIC is consistent
If the true model is in $\obj{\Mcal_{k_1},\ldots,\Mcal_{k_L}}$ then the the minimal BIC pick the true model with probability 1 as $n$ gets larger.

Heuristics on the values of BIC $\text{BIC}-\text{BIC}_{\min}$
\begin{itemize}
	\item if $\in\clo{0,2}$ -- virtually no evidence against;
	\item if $\in\clo{2,6}$ -- the evidence id positive;
	\item if $\in\clo{6,10}$ -- the evidence is strong;
	\item if $\in\clop{10, +\infty}$ -- the evidence is very strong.
\end{itemize}

% Kass & Raffey (1995)

% subsection the_bayesian_information_criterion (end)

% section lecture_6 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

The goal is to estimate the test error given the small sample we have.

\subsection{Validation techniques} % (fold)
\label{sub:validation_techniques}

Enough data : split the data into two samples and fit on one subsample and test (validate) in the other.

Hold out a set of points in the training set $\brac{x_{2k}}_{k=1}^{n_2}$ on which to test the error of the model.
On the remaining subsample $\brac{x_{1k}}_{k=1}^{n_1}$ fit the model. The splitting is done randomly.
However there are some shortcomings:
\begin{description}
	\item[Variability] \hfill\\
		Depends on which observations are put in both subsamples ;
	\item[Test error] \hfill\\
		The true error might be overestimated due to having less data to estimate the model.
\end{description}

\subsubsection{Leave one out cross validation} % (fold)
\label{ssub:leave_one_out_cross_validation}

The idea is instead of splitting the dataset in two subsets, we test the fit on one single observation.

Is the full training sample is $X=\brac{x_k}_{k=1}^n$, then for each $j=1,\ldots n$ fit the model on $X_{-j} = \brac{X_k}_{k=1,\neq j}^n$.
Let $\hat{T}_j$ be the prediction of the model fit on $X_{-j}$, then the \textbf{M}ean \textbf{S}quare \textbf{E}rror is 
\[\text{MSE} \defn \frac{1}{n}\ \sum_{j=1}^n \big(\hat{T}_j - T_j\big)^2 \]

\begin{description}
	\item[Pros] \hfill\\
		\begin{itemize}
			\item Very general and applicable in many settings;
			\item It is not variable at all -- yields the same results, since the splitting is deterministic;
			\itme Compared to validation, the bias of the model error is reduced;
		\end{itemize}
	\item[Cons] \hfill\\
		\begin{itemize}
			\item The variance of the MSE is increased, since there is a substantial overlap in the subsamples;
			\item It inflates the computational complexity (not true for linear estimators -- a linear combination of the response variables);
			\[\text{MSE} = \frac{1}{n}\sum_{j=1}^n \Big(\frac{y_j - \hat{y}_j}{1-h_{jj}}\Big)^2\]
			where $h_{jj}$ is the $j$-th diagonal element of the hat matrix. It is also known as the \emph{leverage}.
			\item Unsuitable for non-independent data.
		\end{itemize}
\end{description}

Covariance function of the residuals $\epsilon = Y - H Y$ is given by 
\[\text{cov}\big(\hat{\epsilon}\big) = \Ex\big( (I-H)YY'(I-H) \big) = \sigma^2 (I-H)\]

Thus $\Var(\hat{\epsilon}_j) = \sigma^2 (1-h_{jj})$.

And we arrive at the concept of normalised residuals
\[r_j = \frac{\hat{\epsilon}_j}{\sigma \sqrt{1-h_{jj}}}\]

Since $\sigma$ is usually unknown, the \emph{Studentized} residuals arise:
\[ \hat{t}_j = \frac{\hat{\epsilon}_j}{\hat{\sigma} \sqrt{1-h_{jj}}}\]

% subsubsection leave_one_out_cross_validation (end)

\subsubsection{$k$-fold cross validation} % (fold)
\label{ssub:_k_fold_cross_validation}

Divide the training sample into ``folds'' all of approximately equal size. Estimate the model and the MSE on each fold. The $k$-fold MSE is given by 
\[\text{MSE}_{(k)} = \frac{1}{k}\sum_{j=1}^k \text{MSE}_j \]
where $\text{MSE}_j$ is the MSE of the $j$-th fold with the model estimated on all the remaining data. The LOOCV is a special case of $k$-fold with $k=n$.

Which method and what values of $k$? Using $k=5,10$ is universally recognized.

The mean square error:
\begin{description}
	\item[$k=10$] \hfill\\
		Bias is reduced, with lower variance;
	\item[$k=n$] \hfill\\
		Bias is further reduced at the cost of higher variance.
\end{description}

% subsubsection _k_fold_cross_validation (end)

\subsection{Wrong ways to cross validate} % (fold)
\label{sub:wrong_ways_to_cross_validate}

There are $p=10^4$ predictors, and the task is to keep at most $100$. If the model includes a preprocessing step which finds the $100$ best predictors, then it must be incorporated in x-validation.

In multi step modelling cross validation must be applied to the entire sequence of the modelling steps. This way the we will get the right estimate of the test error (of picking the ``best'' $100$ predictors).

% subsection wrong_ways_to_cross_validate (end)

% subsection validation_techniques (end)

\subsection{Bootstrap} % (fold)
\label{sub:bootstrap}

It is also a resampling technique, where the data is drawn from the already observed data points. The idea is to artificially recreate new samples.

If the training set is $Z=\brac{z_k}_{k=1}^n$, then the bootstrap sample $Z^*_b$ is created by sampling with replacement from $Z$ (non-parametric bootstrap).

This allows to compute the accuracy of any statistic, which is derived from the sample, using the empirical distributions function.

If $S(Z)$ is computed directly form the original sample $Z$, then 
\[\var\big(S(Z^*)\big) = \frac{1}{B}\sum_{b=1}^B S(Z^*_b)\]

Bootstrap seems a bit magical.
% (Peter Hall. Edgeworth's view of the bootstrap). 

Cross validation is simple: fit the model on the bootstrap sample and then compute the MSE on those observations, which did not make it into the bootstrap sample.

The probability that an observation ends up in a bootstrap sample is 
\[\Pr\big(\text{obs}_i\in Z^*_b\big) = 1 - \big(1-\frac{1}{n}\big)^n\approx = 1-e^{-1}\]

The number of distinct observations in a bootstrap sample is $Y$. Then $Y = n (1-\frac{1}{n})$.

Thus the bootstrap cross-validation is approximately equal to the $3$-fold cross validation.
Since bootstrapping is computationally expensive, $k$-fold cross validation is preferred.




% Model selection with penalty of complexity
Estimate the test error directly from the data.
Bootstrap is used to estimate the accuracy of the estimates.



% subsection bootstrap (end)

Classification procedures

% section lecture_7 (end)

\section{Lecture \# 8} % (fold)
\label{sec:lecture_8}

SUpervised learning

an input $X$ and the output $T$ assuming the existence of a link between them. Up to now the case of continuous response $T$ variable has been considered, which led to regression methods.

In classification the value space of the response is finite or countable (a class, category or any discrete value).

Thus there is no special order between the distinct values of $T$. 
Thus the problem of classification arises :
construct a function $y(\cdot)$ of $X$ so that it predicts the value of the target variable $T$.

Training data $\big(x_k, t_k\big)_{k=1}^n$ 

Consider a simple case of a binary $T$ -- a binary classification problem and let $X$ be univariate.

For small $x$ the response is zero, for large -- $T=1$, and in the intermediate region the intervals with $T=0$ and $T=1$ overlap.

Minimizing the expectd sum of squares, the minimizer is the conditional expectation $\Ex\big(\big. T\big.\big\rvert X\big.\big)$, which is equal to 
\[\Pr\big(\big. T = 1\big.\big\rvert X = \mathbf{x}\big.\big) = \mathbf{x}\beta\]
and the classification is done through comparison of the probability of $T=1$ versus $T=0$.

%% an estimated slop
In classification the problem is to assign a new class $c\in \Ccal$ to an observation, whereas in the regression setting the goal was to predict the response gicen the new observation.

\subsection{As little misclassification as possible} % (fold)
\label{sub:as_little_misclassification_as_possible}

Minimize the misclassification, by looking at the probability of a mistake:
\[\Pr\big(y(X)\neq T\big)\]
observe that this is a generative approach.

The loss function is
\begin{align*}
	\Pr\big(\big. y(X) = C_1\big.\big\rvert X\in C_2\big.\big) + \Pr\big(\big. y(X) = C_2\big.\big\rvert X\in C_1\big.\big) = \\
	&= \int_{R_1} p(x, C_2) dx + \int_{R_2} p(x, C_1) dx \to \min_{R_i}
\end{align*}

If $p(x, C_2) > p(x, C_1)$ then classify $x$ in $C_2$, otherwise in $C_1$.
This is known as the Bayes classifier, and it is the best possible classification in this setting.

Consider the conditional probabilities and the Bayesian inference rules:
\[\Pr\big(\big. X\in C_1\big.\big\rvert X\big.\big) \Pr( X ) > \Pr\big(\big. X\in C_2\big.\big\rvert X\big.\big) \Pr( X ) > \]

There are two possibilities \begin{itemize}
	\item a generative approach, which models the joint distribution : linear discriminative analysis;
	\item a discriminative approach, with a parametric form, to come up with tractable models;
\end{itemize}

% subsection as_little_misclassification_as_possible (end)

\subsection{Expected loss} % (fold)
\label{sub:expected_loss}

A different perspective: minimize the expected loss, which is suitable under various circumstances where the risk of misclassification of significant.

Introduce a cost of misclassification of a new $x\in C_k$ to a region $R_j$ : $L_{jk}$.
and minimize the expected loss:
\[\text{EL} = \sum_{jk} \int_{R_j} L_{jk} p(x, C_k) dx\]

Thus for each $x$ minimize 
\[\sum_{jk} L_{jk} p(x, C_k) = \sum_{jk} L_{jk} p\big(\big. C_k\big.\big\rvert x\big.\big) p(x)\]

Indeed a similar problem.
% subsection expected_loss (end)

\subsection{Discriminative approach} % (fold)
\label{sub:discriminative_approach}

There is a function $y(\cdot):\mathcal{X}\to \Ccal$ which maps $x$ directly to q decision region.

In the case of two classes, assign $x$ to $C_1$ if $y(x) \geq 0$.

This approach does not have probabilistic motivation, whereas the previous approaches are not.

There is a problem with higher number of classes when using discriminant functions.

\begin{description}
	\item[One-versus-all]\hfill\\
		Classify according to the rule $C_j$ and $\neg C_j$ to get different classification boundaries.
		Basically fits $k$ classifiers. It works in some cases, but may yield undefined regions;
	\item[One-on-one] \hfill\\
		Consider the classes in pairs $C_i$ and $C_j$, neglecting the others. This results in $i-j$ boundaries,
		but requires $\frac{k(k-1)}{2}$ classifiers to fit. Still this can produce undefined regions;
	\itme[]
	Consider a single $K$-class discrimiminant given by $K$ linear functions $\brac{y_k(\cdot)}_{i=1}^K$.
	In the $\Real^d$ case these function look like 
	\[y_k(x) = \beta_{0k} + \beta_k'x\]
	Classify $x$ to class $C_k$ if $y_k(x) > y_j(x)$ for all $j\neq k$.
	The boundaries are given by $y_k(x) = y_j(x)$ which define hyperplanes in $\Real^d$:
	\[\big(\beta_j-\beta_k\big)'x + (\beta_{0j}-\beta_{0k}) = 0\]

	This partitions the space into convex decision regions. Indeed:
	If $x_1,x_2\in R_k$, then for any $\lambda\in \clo{0,1}$ consider $x = \lambda x_1 + (1-\lambda x_2)$.
	Then $y_k(x)$ is given by:
	\begin{align*}
		y)k(x) &= \beta_k'x +\beta_{0k} = \lambda\beta_k'x_1 + (1-\lambda)\beta_k'x_2 + \beta_{0k} \\
		& = \lambda\big(\beta_k'x_1 +\beta_{0k}\big) + (1-\lambda)\big(\beta_k'x_2 +\beta_{0k}\big) \\
		& = \lambda y_k(x_1) + (1-\lambda)y_k(x_2)
	\end{align*}
	Since $x_1, x_2\in R_k$ we have $y_k(x_m)>y_j(x_m)$ for all $j\neq k$ and $m=1,2$, whence
	$y_k(x) > y_j(x)$ for all $j\neq k$, which means that $R_k$ is convex. And thus single connected (?)...
\end{description}

% subsection discriminative_approach (end)

\subsection{Logistic regression} % (fold)
\label{sub:logistic_regression}

Suppose there are $K$ distinct categories.

This is a discriminative approach: model $\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big)$ with
\[\sum_{j=1}^K \Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = 1\]

The form of a logistic regression:
\[\ln\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \beta_{0j} + \sum_{s=1}^p\beta_{js} x_s \]

Take the last category as the reference category: for $j=1,\ldots,K-1$
\[\ln\frac{\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big)}{\Pr\big(\big. T=K\big.\big\rvert X=x\big.\big)}
	= \beta_{0j} + \sum_{s=1}^p\beta_{js} x_s = \beta_j'x \]
where $x_0 = \mathbf{1}$.

The probability that $T=j$ given $x$ is
\[\Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) e^{\beta_j'x}\]

Using the constraint yields
\[1 = \Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) \Big(1 + \sum_{i=1}^{K-1} e^{\beta_i'x}\Big)\]

whence 
\[\Pr\big(\big. T=K\big.\big\rvert X=x\big.\big) = \frac{1}{1 + \sum_{i=1}^{K-1} e^{\beta_i'x}}\]

and
\[\pi_j = \Pr\big(\big. T=j\big.\big\rvert X=x\big.\big) = \frac{e^{\beta_j'x}}{1 + \sum_{i=1}^{K-1} e^{\beta_i'x}}\]

\subsubsection{Interpretation} % (fold)
\label{ssub:interpretation}

Valid for the two class classification only.

What might the interpretation of the coefficients in $\beta_j$.

First note that the odds of classifying to $j$
\[\frac{\pi_j}{1-\pi_j} = e^{\beta_j'x}\]
which means that
\[\ln \frac{\pi_j}{1-\pi_j} = \beta_j'x\]
The log of the odds of the posterior behaves linearly.

The \textbf{logit} transform of $x\in \brac{0,1}$ is 
\[\ln\frac{x}{1-x}\]

An increase of one unit in $x_s$ multiplies the odds by $e^\beta_{js}$

% subsubsection interpretation (end)

\subsection{Estimation} % (fold)
\label{sub:estimation}
Consider the case of binary classification of the training sample $\big(x_k,t_k\big)_{k=1}^n$. What is the MLE of $\beta$?
Let $\theta = \brac{\beta_{sj}}_{s=0, j=1}^{p, K}$.

\begin{align*}
	l(\theta) &= - \ln \prod_{i=1}^n p\big(\big. t_i\big.\big\rvert x_i, \theta\big.\big) \\
	&= - \sum_{i=1}^n \ln p\big(\big. t_i\big.\big\rvert x_i, \theta\big.\big)
\end{align*}

Notation: $t_i$ has zero-one values, and $\sigma_i = \sigma\big(\beta'x_i\big)$ where $\sigma(a) = \frac{e^a}{1-e^a}$.

Note that 

Therefore the log-likelihood is 
\begin{align*}
	l(\theta) &= - \sum_{i=1}^n \ln \big( \sigma_i^{t_i} (1-\sigma_i)^{1-t_i} \big) \\
	&= - \sum_{i=1}^n t_i\ln \sigma_i + (1-t_i)\ln (1-\sigma_i)
\end{align*}

The first order conditions are given by
\begin{align*}
	\frac{\partial}{\partial \beta_j}\ln \sigma_i & = \frac{\partial}{\partial \beta_j}-\ln ( 1 + e^{-\beta'x_i} ) \\  
	& = \frac{\partial}{\partial \beta_j} -\ln ( 1 + e^{-\beta'x_i} ) = \frac{x_{ij}e^{-\beta'x_i}}{1 + e^{-\beta'x_i}} \\
	& = x_{ij} (1-\sigma_i)
	\frac{\partial}{\partial \beta_j}\ln (1-\sigma_i) & = - \sigma_i x_{ij}
\end{align*}
notice that $\ln(1-\sigma_i) = -\beta'x_i - \ln(1 + e^{-\beta'x_i})$

therefore
\[\frac{\partial}{\partial \beta_j} l(\theta) = -\sum_{i=1}^n \big( t_i x_{ij} (1-\sigma_i) - (1-t_i)\sigma_i x_{ij} \big) = \sum_{i=1}^n (\sigma_i-t_i)x_{ij} \]

In the matrix notation we have the following:
\begin{align*}
	\nabla_\beta l(\beta) = \underset{n\times p+1}{X}' (\underset{n\times 1}{\sigma} - \underset{n\times 1}{t})
\end{align*}

in appears to be non-linear in $\beta$

In order to employ numerical procedures, the second order conditions must be checked.
\begin{align*}
	\frac{\partial}{\partial \beta_i}\frac{\partial}{\partial \beta_j} l(\theta)
	&= \frac{\partial}{\partial \beta_i} \sum_{i=1}^n (\sigma_i-t_i)x_{ij} \\
	&= \sum_{i=1}^n x_{ij}\frac{\partial}{\partial \beta_i} \sigma_i \\
	&= \sum_{i=1}^n x_{ij} x_{ik} \sigma_i (1 - \sigma_i)
\end{align*}

Introduce a diagonal matrix $B = \text{diag}\big(\sigma_i(1-\sigma_i)\big)_{i=1}^n$.

Therefore
\[\nabla_\beta^2 l(\beta) = X'BX\]
which means that the Hessian is a positive semidefinite matrix.
implying that the optimal $\beta$ is the minimizer. The whole problem is that of convex optimization.

\subsubsection{Newton-Raphson} % (fold)
\label{ssub:newton_raphson}

The idea is to find a second order approximation of $f:\Real^d\to \Real$ in order to find its roots. As usual use the multivariate Taylor expansion
\[f(x)-f(a)  = \nabla f(a)' \big(x-a\big) + \frac{1}{2}\big(x-a\big)'\nabla^2 f(a) \big(x-a\big) + \ldots \]
Thus we approximate $f$ by 
\[q(x) = \frac{1}{2}x'\nabla^2 f(a) x + \alpha' x + \beta\]
where $\alpha = \nabla f(a) - \nabla^2 f(a)a$ and $\beta = \ldots$.

The main ides is to approximate a function near its minimum by a quadratic.

The minimum of $q$ is
\[\nabla q(x) = \nabla^2 f(a) + \alpha = 0\]
whence 
\[ x = -\big(\nabla^2 f(a)\big)^{-1} \alpha \]
and 
\[ x^* = a - \big(\nabla^2 f(a)\big)^{-1} \nabla f(a) \]

Let's apply this idea to the log-likelihood.
\begin{enumerate}
	\item begin with some initial $\beta_0$;
	\item update $\beta\to \beta_{\text{new}}$ using
	\[\beta_{\text{old}} - \big(X'BX\big)^{-1} X'(\sigma - t) = \big(X'BX\big)^{-1} X'B \big( X \beta_{\text{old}} - B^{-1} (\sigma- t ) \big)\]
	the term in brackets is the adjusted response $Z = X \beta_{\text{old}} - B^{-1} (\sigma - t )$ and the $\sigma$ is evaluated at $\beta_{\text{old}}$;
	use the weighted least squares.
\end{enumerate}


% subsubsection newton_raphson (end)

\subsubsection{Probit regression} % (fold)
\label{ssub:probit_regression}

Motivation: there is a latent variable $t^* = \beta'x + \epsilon$ with $\epsilon\sim \Ncal(0,1)$ and the observed binary variable $T = 1_{T^*(x)>0}(x)$.
Now $\Pr\big(\beta'x+\epsilon > 0\big) = \Pr\big(\epsilon > -\beta'x\big) = 1-\Phi(-\beta'x) = \Phi(\beta'x)$


% subsubsection probit_regression (end)

% subsection estimation (end)

% subsection logistic_regression (end)

% section lecture_8 (end)

\section{Lecture \# 9} % (fold)
\label{sec:lecture_#_9}

Two approaaches discrimimnative and generative.

Discriminative: modelling the posterior (conditional) distribution of the class given the variable; do not necessarily have probabilistic interpretation.
Generative: modelling the joint distribution of the class and the variable;
Direct modelling of the probabilities.
Linear discriminant (quadratic) analysis.
Discriminant functions.
The sample data $(X_k)_{k=1}^n\in \Real^d$ in latent classes $\Ccal$. Want to model the distribution of $X$, conditional on the class to which it belongs.
Once there is the likelihood, ``flip'' it to get the Bayesian posterior. The Bayesian classifier has the lowest error rate $\Pr(C_k\vert x)$.

The probability that 
\[\Pr(T = j\vert X = \mathcal{x}) \propto \Pr(X = \mathcal{x}\vert T = j) \Pr( T = j ) = f_j(x) \pi_j\]
To recover the normalising factor compute $N(x) = \sum_{k=1}^K f_j(x) \pi_j$.

We can model the likelihoods as well as the priors.

Choices for $f_j(x)$: \begin{itemize}
	\item Gaussian yields the LDA or the QDA (access to analytical expression);
	\item each data point comes from a mixture if gaussians gives rise to the MDA (only numerical EM algorithms);
	\item for a large number of predictors assume independence between the components of $X$ within each class (conditionally on the class the features are indepenednent) this results in the na\"ive bayes classifier.
\end{itemize}


\subsection{Linear discriminant analysis} % (fold)
\label{sub:linear_discriminant_analysis}

Each
\[f_j(x) = \frac{1}{\sqrt{2\pi}^p \lvert \Sigma_j\rvert^\frac{n}{2}} \text{exp}\bigg(-\frac{1}{2}(x-\mu_j)'\Sigma_j^{-1}(x-\mu_j)\bigg)\]

all classes have the same covariance matrix $\Sigma_j = \Sigma$.

compare the log of the ration of the probability
\[\log \frac{\Pr\big(T=k\vert X=x\big)}{\Pr\big(T=j\vert X=x\big)} \]
for the posteriors:
\[ = \log \frac{\pi_k}{\pi_j} + \log \frac{\Pr\big(X=x\vert T=k\big)}{\Pr\big(X=x\vert T=j\big)} = \log \frac{\pi_k}{\pi_j} + \log \frac{f_k(x)}{f_j(x)} \]
thus
\[ = \log \frac{\pi_k}{\pi_j} + \Big( -\frac{1}{2}(x-\mu_k)'\Sigma_k^{-1}(x-\mu_k\Big) + \frac{1}{2}(x-\mu_j)'\Sigma_j^{-1}(x-\mu_j) \Big) + \log\frac{\lvert \Sigma_j\rvert}{\lvert \Sigma_k\rvert} \]

using the covariance matrix constance
\[ = \log \frac{\pi_k}{\pi_j} + -\frac{1}{2}\big(\mu_k'\Sigma^{-1}\mu_k - \mu_j'\Sigma^{-1}\mu_j \big) + x'\Sigma^{-1}\big( \mu_k - \mu_j\big) \]
which is linear in $x$ -- the linear discriminant analysis. Define the discriminant function
\[\delta_k(x) = x'\Sigma^{-1}\mu_k - \frac{1}{2} \mu_k'\Sigma^{-1}\mu_k + \log \pi_k\]
Then  classify $x$ as $k$ if $\delta_k(x)>\delta_j$ for $k\neq k$.

Since we have a probabilistic model is is sensible to estimate the parameters using the MLE.

\subsection{The MLE} % (fold)
\label{sub:the_mle}

Summary:
we have got $K$ classes with probabilities $\big(\pi_j\big)_{j=1}^K$ subject to $\sum_{j=1}^K \pi_j = 1$, and a sample $\big(X,T\big)$.


The likelihood of a particular observation $x_i$
\[\prod_{j=1}^K \pi_j^{t_{ij}} f_j(x_i)^{t_{ij}}\]
where $t_{ij}$ is the indicator whether the $i$-th observation belongs to the class $j$ in the training sample.

Thus the complete likelihood of the observed data is
\[L = \prod_{i=1}^n \prod_{j=1}^K \pi_j^{t_{ij}} f_j(x_i)^{t_{ij}}\]
whence
\[\mathcal{L} = \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log \pi_j + t_{ij} \log f_j(x_i)\]
Using teh fact that everything is gaussian and the $\pi_j$ are related:
\[\mathcal{L} = \sum_{i=1}^n t_{i1} \log \big( 1- \sum_{j=2}^k\pi_j \big) +  \sum_{i=1}^n \sum_{j=2}^K t_{ij} \log \pi_j +  \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log f_j(x_i)\]
thus the likelihood is separable in terms of the prior probabilities and the parameters of the gaussians.
Computing the derivatives:
\begin{align*}
	\sum_{i=1}^n \frac{-t_{i1}}{\pi_1} +  \sum_{i=1}^n t_{ij} \frac{t_{i1}}{\pi_j} = 0\\
\end{align*}
whence for all $j\geq 2$:
\[\frac{\pi_j}{\pi_1} = \frac{\sum_{i=1}^n t_{ij}}{\sum_{i=1}^n t_{i1}}\]
thus
\[\hat{\pi_j} = \frac{\sum_{i=1}^n t_{ij}}{\sum_{i=1}^n \sum_{l=1}^K t_{il}}\]

The parameters of the gaussians: maximize with respect to $\mu_j$ and $\Sigma$. The only contribution is
\[\ldots + \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log f_j(x_i) \]
this is equivalent to maximizing
\[ -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^K t_{ij} \big(x_i-\mu_j\big)'\Sigma^{-1}\big(x_i-\mu_j\big)\] 
Thus
\[\sum_{i=1}^n t_{ij} \big(x_i-\hat{\mu}_j\big)'\Sigma^{-1} = 0\]
whence
\[\sum_{i=1}^n t_{ij} x_i = \sum_{i=1}^n t_{ij} \hat{\mu}_j\]
which leads to
\[\hat{\mu}_j = \frac{\sum_{i=1}^n t_{ij} x_i }{\sum_{i=1}^n t_{ij}}\]
the average of the $x_i$ belonging to the class $j$.

And finally the $\Sigma$, which is more tricky.
Collect the terms that depend on $\Sigma$ which depend on it. We get this to minimize:
\[ \sum_{i=1}^n \sum_{j=1}^K t_{ij} (x_i-\mu_j)'\Sigma^{-1}(x_i-\mu_j) + t_{ij} \log \lvert\Sigma\rvert \]
since the trace of a scalar is equal to its trace and $\Tr(ABC) = \Tr(BCA)$:
\[\sum_{i=1}^n \sum_{j=1}^K t_{ij} \Tr\big( \Sigma^{-1}(x_i-\mu_j)(x_i-\mu_j)'\big) + t_{ij} \log \lvert\Sigma\rvert\] 
define $S_j = \frac{1}{n_j}\sum_{i=1}^n(x_i-\mu_j)(x_i-\mu_j)'$ with $n_j=\sum_{i=1}^n t_{ij}$. Therefore
\[\sum_{j=1}^K n_j \Tr\big( \Sigma^{-1} S_j \big) - n \log \lvert\Sigma^{-1}\rvert\] 

Now the following could be proved directly:
\[\frac{\partial}{\partial A}\Tr(AB) = \frac{\partial}{\partial A}\Tr(BA) = B'\]
and
\[\frac{\partial}{\partial A}\log \lvert A\rvert = (A^{-1})'\]
therefore $\sum_{j=1}^K n_j S_j - n \hat{\Sigma} = 0$.

Remarks: \begin{itemize}
	\item if $\Sigma\neq\Sigma_j$ then we have quadratic decision boundaries, and 
	\[\delta_k(x) = - \frac{1}{2} (x-\mu_k)'\Sigma^{-1}(x-\mu_k) + \log \pi_k - \frac{1}{2}\log\lvert\Sigma_k^{-1}\rvert\]
	\item to get quadratice decision boundaries with LDA enrich the feature space by including functions of the predictors;
	\itme QDA (and LDA) do not scale well with the number of predictors: use the Na\"ive Bayes;
	The na\"ive Bayes conditional independence. The na\"ive part
	\[\Pr\big(X=x\vert T=j\big) = \prod_{m=1}^p \Pr\big(X_m=x_m\vert T=j\big)\]

	the classification rule is using the Bayes part (maximizing the posterior):
	\[\hat{j}_x = \text{argmax}_{j=1,\ldots, K} \Pr\big(X=x\vert T=j\big) \Pr\big(T=j\big)\]
	This has nothing to do with Bayesian inference. Thus there are Bayesian Na\"ive Bayes classifiers :).
	\item The difference between the logistic regression and the LDA. In logistic we looked at
	the log-ratio of class likelihoods given the data. It also linear in $x$, but the regression
	does not take the structure of the predictors.
	LDA is less flexible:
	\[\delta_k(x) = x'\Sigma^{-1}\mu_k - \frac{1}{2} \mu_k'\Sigma^{-1}\mu_k + \log \pi_k\]
	whereas the logistic had $\veta_0 + x\beta$ -- more general.
	If the true $x$ are Gaussian then it is better to use the LDA.
\end{itemize}

Some bias, but low variance due to simple linear hyper-boundaries.


% subsection the_mle (end)

% subsection linear_discriminant_analysis (end)

\subsection{Mixture of gaussians discriminant analysis} % (fold)
\label{sub:mixture_of_gaussians_discriminant_analysis}

\subsubsection{the Expectation-Maximisation} % (fold)
\label{ssub:the_expectation_maximisation}

the Expectation Maximization algorithm (Dempster et al. (1977)) is used in the case of latent or missing values.

Suppose the there is a probabilistic model with observed variables $X$ or $X,T$ and a set of latent variables $Z$.
In the mixture of distributions the latent variable is the mixture component, from which the observation was drawn.

Gaussian mixture is a superposition of Gaussian distributions:
\[g(x) \sim \sum_{k=1}^K \pi_k \phi_k(x\vert \mu_k, \Sigma_k)\]
such that $\sum_{k=1}^K \pi_k = 1$. It allows multimodality and greater flexibility.
The $\pi_k$ are the mixing coefficients and are the prior probabilities that $x$ comes from $k$-th component.

For the MLE we would have to estimate $\theta=(\pi_k,\mu_k,\Sigma_k)_{k=1}^K$ with the complete log-likelihood
\[l(\theta) = \sum_{i=1}^n \log \big(\sum_{k=1}^K\pi_k \phi_k(x\vert \mu_k, \Sigma_k)\big)\]

A way to get rid of the sum-under-the-log is to add the hidden information (here the original component).
Other examples include \textbf{H}idden \textbf{M}arkov \textbf{M}odels and the \textbf{M}ixture \textbf{D}iscriminant \textbf{A}nalysis.

we need a joint distribution of $(X,Z)$ governed by some $\theta$
\[p(X, Z\vert \theta)\]
hte goal is to maximize the likelihood function
\[p(X\vert \theta) = \int p(X, Z\vert \theta) dZ = \sum_{z} p(X, Z=z\vert \theta) \]
This is in general intractable.
Suppose $q(Z\vert X,\theta)$ is an arbitrary density of $Z$ and proceed backwards from the goal.
\[l(\theta) = \log p(x\vert \theta) = \log \sum_z q(z\vert x,\theta) \frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} \]
due to concavity of log, the Jensen's inequality yields:
\[\ldots \geq \sum_z q(z\vert x,\theta) \log \frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} = F(q,\theta) \]

To maximize the $l(\theta)$, first maximize the lower bound. The process is
\begin{description}
	\item[E-step] \[q^{(t+1)} = \text{argmax}_q F(q,\theta^{(t)})\]
	\item[M-step] \[\theta^{(t+1)} = \text{argmax}_\theta F(q^{(t)},\theta)\]
\end{description}
But first we need to initialize the $\theta^{(0)}$ and the EM converges only to a local maximum.
However even in this setting the problem is intractable. We may find a sufficient condition such that the lower bound becomes the equality.

The sufficient condition on $q$ is when for all $z$
\[\frac{p(x\vert z, \theta)}{q(z\vert x,\theta)} = \text{const}\]
whence $q(z\vert x,\theta) \propto p(x\vert z, \theta)$ and 
\[q(z\vert x,\theta) = \frac{p(x, z\vert \theta)}{\sum_z p(x\vert z, \theta)} = p(z\vert x, \theta)\]
which leads to 
\[q^{(t+1)}(z\vert x,\theta) = p(z\vert x, \theta^{(t)})\]

The $M$-step:
\[l(\theta) \geq F(q,\theta) = \sum_z q(z\vert x,\theta) \log p(x\vert z, \theta) - \sum_z q(z\vert x,\theta) \log q(z\vert x,\theta)\]
-- the entropy and the cross entropy of $q$.
Since $q(z\vert x,\theta) = q^{(t+1)}(z\vert x,\theta) = p(z\vert x, \theta^{(t)})$, we have
\[F(q,\theta) = Q(\theta, \theta^{(t)}) + H(q)\]
and
\[Q(\theta, \theta^{(t)}) = \sum_z p(z\vert x, \theta^{(t)} \log p(x\vert z, \theta)
= \Ex_{p(z\vert x, \theta^{(t)})} \log p(z\vert x, \theta) \]

In summary: \begin{itemize}
	\item The $E$-step is the computation of $Q(\theta, \theta^{(t)})$;
	\item The $M$-step return (update) the $\theta^{(t+1)} = \text{argmax}_\theta Q(\theta, \theta^{(t)})$;
\end{itemize}
If the density family of the classes is exponential, then it is easy to maximize and compute the $Q$ as well.

\subsubsection{Proof of convergence of EM} % (fold)
\label{ssub:proof_of_convergence_of_em}

Show that $\theta^{(t)}$ converges and $l(\theta^{(t)})$ does not decrease.
Look at the values of $l(\theta)$:
\[l(\theta^{(t)}) = \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta^{(t)})}\]
where we used the chosen good $q$ 
\[\geq \sum_{i=1}^n \sum_z q(z_i\vert x_i, \theta^{(t+1)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta)}\]
thus
\[\geq \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t+1)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta^{(t+1)})}\]
since $Q(\theta^{(t+1)}, \theta^{(t)}) \geq Q(\theta^{(t)}, \theta^{(t)})$ :
\[\geq \sum_{i=1}^n \sum_z p(z_i\vert x_i, \theta^{(t)}) \log \frac{p(z_i, x_i\vert \theta^{(t)})}{p(x_i\vert z_i, \theta^{(t)})}\]
In fact $Q(\theta^{(t)}, \theta^{(t)}) = l(\theta^{(t)})$ and $Q(\theta, \theta^{(t)})$ is convex (quadratic in $\theta$).

% subsubsection proof_of_convergence_of_em (end)

% subsubsection the_expectation_maximisation (end)

% subsection mixture_of_gaussians_discriminant_analysis (end)

% section lecture_#_9 (end)
\end{document}