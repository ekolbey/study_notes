\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ex}[0]{{\mathbb{E}}}
\newcommand{\pr}[0]{{\mathbb{P}}}
\newcommand{\Var}[0]{{\text{Var}}}
\newcommand{\RSS}{{\text{RSS}}}
\newcommand{\TR}{{\text{Tr}}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\rank}{\mathop{\text{rank}}\nolimits}



\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Document title}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section*{Preliminaries} % (fold)
\label{sec:preliminaries}

\url{http://cs.hse.ru/ai/mmdm}

The folowing topics would be covered in the course:
\begin{itemize}
	\item Decision theory wit linear regression
	\item Shrinkage methods (Ridge regression, Lasso, elastic net)
	\item Polynomial regression, splines
	\item Model selection techniques, validation methods
	\item Classification problems (the Support Vector Machine, decision trees, bagging and boosting)
	\item Neural networks (flexible regression and classification)
\end{itemize}

Main books:
\begin{description}
	\item[Tibshirani2013] \hfill\\
		James, Witten, Hastie and Tibshirani (2013): ``An Introduction to Statistical Learning with Applications in R''
	\item[Bishop2006] \hfill\\
		Bishop (2006): ``Pattern Recognition and Machine Learning''
	\item[Tibshirani2013] \hfill\\
		Hastie, Tibshirani, Friedman (2013): ``The Elements of Statistical Learning''
\end{description}

% section* preliminaries (end)

\section{Lecture \# 1} % (fold)
\label{sec:lecture_1}

Linear regression lies in the class of supervised learning problems.
Such problems work with paired data of arbitrary nature: univariate, multivariate, categorical, numerical, abstract (like graphs in chemical classification problems) et c.
In general there is a learning (training) sample $\brac{x_i, T_i}_{i=1}^N$ of a pairs of an input variable $X$ and the associated output target $T$.

The goal is to construct a function $y(\cdot)$ of the input that predicts, forecasts, estimates or otherwise returns the target, associated with the given input.

The target variable $T$ is always considered to be a random variable at least due to measurement or other irreducible uncertainty, that is present in any system.
The input variable $X$, depending on the approach, may or may not be random.

In the case when it is an rv, the data is characterized by the join probability density (distribution) function $p_{X,T}\brac{x,t}$.
The joint pdf can be factorized into the density of the response conditional on a particular input and the density (probability) of the input:\[p_{X,T}\brac{x,t} = p_{\induc{T}X}\brac{\induc{t}x} p_X(x) \]
The analysis of the problem may now fork:
\begin{description}
	\item[Generative model] \hfill \\
		both the data and the response given data are modelled;
	\item[Discriminative model] \hfill \\
		model just the conditional response.
\end{description}

Statistical decision theory helps decide on a functional form of $y(\cdot)$:
choose $y(\cdot)$ so a to minimize the expected loss functional $\Ex_{X,T} L\brac{T,y(X)}$
over the bivariate distribution of the input data.
This way the incorrect estimation of $t$ by $y(x)$ given $x$ is penalized.

Among the possible loss functions the most frequently used is the quadratic
loss function $L(t,y) \defn \brac{t-y}^2$, which has its roots in the $L^p$ norm
\[\nrm{f}_p = \brac{\int \abs{f}^p d\mu}^\frac{1}{p}\] for $p = 2$.

Another possible loss function is the so called Minkowski cost, defined as $L(t,y) \defn \abs{t-y}$,
which is the $L^p$ norm with $p=1$.

Sidenote: conditional expectation as the projection of $f$ onto a subspace of $g$ measurable functions in the Hilbert space of square integrable functions with the usual $\int f\bar{g} d\mu$ inner product.

% Remember the measure theory? with its sigma-finite measure \mu with respect to which another measure \nu is absolutely continuous, and there is f\in L^1(\mu) such that \nu=\int f d\mu and \int gd\nu = \int fg d\mu.

% Which approach to use?
The joint randomness of $(X,T)$ leads to the following problem in the case of $L^2$ loss function:
\[\Ex_{X,T} \brac{T-y(X)}^2 \to \min_y\]
Using the tower property of the conditional expectation the optimal $y$ is the conditional expectation $\phi(X)\defn \Ex\brac{\induc{T}X}$.
Indeed, since $\Ex_{\induc{T}X} \brac{\brac{T-\phi(X)} } = 0$ , the mean squared error is
\begin{align*}
	\Ex_{X,T} \brac{T-y(X)}^2 = \Ex_{X} \brac{\Ex_{\induc{T}X} \brac{T-y(X)}^2 } \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{ \brac{T-\phi(X)}^2+\brac{\phi(X)-y(X)}^2} + 2 \Ex_{\induc{T}X} \brac{\brac{T-\phi(X)}\brac{\phi(X)-y(X)} }} \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{T-\phi(X)}^2+\brac{\phi(X)-y(X)}^2 } + 2 \Ex_X \brac{\brac{\phi(X)-y(X)} \Ex_{\induc{T}X} \brac{T-\phi(X)} } \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{T-\phi(X)}^2 + \Ex_{\induc{T}X} \brac{\phi(X)-y(X)}^2 }
\end{align*}

The latter conditional expectation is vanishes whenever $y=\phi$, whilst the first one is independent of $y$ and can be considered as the irreducible variance due to structural variance.
The mean prediction error is the irreducible variance and the approximation error:
\[\text{error} \defn \Ex_X \Var\brac{\induc{T}X} + \Ex_x \brac{\brac{\phi(X)-y(X)}^2} \]

However it is never possible to achieve $y(x) = \Ex\brac{\induc{T}X=x}$.
Therefore some insight is needed on $\Ex_{\induc{T}X} \brac{\phi(X)-y(X)}^2$.
\begin{description}
	\item[Frequentist] $\pr\brac{\induc{\Dcal}\Theta}$ -- uncertainty is due to dataset
	\item[Bayesian] $\pr\brac{\induc{\Dcal}\Theta} \pi(\Theta)$ -- uncertainty is due parameters, where $\pi(\Theta)$ is the prior distribution.
	The posterior distribution is proportional to
		\[\pr\brac{\induc{\Theta}\Dcal} \sim \pr\brac{\induc{\Dcal}\Theta} \pi(\Theta)\]
\end{description}

Error is for a particular training dataset, on which the $y$ is estimated. In effect the functional form of $y$ depends on the data set $D$:
\[\Ex_\Dcal\brac{\text{Error}_\Dcal} = \text{Noise} + \Ex_\Dcal \Ex_X \brac{ y(x,D)-\phi(x) }^2\]
where noise is defined as $\Ex_X \Var\brac{\induc{T}X}$.

Now put $f(x) \defn \Ex_\Dcal y(x,D)$.
\begin{align*}
	\Ex_\Dcal\brac{ y(x,D)-\phi(x)\pm f(x) }^2 &
		= \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \Ex_\Dcal\brac{ f(x) - \phi(x) }^2 \\ &+ 2 \Ex_\Dcal\brac{ y(x,D) - f(x) }\brac{ f(x) - \phi(x) } \\
		& = \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \brac{ f(x) - \phi(x) }^2 \\&+ 2\brac{ f(x) - \phi(x) } \Ex_\Dcal\brac{ y(x,D) - f(x) } \\
		& = \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \brac{ f(x) - \phi(x) }^2 \\
\end{align*}

Therefore the expected value of the loss is 
\[\Ex_\Dcal\brac{\text{Error}_\Dcal} = \text{Noise} + \text{Variance} + \text{Bias}^2\]
with the variance is due to the fact that the instance of $y$ is not static with varying dataset $D$,
and the bias is the measure of average closeness of the approximation $y(\cdot)$ to the real conditional expectation $\Ex\brac{\induc{T}X}$.
This is the decomposition of the loss into systemic noise, bias and variance.

% Non-parametric approach + low dimension: smoothing local data (kNN -- $k$ nearest neighbours).

Parametric approach would require a parametric expression of $y(\cdot)$ as an approximation of $\Ex\brac{\induc{T}X}$ at every $x$ (or at most almost all).

Suppose $\Lcal^2$ is the space in which the best approximation of $\phi(X)$ is searched for.
Projecting $\Ex\brac{\induc{T}X}$ onto $\Lcal^2$ yields $\phi(X) = h^\perp(X) + h(X)$, where $h^\perp \perp \Lcal^2$ (i.e. $\brkt{h^\perp,g} = 0$ for all $g\in \Lcal$) and $h\in \Lcal^2$.

% Here should be a picture of the projection: the conditional expectation is projected onto the subspace of square integrable functions.

% \Ex_\Dcal y(X,D) -- is an element of the 

Since $f-h\in\Lcal^2$ it must be true that
\[\Ex_X \brac{ f(X) - h(X) } h^\perp(X) = \brkt{f-h, h^\perp} = 0 \]
whence the following decomposition must hold:
\begin{align*}
	\text{Bias}^2 &= \Ex_X \brac{ \Ex_\Dcal y(X,D) - \phi(X) }^2 
		= \Ex_X \brac{ f(X) - h^\perp(X) - h(X) }^2 \\
		& = \Ex_X \brac{ f(X) - h }^2 + \Ex_X \brac{ h^\perp(X) }^2 + \Ex_X \brac{ f(X) - h(X) } h^\perp(X) \\
		& = \Ex_X \brac{ f(X) - h(X) }^2 + \Ex_X \brac{ h^\perp(X) }^2
\end{align*}
This means that the squared bias is the sum of how close the model is to the projection of $\Ex\brac{\induc{T}X}$ onto $\Lcal^2$, and how well the projection approximates the conditional expectation itself.

In linear regression problems the space $\Lcal^2$ is taken to be the set of all linear functions of $X$.

\subsection*{Linear regression} % (fold)
\label{sub:linear_regression}

Suppose $X$ is a $p$-dimensional vector of features and that we attempt to approximate the $\Ex\brac{\induc{T}X}$ with $y(x) \defn \beta_0 + \sum_{k=1}^p \beta_k x_k$.

In effect we suppose the following econometric model
\[T = \beta_0 + \sum_{k=1}^p \beta_k X_p + \epsilon;\,\epsilon\sim\Ncal(0,\sigma^2)\]
Therefore conditional upon $X$ the target $T$ is distributed like \[\induc{T}X\sim \Ncal(y(X),\sigma^2)\]

In this setting the following questions arise: \begin{itemize}
	\item How $\beta_{1\times(p+1)}$ is estimated?
	\item Which coefficients $\beta_k$ are significant?
	\item What is their interpretation?
\end{itemize}

In the matrix form, the linear regression problem is stated as follows:
\[\underset{n\times 1}{T} = \underset{n\times (1+p)}{X}\underset{(1+p)\times 1}{\beta} + \underset{n\times 1}{\epsilon}\]
where the vector $X$ of the predictors of the training set has been altered by adding a constant term -- the intercept. The predictors now look like this
\[X = \brac{ \one \defn \underset{n\times 1}{1}, \underset{n\times 1}{x_1}, \ldots, \underset{n\times 1}{x_p} }\]

Two approachs to estimating $\beta$ is possible: the \textbf{L}east \textbf{S}quares\footnotemark  and the \textbf{M}aximum \textbf{L}ikelihood.
Effectively the LS approach to parameter estimation is non-probabilistic.
There are different flavours of least squares: the ordinary, the generalised (with a weighting matrix), two-stage least squares (2SLS), to name a few, and others.
The 2SLS is an extension of the GLS that attempts to battle heteroskedasticity:
first run OLS to get the squared residuals, then GLS with weights that standardise the residuals.

The goal of LS is to solve the following problem: minimize the \textbf{R}esidual \textbf{S}um of \textbf{S}squares
\[\RSS \defn \brac{T-X\beta}'\brac{T-X\beta} \to \min_\beta\]
The first order conditions on the potential minimizer are in the matrix form:
\begin{align*}
	\frac{\partial}{\partial \beta}\RSS &= \frac{\partial}{\partial \beta}\brac{T'T-\beta'X'T - T'X\beta + \beta'X'X\beta}\\
	&= \frac{\partial}{\partial \beta} T'T - \frac{\partial}{\partial \beta} \beta'X'T - \frac{\partial}{\partial \beta} T'X\beta + \frac{\partial}{\partial \beta} \TR\brac{\beta'X'X\beta}\\
	&= - 2 \frac{\partial}{\partial \beta} T'X\beta + \beta'\brac{X'X + \brac{X'X}'}\\
	&= - 2 T'X + 2 \beta'X'X = 0
\end{align*}
If the matrix $X$ is full rank, then the matrix $X'X$ is invertible, and is positive-semidefinite.
Therefore $\frac{\partial^2}{\partial \beta\partial \beta}\RSS = 2 X'X \geq 0$, whence the Least Squares solution $\hat{\beta} \defn \brac{X'X}^{-1} X'T$ is indeed the minimizer of the RSS.

The best linear approximation to $\Ex\brac{\induc{T}X}$ is the projection of $T$ onto the space of linear functionals of $X$.

The matrix $\hat{H} \defn X\brac{X'X}^{-1}X'$ is called the \textbf{hat matrix} and is in fact a projector onto the linear subspace spanned by the column vectors of $X$.

Every projector matrix has the following property: if $v$ is in the spanned linear subspace, then $\hat{H}v = v$.
Indeed, since $v\in \clo{X}$, there must exist $\underset{(p+1)\times 1}{\alpha}\in \Real^{(p+1)}$ such that $v = X\alpha$.
Therefore
\[X\brac{X'X}^{-1}X'v = X\brac{X'X}^{-1}X'X\alpha = X\alpha = v\]

If the columns of $X$ are orthogonal, then $X'X$ is in fact a diagonal matrix with the squared Euclidean norms of columns of $X$ on the diagonal.
In this case each of $\brac{\beta_k}_{k=0}^n$ is the coefficient in the projection of $T$ onto th respective column of $X$.

% subsection* linear_regression (end)

\subsection*{Gram-Schmidt orthogonalisation} % (fold)
\label{sub:gram_schmidt_orthogonalisation}

Suppose there is a set of linearly independent vectors $\brac{f_i}_{i=1}^m$ in some Euclidean space $V$.
\begin{description}
	\item[Initialisation] \hfill \\
		Set $e_1 \defn \frac{1}{\nrm{f_1}} f_1$;
	\item[General step $k=1,\ldots,m-1$] \hfill \\
		Let \[u_{k+1} \defn f_{k+1} - \sum_{i=1}^k \frac{\brkt{e_i, f_{k+1}}}{\brkt{e_i,e_i}} e_i\]
		and put $e_{k+1}\defn \frac{1}{\nrm{u_{k+1}}} u_{k+1}$.
\end{description}
An effective algorithm is given in \emph{Golub \& Van Loan 1996} section 5.2.8.

% subsection* gram_schmidt_orthogonalisation (end)

%% I don't recall what the section below is for...
In a bivariate case $X = (\one,x)$ and
\[\hat{\beta} = \brac{\begin{matrix} \one'\one & \one'x\\ x'\one & x'x \end{matrix}}^{-1} \brac{\begin{matrix} \one't \\ x't \end{matrix}}
= \frac{1}{n^2 \brac{\overline{x^2} - \bar{x}^2}} \brac{\begin{matrix} \one'\one \one't - \one'x x't \\ - x'\one \one't + x'x x't \end{matrix}}
\]
since $\one'\one = n$ and $x'\one = \one'x = n\bar{x}$.

%% p-predictors

Set $\hat{z}_0 \defn \one$. For each $j=1,\ldots,p$ regressing $x_k$ onto $\brac{\hat{z}_j}_{j=0}^{k-1}$ to get the LS coefficients $\brac{\alpha_{jk}}_{j=0}^{k-1}$.
Construct $\hat{z}_k$ as the residuals of $x_k$ from projecting onto $\brac{\hat{z}_j}_{j=0}^{k-1}$ :
\[\hat{z}_k = x_k - \sum_{j=0}^{k-1} \alpha_{jk} \hat{z}_j\]
This is similar to the Gram-Schmidt procedure described previously.

Regressing $t$ on $z_k$ yields the coefficient $\hat{\beta}_k$ which is not the OLS coefficient, but shows new information gained from the $k^\text{th}$ predictor.

Discriminative approach.
Since $\hat{\beta}$ is a linear transformation of $T$ and $\hat{\beta} = \beta + \brac{X'X}^{-1}X'\epsilon$, \textbf{provided the model is specified correctly}, it is therefore true that
\[\hat{\beta}\sim\Ncal\brac{\beta,\brac{X'X}^{-1} \sigma^2}\]

In fact if $\epsilon\sim \Ncal\brac{0,\Sigma}$ -- i.e the noise has some other covariance structure, then $\Var(\hat{\beta}) = \brac{X'X}^{-1}X'\Sigma X\brac{X'X}^{-1}$ -- this is useful in GLS, mentioned above, especially if $\Sigma$ is decomposed into $C'C$ with $C$ -- non-singular lower triangular matrix (Cholesky).

Indeed, if we transform (weigh) the observations according to matrix $W$ with $W'W = \Sigma^{-1}$, then the LS model becomes $WT=WX\beta + W\epsilon$, whence
\[\hat{\beta} = \brac{X'W'WX}^{-1}X'W'WT = \brac{X'\Sigma^{-1}X}^{-1}X'\Sigma^{-1}T\]
whence $\hat{\beta}\sim\Ncal\brac{\beta, \brac{X'\Sigma^{-1}X}^{-1}}$.

\noindent\textbf{Cochran's Theorem} \hfill\\
Suppose $\brac{\xi_i}_{i=1}^n \sim\Ncal(0, 1)$ and that
\[\sum_{i=1}^m \xi_i^2 = \sum_{j=1}^k Q_j\]
where $Q_j = \xi' A_j \xi$ is a positive-semidefinite quadratic form for each $j=1,\ldots,k$.
If $\sum_{j=1}^k r_j = n$, with $r_j = \rank{Q_j}$, then \begin{enumerate}
	\item The random variables $Q_j$ are independent;
	\item each $Q_j\sim \chi^2_{r_j}$.
\end{enumerate}

Returning to the simple model, what is the estimator of $\sigma^2$?

It turns out that $\frac{\RSS}{\sigma^2}\sim \chi^2_{n-p-1}$.
Indeed, provided the model is specified correctly, \begin{align*}
	\RSS &= \brac{T-X\hat{\beta}}'\brac{T-X\hat{\beta}} \\
	& = \brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon}'\brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon} \\
	& = \epsilon' \brac{I - X\brac{X'X}^{-1}X'}'\brac{I - X\brac{X'X}^{-1}X'}\epsilon \\
	& = \epsilon' \brac{ I - X\brac{X'X}^{-1}X' } \epsilon = \epsilon' \brac{ I - \hat{H} } \epsilon
\end{align*}
Since the trace of an idempotent matrix equals its rank, $\rank(I - \hat{H}) = \TR(I) - \TR(\hat{H}) = n- (p+1)$, whence by Cochran's theorem $\frac{1}{\sigma}\RSS\sim\chi^2_{n-(p+1)}$.
Furthermore, $\hat{\sigma}^2 \perp \hat{\beta}$.

\subsection{Hypothesis testing} % (fold)
\label{sub:hypothesis_testing}

If $V_j \defn \brac{\brac{X'X}^{-1}}_{jj}$ -- the $j^\text{th}$ diagonal element of $\brac{X'X}^{-1}$,
whence $\Var\hat{\beta}_j\defn \sigma^2 V_j$.

To test $H_0:\beta_j=0$ versus $H_1:\beta_j\neq 0$, one should use the following statistic, which under the null is distributed as follows:
\[\frac{\sfrac{\hat{\beta}_j}{\sqrt{\sigma^2 V_j}}}{\sqrt{ \frac{\sfrac{n-p-1}{\sigma^2} \hat{\sigma}^2 }{ n-p-1 } }} \sim \frac{\Ncal(0, 1)}{\sqrt{\chi^2_{n-p-1}}} = t_{n-p-1}\]

The $\alpha$-level confidence interval for $\beta_j$ is defined as
\[\clo{ \hat{\beta}_j - \hat{\sigma} \sqrt{ V_j },  \hat{\beta}_j + \hat{\sigma} \sqrt{ V_j } }\]

For the false rejection rate of the t-test $\alpha = 0.05$, on average $5\%$ of statistically significant results would be false positives.

To test for simultaneous insignificance of $\brac{\beta_j}_{j=1}^p = 0$ one should use the F-test
(more generally an F-test for linear restriction). The following takes place under the null hypothesis that the simpler model is correct :
\[F = \frac{\sfrac{\RSS_0-\RSS_1}{p_1-p_0}}{\sfrac{\RSS_1}{n-p_1-1}}\sim F(p_1-p_0, n-p_1-1)\]

To gauge the model accuracy one can use both the RSS as a measure of lack of fit and the $R^2$ as a measure of goodness-of-fit.

If $\one\in \clo{X}$ then
\[ \text{TSS} = T' \brac{I - \one\brac{\one'\one}^{-1}\one'} T = T' \brac{I - X\brac{X'X}^{-1}X'} T + T' \brac{ X\brac{X'X}^{-1}X' - \one\brac{\one'\one}^{-1}\one'} T = \RSS + \text{ESS}\]

the goodness-of-fit measure $R^2$ is the ratio of the explained variance to the total variance
\[R^2 \defn \frac{\text{ESS}}{\text{TSS}} = 1 - \frac{\RSS}{\text{TSS}} \]
For a simple case of a bivariate regression
\[R^2 = \rho^2 = \frac{\abs{\text{corr}(x,t)}^2}{\Var(x)\Var(t)} = \beta^2 \frac{\Var(x)}{\Var(t)}\]


% subsection hypothesis_testing (end)

% section lecture_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}
% 2015-01-20

The uncertainty on the model for a new input $x_0$ 

Prediction of the value of $T$ at a new input $x_0$ has two sources of uncertainty: \begin{enumerate}
	\item Uncertainty of the first type -- due to the estimation of $y(\cdot)$ ($\hat{\beta}$) in $\Lcal^2_X$;
	\item Uncertainty of the second type -- due to the additive noise term. Even if the true $\beta$ were known, the true values of the response $T$ at $x_0$ would still be unknown.
\end{enumerate}
Predictive intervals incorporate the noise as well as uncertainty due to imprecise estimation of the coefficients.

Variability due to estimation of an approximate model.

\subsection{Shrinkage models} % (fold)
\label{sub:shrinkage_models}

In the ordinary linear squares method we minimized the following fitness function:
\[\text{min} \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2\]

Ridge regression
Minimize the following penalized Residual Sum of Squares as:
\[\RSS\defn \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2 + \lambda \sum_{j=1}^k \beta_j^2\]
the $\lambda>0$ is called the tuning parameter.
It controls the amount of shrinkage:
if $\lambda = 0$ then there is absolutely no shrinkage, and the optimal beta coincide with the OLS.
But if $\lambda\to \infty$ then the optimal coefficient vector tends to zero. 

However with shrinkage, the scale of each predictor $\brac{x_j}_{j=0}^k\in \Real^n$ matter!

Thus the first step is to standardize the variables before fitting the Ridge regression.

Next observation: the intercept $\beta_0$ is indeed excluded from the shrinkage term.

Suppose the $t$ and $X$ are standardized (mean and centre), which means that $\beta_0 \equiv 0$
Then in the matrix form the following is minimized:
\[\brac{t-X\beta}'\brac{t- X\beta} + \lambda \beta'\beta \to \text{min}\]

The first order conditions are:
\begin{align*}
	\frac{\partial \RSS}{\partial \beta } &= -2 X\brac{t-X\beta}+2\lambda \beta\\
	% \frac{\partial \RSS}{\partial \lambda } = \beta'\beta = 0
\end{align*}

The optimal coefficients are given by
\[\hat{\beta}_\text{ridge}\defn \brac{X'X + \lambda I}^{-1}X't\]

% revise matrix derivatives.

% subsection shrinkage_models (end)

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

Limiting the size of the coefficients. Modified RSS with a tuning parameter $\lambda$.
\[\RSS\defn \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2 + \lambda \sum_{j=1}^k \beta_j^2\]
important: \begin{itemize}
	\item Consider dimensionless inputs (standardized by standard deviation) -- comparable $\beta_j$.
	\[x_{ij}\]
	\item If the input vectors are centred, then the estimation is separable.
\end{itemize}
Consider \begin{align*}
	\RSS &= \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j \brac{x_{ij}-\bar{x}_j} - \sum_{j=1}^k \beta_j \bar{x}_j}  + \lambda \sum_{j=1}^k \beta_j^2 \\
	\RSS' &= \sum_{i=1}^n \brac{t_i - \beta_0^c - \sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}  + \lambda \sum_{j=1}^k {\beta_j^c}^2
\end{align*}
Notice that $\beta_j^c = \beta_j$ for all $j=1,\ldots,n$ and $\RSS=\RSS'$.
\begin{align*}
	\RSS' &= \lambda \sum_{j=1}^k {\beta_j^c}^2 + \sum_{i=1}^n \brac{t_i - \beta_0^c}^2 \\
	& + \sum_{i=1}^n \brac{\sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}^2 \\
	& - 2 \sum_{i=1}^n \brac{t_i - \beta_0^c} \sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}
\end{align*}
Notice that the last term is zero. Indeed \begin{align*}
	\sum_{i=1}^n \brac{t_i - \beta_0^c} \brac{\sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}
	& = \sum_{j=1}^k \beta_j^c \sum_{i=1}^n \brac{t_i - \beta_0^c} \brac{x_{ij}-\bar{x}_j}
\end{align*}
The problem of ridge regression in the matrix form, where $X$ the $n\times k$
is matrix of centred standardized independent variables with no intercept:
\[\RSS = \brac{t-X\beta}'\brac{t-X\beta} + \lambda \beta'\beta\]

The solution is
\[\hat{\beta}_{\text{ridge}}\defn \brac{X'X + \lambda I}^{-1} X'T\]

An equivalent optimization problem:
\begin{align*}
	\hat{\beta}_{\text{ridge}} = \text{argmin}_{\beta} \sum_{i=1}^n \brac{t_i - \sum_{j=1}^k \beta_j x_{ij}^c}
	\text{subject to } \sum_{j=1}^k \beta_j^2 \leq \eta
\end{align*}
$\eta$ -- the budget, $\lambda$ -- the complementary slackness coefficient.

There is a certain universality of the statement of the regression problem in this form:
it is possible to plug in different constraints ($L^1$ constraints -- LASSO).
LASSO is automatically performing feature selection due to $L^1$ norm.

% How to tune $\lambda$? Wait until model selection and cross-validation.

How does the shrinkage work?
Consider two cases of columns of $X$ \begin{itemize}
	\item orthogonal
	\item correlated
\end{itemize}

\subsection*{Orthogonal columns} % (fold)
\label{sub:orthogonal_columns}

Suppose $X$ are orthogonal, in which case $X'X = I$. Therefore
the OLS solution is $\hat{\beta}_{\text{OLS}} = {X'X}^{-1}X'T = X'T$.
The ridge regression solution is given by 
\[\hat{\beta}_{\text{ridge}} = \brac{X'X + \lambda I}^{-1} X'T = \frac{1}{1+\lambda} \hat{\beta}_{\text{OLS}}\]
Hence the shrinkage!

The lasso solution:
\[\text{argmin}_\beta \brac{T - X\beta}'\brac{T - X\beta} + \lambda \sum_{j=1}^k \abs{\beta_j}\]
\begin{align*}
	\brac{T - X\beta}'\brac{T - X\beta} + \lambda \sum_{j=1}^k \abs{\beta_j}
	& = T'T - 2 \beta'X'T + \beta'X'X\beta + \lambda \sum_{j=1}^k \abs{\beta_j}\\
	& = T'T - 2 \beta'\hat{\beta}_{\text{OLS}} + \beta'X'X\beta + \lambda \sum_{j=1}^k \abs{\beta_j}
\end{align*}
The minimization problem is equivalent to the following (in this orthogonal case):
\[\text{argmin}_\beta \sum_{i=1}^k -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}\]
The problem is additively separable. Thus \begin{align*}
	\text{argmin}_{\beta_i\geq 0} -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}\\
	\text{argmin}_{\beta_i\geq 0} -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}
\end{align*}
The FOC for the first problem is given by:
\[\frac{\partial}{\partial \beta_i} = - \beta_{\text{OLS}\,i} + \beta_i + \frac{\lambda}{2}\]
whence the optimal value is given by:
\[\hat{\beta}_{\text{LASSO}\,i} \defn \brac{ \beta_{\text{OLS}\,i} - \frac{\lambda}{2} }^+\]

Due to soft thresholding in the case of the LASSO there is feature selection.

% subsection* orthogonal_columns (end)

\subsection*{The best subset selection} % (fold)
\label{sub:the_best_subset_selection}
Suppose we have a set of $p$ predictors. Chose the subset with the least
residual sum of squares.

For each $k = 1,\ldots, p$ pick the model with the lowest RSS.
\begin{align*}
\min_{\beta} \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}}^2 \\
\text{subject to} \sum_{j=1}^p 1_\obj{\beta_j\neq 0} \leq k
\end{align*}

This is equivalent to the following problem:
\[\min_{\beta} \sum_{i=1}^n \brac{ - 2 \beta_j \hat{\beta}_{\text{OLS}\,j} + \beta_j^2 }\]
subject to keeping at most $k$ coefficients nonzero.
\[\hat{\beta}_{\text{BSS}\,i} \defn \beta_{\text{OLS}\,i} 1_{\clop{\hat{\beta}, \infty}}\brac{ \abs{\beta_{\text{OLS}\,i}} }\]
where $\hat{\beta}$ is the $n-k$ th order statistic. This given hard thresholding.

Gram-Schmidt orthogonalization is equivalent to the QR decomposition.

The best idea is to use SVD decomposition of $X = U\Lambda V'$, with $U$ and $V$ orthogonal matrices.

% subsection* the_best_subset_selection (end)

\subsection*{SVD} % (fold)
\label{sub:svd}

Suppose there is a martux $X$ with dimensions $n\times p$ with rank $r$.
SVD decomposition claims that there exist square orthogonal matrices $U_{n\times n}$ and $V_{p\times p}$
and a diagonal matrix $\Lambda_{n\times p}$ of rank $r$, such that $X = U \Lambda V'$.
Orthogonality of $U$: $U'U = I_{n\times n}$.

Orthogonality greatly simplifies the projectors onto the column space of $X$: $\hat{T} = U U' T$.

% subsection* svd (end)

\subsection*{Ridge regression and the SVD} % (fold)
\label{sub:ridge_regression_and_the_svd}

Begin with the expression of the solution:
\[\hat{\beta}_{\text{ridge}} \defn \brac{X'X + \lambda I}^{-1} X'T\]

Since $X'X = V\Lambda U' U \Lambda V' = V\Lambda^2 V'$, it is true that 
\begin{align*}
	\hat{\beta}_{\text{ridge}} & = \brac{X'X + \lambda I}^{-1} X'T\\
	& = \brac{V\Lambda^2 V' + \lambda VV'}^{-1} X'T\\
	& = V \brac{\Lambda^2 + \lambda I}^{-1} V' V\Lambda U' T \\
	& = V \brac{\Lambda^2 + \lambda I}^{-1} \Lambda U' T
\end{align*}

Therefore the ridge regression shrinks the coefficients over the tiniest principal directions.

The sample correlation matrix
\[\frac{1}{n} X'X = \frac{1}{n} V'\Lambda^2 V\]
The principal components are in $V$
the eigenvalue-eigenvector pair for the sample correlation is given by the $\brac{\frac{\lambda_i^2}{n}, V_\clo{i}}$.

The $i$ th principal component is given by $Z_i = X V_\clo{i}$ and $\text{var}(z_i) = \lambda_i^2$

The ridge regression shrinks the coefficients corresponding to the principal directions in the column space of $X$ having the smallest variance.

% subsection* ridge_regression_and_the_svd (end)

\subsection*{The effective degree of freedom} % (fold)
\label{sub:the_effective_degree_of_freedom}

It is a function of the tuning parameter $\lambda$ which governs the complexity of the model.

Complexity id the trace of the hat matrix (the projector) of the ridge regression:
\[\text{df}(\lambda) \defn \text{tr}\brac{X\brac{X'X + \lambda I}^{-1}X'}\]
Using the SVD of $X$ gives us:
\[\text{tr}\brac{X\brac{X'X + \lambda I}^{-1}X'} = \sum_{j=1}^p \frac{\lambda_i^2}{\lambda_i^2 + \lambda^2}\]

If $\lambda = 0$ then $\text{df}(\lambda) = p$, and $\lambda\to +\infty$, then $\text{df}(\lambda) \to 0$.

Correlation between this and this (?)
\begin{align*}
	\text{Var}(T) = \sigma^2 I
\end{align*}
Covariance of $\hat{T}$ and $T$.
% \[\ex(T, \hat{T}) = T' T\] 

% subsection* the_effective_degree_of_freedom (end)

Take a matrix of predictors $X$, its standardized version \[X^\sigma \defn \frac{X}{\sigma}\]
and \[X^c \defn \frac{X-m}{\sigma}\]


% section lecture_3 (end)

\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}
Departing from the linearity of the Linear regression.

Transform $\brac{f_m}_{m=1}^M:\Real^p \to \Real$ since there are initially $P$ predictors.
Sample space is augmented and now has $M$ predictors. The regression function now becomes:
\[f(X) = \sum_{m=1}^M \theta_m f_m(x)\]

This model generalises the original linear regressions since $f_m(x) = \pi_m(x)$ 

Polynomial regression problem what happens in the tails. It is no t very trustworthy in near the endpoints.
In this respect Polynomial regression is no a good idea -- use local regression by partitioning the range into regions with different model in each area: $f_m(X) = 1_{\clo{l_m, u_m}}(x_k)$

Piecewise polynomials and splines

Suppose that $x$ is unidimensional -- only one feature.
%%% The following really resembles the dummy variables in ecnometrics.

Divide $x$ in several region with different thresholds(knots since they link the regions). To each region fit the constant.In order to cover the whole range of $x$ in this case one has to have three basis functions:

$1_{\ploc{-\infty, \xi_1}}$, $1_{\ploc{\xi_1, \xi_2}}$ and $1_{\ploc{\xi_2, \infty}}$.

Consider linear function of each region: fitting something piecewise linear. In this aces we nee 6 basis functions, since we don not only model the piecewise intercept but also the piecewise slope.

Enforcing continuity at two knots than four basis functions are only needed.

$h_1 = 1$ and $h_2 = x$ (no need for the indicator here, since it would introduce discontinuity), $h_3 = \brac{x-\xi_1}^+$ and $h_4 = \brac{x-\xi_2}^+$ -- fix their intercepts.

Overlapping region are refined into a partition, which is then estimated and the test on linear constraints of coefficients on the estimated coefficients.

To add smoothness at the knots, one has to impose continuity of the derivatives at the knots.

Three regions and two knots, require continuity up to the second derivative at the first knot.
The remaining degree of ``freedom'' at this knot is the convexity of the fitted line.
The set of basis function is $h_1 = 1$, $h_2 = x$, $h_3 = x^2$, $h_4 = x^3$, $h_5 = \brac{\brac{x-\xi_1}^+}^3$ and $h_6 = \brac{\brac{x-\xi_2}^+}^3$.


In general Splines are piecewise polynomial regression with smoothness at the binding knots:
\begin{align*}
	h_j(x) = x^{j-1}\quad i=1,\ldots, M\\
	h_{m+k}(x) = \brac{\brac{x-\xi_i}^+}^M \quad k=1,\ldots,K
\end{align*}

One polynomial -- global and smooth.
More polynomials with a local regression are not necessarily smooth.
Splines are local and smooth!

What to do with the endpoints?
Problems with the values out the common region of $x$.

Natural cubic splines: fit a linear model in the ends to achieve robustness and get more trustworthy prediction

Order-4-spline (cubic spline) $\to$ 6 basis functions ($K+4$ functions).
Natural cubic spline $\to$ 2 basis functions ($K$ basis functions):
\[G(x) = \sum_{j=1}^K \theta_j N_j(x)\]
where $\brac{N_j}_{j=1}^K$ is the set of basis functions for the natural cubic spline.

Goal: show that it is enough to know the value of the NCS at $K$ knots to know it everywhere.

$K$ -- the number of knots $\brac{\xi_k}_{k=1}^K$. Consider a regression problem on some interval $\clo{a,b}$ with $a<\xi_1\<\ldots<\xi_K<b$.

Let $g$ be the NCS -- piecewise function, cubic in the main region, linear at the ends.
Let $g_k = g(\xi_k)$ for all $k=1,\ldots K$.

Since NCS is cubic, the second derivative must be linear in $x$ between any two consecutive knots.

Let $\gamma_k=g''(\xi_k)$. Due to linearity at the ends $\gamma_K = \gamma_1 = 0$.

$g''(x) - \alpha x + \beta$ since $g''$ is linear.
...
thus we get
\begin{matrix}
	\gamma_k = \alpha_k \xi_k + \beta_k \\
	\gamma_{k+1} = \alpha_k \xi_{k+1} + \beta_k
\end{matrix}


g(x) = \frac{(x-\xi_k)g_{k+1} + (\xi_k-x)g_k}{\xi_{k+1}-\xi_k} - \frac{1}{6}(x-\xi_k)(\xi_{k+1}-x)
g(x) = g_1 - (\xi_1-x)g'(\xi_1) & \text{if}\,x\leq \xi_1 \\
g(x) = g_K - (x-\xi_K)g'(\xi_K) & \text{if}\,x\geq \xi_K \\


There must be some relation between the coefficients since continuity across the knots is required.
g'(x) = \frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}
	-\frac{1}{6} \brac{ \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} (x-\xi_k)(\xi_{k+1}-x) }
	+ \obj{
		\brac{1+\frac{x-\xi_k}{\xi_{k+1}-\xi_k}}\gamma_{k+1}
		+ \brac{1+\frac{\xi_{k+1}-x}{\xi_{k+1}-\xi_k}}\gamma_k
	} \brac{\xi_{k+1} + \xi_k - 2 x}

g'(\xi_k+) = \frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} - \frac{1}{6} \brac{\xi_{k+1}-\xi_k}\brac{\gamma_{k+1}+2\gamam_k}
g'(\xi_{k+1}-) =\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} + \frac{1}{6} \brac{\xi_{k+1}-\xi_k}\brac{2\gamma_{k+1}+\gamam_k}
% g'(\xi_k-) =\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}

Smoothness condition: for $g$ to be an NCS $g'(\xi_k+)=g'(\xi_{k+1}-)$.

Which yields:

\[ \frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} - \frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k}\]


All of the above in the matrix notation:
\[\underset{(K-2)\times K}{Q}'\underset{K\times1}{g} = \underset{(K-2)\times(K-2)}{R}\underset{(K-2)\times1}{\gamma}\]

the matrix $R$ is invertible whence its inverse exists.

% the structure of $R$ and $Q$ is on the sheet.

\noindent\textbf{Theorem}\hfill \\
The vectors $g$ and $\gamma$ fully specify the NCS if and only if $Q'g = R'\gamma$.

When this is satisfied, then 
\[\int_a^b \abs{g''(s)}^2 ds = \gamma' R \gamma = g' Q R^{-1} R R^{-1} Q g = g' K g\]

The nice observation is that this integral can be used as a penalty term in the regression problem.

\begin{align*}
	\int_a^b \abs{g''(s)}^2 ds &= \induc{\brac{g''(s)g'(s)}}_a^b - \int_a^b g'(s) g'''(s) ds \\
% Since on $(a, \xi_1]$ and $[\xi_K, b)$ the function $g$ is linear.
	&= - \int_{\xi_1}^{\xi_K} g'(s) g'''(s) ds = - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} g'(s) g'''(s) ds\\
% Due to piecewise linearity
	&= - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} g'(s) \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} ds\\
%  But $g'''$ is constant over each interval
	&= - \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} \int_{\xi_k}^{\xi_{k+1}} g'(s) ds\\
%  Since \int_{\xi_k}^{\xi_{k+1}} g'(s) ds = g(\xi_{k+1}) - g(\xi_k)
	&= - \sum_{k=1}^{K-1} \frac{\gamma_{k+1}-\gamma_k}{\xi_{k+1}-\xi_k} \brac{\gamma_{k+1}-\gamma_k}\\
	&= - \sum_{k=2}^{K-2} \brac{\frac{g_{k+1}-g_k}{\xi_{k+1}-\xi_k} - \frac{g_k-g_{k-1}}{\xi_{k+1}-\xi_k}} \gamma_k \\
	& = \gamma' Q' g = \gamma' R \gamma
\end{align*}

Putting things in other words:
the NCS has the minimum value of the smoothness integral among all smooth curves interpolating the data.

This means that formally we have the following:
with at leas two knots and $g$ being the NCS interpolating $z_1,\ldots,z_K$ on knots $\xi_1,\ldots,\xi_K$ within a bounded interval.

Then for any other interpolating function on the same mesh $\bar{g}$, the total variation of $\bar{g}''$ is necessarily grater than that of the NCS.


The function $h \defn g - \bar{g}$ has root at the knots $\xi_k$
Consider the difference of these integrals. 
\begin{align*}
	\int_a^b h'' g'' ds
%% Integrate by part using the nice properties of g 
	& = \brac{h'(s)g''(s)}_a^b - \int_a^b h'(s) g'''(s) ds \\
%% since g''' is zero outside \clo{\xi_1, \xi_K} and g'' is zero at the endpoints of the interval \clo{a,b}
	& = - \int_{\xi_1}^{\xi_K} h'(s) g'''(s) ds \\
	& = - \sum_{k=1}^{K-1} \int_{\xi_k}^{\xi_{k+1}} h'(s) g'''(s) ds \\
	& = - \sum_{k=1}^{K-1}  g'''(\xi_k+) \int_{\xi_k}^{\xi_{k+1}} h'(s) ds \\
	& = - \sum_{k=1}^{K-1}  g'''(\xi_k+) \brac{h(\xi_{k+1})-h(\xi_k)} = 0
\end{align*}


\int_a^b \abs{\bar{g}''}^2 ds &= 
\int_a^b \abs{g'' + h''}^2 ds = \int_a^b \abs{g''}^2 + \abs{h''}^2 ds + \int_a^b 2h''g'' ds= \int_a^b \abs{g''}^2 + \abs{h''}^2 ds \geq \int_a^b \abs{g''}^2 ds

\begin{itemize}
	\item the NCS on $\clo{a,b}$ is completely determined by the levels and derivatives at the interpolation knots;
	\item The NCS brings the value of the $\int_a^b \abs{g''}ds$ to its minimum in the class of all smooth curves interpolating the data at the same knots;
	\item The NCS with $K$ knots has $K$ basis functions.
\end{itemize}


Cubic spline with $K$ knots
\[f(x) = \sum_{k=1}^3 \beta_k x^k + \sum_{k=1}^K \theta_k \brac{\brac{x-\xi_k}^+}^3\]

% See the handwritten notes 


\subsection{Smoothing spline} % (fold)
\label{sub:smoothing_spline}

The goal of fitting a smoothing spline is among all functions $g(x)$ with two continuous derivatives, to find the one that minimizes the following penalized sum of squares
\[\RSS(g,\lambda) = \sum_{i=1}^n \brac{t_i - g(x_i)}^2 + \lambda \int \abs{g''(s)}^2 ds\]
Trying to make $g$ fit the data s much as possible and yet penalizing if it varies too much. This is basically a problem of calculus of variations. The penalty term sums up all non-linearities in the function $g$ and is higher the more oscillations the $g$ has.

$\lambda\to \infty$ then the optimal solution is linear $g$
$\lambda = 0$ then the optimal solution is some ``wild'' function, matching all the points, with no structure known beforehand.


This seems to be a problem of functional analysis.

consider the function $g(\xi_k) = \bar{g}(\xi_k)$ for all $k$.
Consider as $\bar{g}$ an NCS at knots given by $\brac{\xi_k}_{k=1}^K$.
Since an NCS has the lest variation integral, the solution of the problem cannot be anything but the NCS.
\[\int \abs{g''(s)}^2 ds\geq \int \abs{\bar{g}''(s)}^2 ds\]

Thus the optimal solution to this infinite dimensional problem is a quite finite dimensional NCS.
% Surprise
Therefore the problem reduces to the following quadratic optimization problem:
\[\RSS(g,\lambda) = {(t - g)}'{(t - g)} + \lambda g'K g \to \min_g\]
\[\RSS(g,\lambda) = t't - g't - t'g + g'( I + \lambda K )g  \to \min_g\]
using the matrix derivatives, the solution turns out to be
\[\frac{\partial}{\partial g} \quad:\quad 2( I + \lambda K )g - 2 t = 0\]
which reduces to
\[\hat{g} = \brac{I + \lambda K}^{-1} t\]

The matrix $\brac{I + \lambda K}^{-1}$ is called the \textbf{smoother matrix} $S_\lambda$.

Where does the smoothing take place? 

Define the effective degree freedom as, again, the trace of $S_\lambda$.
\[\text{df}(\lambda_ = \tr\brac{S_\lambda}\]

Use the SVD of $S_\lambda$ to extract the eigenvalues and eigenvectors.

Since $K \defn Q R^{-1} Q'$, whence $K$ is symmetric. The unit matrix is symmetric, hence $S_\lambda$ is symmetric and its inverse is symmetric.
% Thus it is an Hermitian matrix, whence the eigenvalues must be real.
\[S_\lambda = \sum_{i=1}^n \lambda_i u_i u_i' = U\Lambda U'\]

Since  $S_\lambda$ is positive-semidefinite, all eigenvalues are non-negative.

\begin{itemize}
	\item The eigenvalues of $K$ do not depend on the parameter $\lambda$;
	\item Both $S_\lambda$ and $K$ have the same eigenvectors.
\end{itemize}

If $\eta$ -- the eigenvalues of $K$, then $\text{det}\brac{K-\eta I} = 0$.
Thus \[\text{det}\brac{\frac{1}{\lambda}\brac{\brac{I+\lambda K} -(1 + \lambda \eta) I} } = 0\]

Thus $1 + \lambda \eta$ are the eigenvalues of $\brac{I+\lambda K}$, and $\frac{1}{1 + \lambda \eta}$ is the eigenvalue of $S_\lambda$.
Thus $\lambda_j = \frac{1}{1+\lambda \eta_{n-j+1}}$ is a decreasing function of $\lambda$.

Consider a $u_j$ the eigenvector of $S_\lambda$ corresponding to $\lambda_j$.
Then $S_\lambda u_j = \lambda_j u_j$ and
\[u_j = \lambda_j \brac{S_\lambda}^{-1} u_j
= \lambda_j \brac{I+\lambda K} u_j
= \lambda_j u_j + \lambda_j \lambda K u_j\]
Whence
\[K u_j = \frac{1-\lambda_j}{\lambda \lambda_j} u_j = \eta_{n-j+1} u_j\]
therefore the eigenvectors of $S_\lambda$ are the eigenvectors of $K$ (with other eigenvalues).


Thus due to the spectral decomposition:
\[g = S_\lambda t = \sum_{j=1}^n \lambda_j u_j u_j' t = \sum_{j=1}^n \lambda_j \brkt{u_j, t} u_j \]

Since each eigenvector $u_j$ is independent of the $\lambda$, the projections on are invariant under $\lambda$. Therefore models are comparable.

\[\sum_{j=1}^n \frac{1}{1+\lambda\eta_{n-j+1}} \brkt{u_j, t} u_j\]
some smoothing take place

The higher the $j$ the more smoothing take place:
	the less principal component is, the more smoothed it is.

\[g = \lambda_1\brkt{u_1, t} u_1 + \lambda_2\brkt{u_2, t} u_2 + \sum_{j=3}^n \lambda_j \brkt{u_j, t} u_j \]
The nature of the matrix $K$ makes the first $\lambda_1$ and $\lamba_2$ equal to $1$.

What is the relationship between $x$ and $u_1$?

The smoothing spline does not shrink linear components.

Higher $u_j$ correspond to more and more oscillating functions of $x$.

% Reproducing Hilbert spaces



\begin{itemize}
	\item
\end{itemize}

% subsection smoothing_spline (end)
% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

\[\RSS = \sum_{i=1}^n \brac{t_i - g(x_i)}^2 + \lambda \int \abs{g''(s)}^2ds\]
The solution to this problem is the Natural cubic spline , which is finite dimensional reduces the problem to
\[\brac{t-g}'\brac{t-g} + \lambda g'K g\]
where $K$ is a symmetric and positive  semi-definite matrix
the fitted $\hat{g}$ are determined by the 
\[S_\lambda = \brac{I+\lambda H}^{-1}\]

The smoother matrix $S_\lambda$ is positive semi-definite and has eigenvalues $\lambda_k$. Why does $\lambda_1$ and $\lambda_2$ are equal to $1$?

%% See handwritten notes

What happens when the soother is applied again to the smoothed output?
In the case of a linear regression we get the same output.

Re-applying the smoother to $\hat{g}$ yields a more linear fit, since the first pair of eigenvalues are unit, thus preserving the linear components, while other higher degree polynomial components diminish.

Indeed, \begin{align*}
	\hat{g}_2 & = \sum_{k=1}^n \lambda_k \brkt{\hat{g}, u_k} u_k \\
	& = \sum_{k=1}^n \lambda_k \brkt{\sum_{j=1}^n \lambda_j \brkt{g, u_j} u_j, u_k} u_k \\
	& = \sum_{k=1}^n \sum_{j=1}^n \lambda_j \lambda_k \brkt{g, u_j} \brkt{u_j, u_k} u_k \\
	& = \sum_{k=1}^n \sum_{j=1}^n \lambda_j \lambda_k \brkt{g, u_j} \delta_{kj} u_k \\
	& = \sum_{k=1}^n \lambda_k^2 \brkt{g, u_k} u_k \\
	& = \sum_{k=1}^n \lambda_k^2 \brkt{g, u_k} u_k
\end{align*}

The projection matrix is idempotent, hence for any eigenvector-eigenvalue pair $(\lamba, u)$ it is true that $\lamba u = Hu = H^2u = H\lamba u = \lamba Hu = \lamba^2 u$. Thus $\lamba$ is either $\pm1$ or $0$.

The solution to the weighted problem is the same Natural Cubic spline.
\[\sum_{i=1}^n \omega_i \brac{t_i - g(x_i)}^2 + \lamba \int \abs{g''(s)}^2 ds \tom \min\]
Since again, we can take an NCS with the same first sum but with lesser penalty term.

The solution is $\hat{g} = \brac{W+\lamba K}^{-1} Wt$.

\subsection{Model selection} % (fold)
\label{sub:model_selection}

%% See the handwritten notes

Four adjustments to the likelihood, which incorporate the complexity of the model.

Elements of information theory

Suppose $x$ is a discrete random variable.
The main question is how much information one gets when one observes a realisation of $X$.

THe amount of information is quantified by the amount of ``'surprise'':\begin{itemize}
	\item very likely -- not surprising;
	\item very unlikely -- very surprising.
\end{itemize}

$H$ the measure of information has the following properties: \begin{description}
	\item[Monotonicity] $H$ is a monotonic functional of the probability measure function;
	\item[Independence] If events are independent or unrelated, then the information gained from both must be poled together: in the case of densities this means that $H(X,Y) = H(X)+H(Y)$ when $p_{X,Y}(x,y) = p_X(x) p_Y(y)$;
	\item[]
\end{description}

Average amount of information for a discrete random variable
\[H(X) = \Ex\brac{H(X)} = - \sum_x p(x) \log_2 p(x)\]
for a continuous:
\[H(X) = \int \log_2\frac{1}{p(x)} p(x) dx\]

Suppose the $f$ is unknown distribution modelled using $\hat{f}$.

The Kullback-Leibler \textbf{divergence} between $f$ and $\hat{f}$: the additional information need to describe the random variable with $f$ using $\hat{f}$: \[\int f(x) \log_2\frac{1}{\hat{f}(x)} dx - \int f(x) \log_2\frac{1}{f(x)} dx \leq 0 \]

Use Jensen's inequality $\Ex g(X) \geq g\brac{\Ex X}$ for any convex $g$.
\begin{align*}
	-\int f(x) \log_2\frac{\hat{f}(x)}{f(x)} dx & \geq -\log_2\int f(x) \frac{\hat{f}(x)}{f(x)}  dx = 
\end{align*}

First criterion of model selection (Akaike; 1973).

Suppose $\brac{Y_i}_{i=1}^n$ is some data generate according to some unknown density $g\brac{\induc{y}\theta_0}$. Let there be a parametric family of models
\[M_K \defn \obj{\induc{f\brac{\induc{y}\theta_k}}\,\theta_k\in \Theta}\]
where $\Theta\subseteq \Real^K$ -- the $K$-dimensional parametric family.

Let $\hat{\theta}_k$ be the MLE estimator of the parameter in $\Theta$, and $\hat{f}_k = f\brac{\induc{y}\theta_k}$.

Assume that the models are nested, and are distinguished by their dimension $K$.

How far the $\hat{f}_k$ is from $g$.

The idea is to look a t the Kullback-Leibler divergence between $g$ and $f_k$, and get the $f_k$ with the smallest distance:
\[\text{KL}(g||f_k) = \int g(y) \log g(y) dy - \int g(y) \log f_k(y) dy\]
is the sum of the inherent entropy of $Y$ and the cross entropy.

The problem is to minimize this function, and it is equivalent to minimizing the Kullback-Leibler \textbf{discrepancy}:
\[d(\theta_k) = -2\int g(y) \log f_k(y) dy = - 2 \Ex_g\brac{\log f_k(Y)}\]

Instead	let's look at
\[d(\hat{\theta}_k) = - 2\induc{\Ex_g\brac{\log f(\induc{y}\theta)}}_{\theta=\theta_k(y)} \]

Since in practice $g$ is unknown, Akaike suggested to use the log likelihood as a proxy for $d(\hat{\theta}_k)$, which is a biased estimator of it.

In fact, using the law of large numbers
\[\frac{1}{N} \sum_{i=1}^n \log f(Y_i|\hat{\theta}_k) \overset{a.s}{\to} \Ex_g\brac{\log f(\induc{Y}\hat{\theta}_k)}\]

Therefore
\[-\frac{2}{N} \sum_{i=1}^n \log f(Y_i|\hat{\theta}_k) \approx d(\hat{\theta}_k)\] and Akaike has something great in mind.

In order to compute the bias, it is necessary look an the average Kullback-Leibler distance with respect to the distribution of the MLE estimate $\hat{\theta}_k$
\[\Ex_{\hat{\theta}_k} d(\hat{\theta}_k) - \Ex_g\brac{- \log f\brac{\induc{Y}\hat{\theta}_k(Y)}}\]

The AIC is given by (up to the terms of the first order):
\[-2\log \hat{f}\brac{\induc{y}\,\hat{\theta}_k(y)} + 2k\]

%% See the handwritten notes!

% subsection model_selection (end)

% section lecture_5 (end)


\end{document}