\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ex}[0]{{\mathbb{E}}}
\newcommand{\pr}[0]{{\mathbb{P}}}
\newcommand{\Var}[0]{{\text{Var}}}
\newcommand{\RSS}{{\text{RSS}}}
\newcommand{\TR}{{\text{Tr}}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\rank}{\mathop{\text{rank}}\nolimits}



\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Document title}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section*{Preliminaries} % (fold)
\label{sec:preliminaries}

\url{http://cs.hse.ru/ai/mmdm}

The folowing topics would be covered in the course:
\begin{itemize}
	\item Decision theory wit linear regression
	\item Shrinkage methods (Ridge regression, Lasso, elastic net)
	\item Polynomial regression, splines
	\item Model selection techniques, validation methods
	\item Classification problems (the Support Vector Machine, decision trees, bagging and boosting)
	\item Neural networks (flexible regression and classification)
\end{itemize}

Main books:
\begin{description}
	\item[Tibshirani2013] \hfill\\
		James, Witten, Hastie and Tibshirani (2013): ``An Introduction to Statistical Learning with Applications in R''
	\item[Bishop2006] \hfill\\
		Bishop (2006): ``Pattern Recognition and Machine Learning''
	\item[Tibshirani2013] \hfill\\
		Hastie, Tibshirani, Friedman (2013): ``The Elements of Statistical Learning''
\end{description}

% section* preliminaries (end)

\section{Lecture \# 1} % (fold)
\label{sec:lecture_1}

Linear regression lies in the class of supervised learning problems.
Such problems work with paired data of arbitrary nature: univariate, multivariate, categorical, numerical, abstract (like graphs in chemical classification problems) et c.
In general there is a learning (training) sample $\brac{x_i, T_i}_{i=1}^N$ of a pairs of an input variable $X$ and the associated output target $T$.

The goal is to construct a function $y(\cdot)$ of the input that predicts, forecasts, estimates or otherwise returns the target, associated with the given input.

The target variable $T$ is always considered to be a random variable at least due to measurement or other irreducible uncertainty, that is present in any system.
The input variable $X$, depending on the approach, may or may not be random.

In the case when it is an rv, the data is characterized by the join probability density (distribution) function $p_{X,T}\brac{x,t}$.
The joint pdf can be factorized into the density of the response conditional on a particular input and the density (probability) of the input:\[p_{X,T}\brac{x,t} = p_{\induc{T}X}\brac{\induc{t}x} p_X(x) \]
The analysis of the problem may now fork:
\begin{description}
	\item[Generative model] \hfill \\
		both the data and the response given data are modelled;
	\item[Discriminative model] \hfill \\
		model just the conditional response.
\end{description}

Statistical decision theory helps decide on a functional form of $y(\cdot)$:
choose $y(\cdot)$ so a to minimize the expected loss functional $\Ex_{X,T} L\brac{T,y(X)}$
over the bivariate distribution of the input data.
This way the incorrect estimation of $t$ by $y(x)$ given $x$ is penalized.

Among the possible loss functions the most frequently used is the quadratic
loss function $L(t,y) \defn \brac{t-y}^2$, which has its roots in the $L^p$ norm
\[\nrm{f}_p = \brac{\int \abs{f}^p d\mu}^\frac{1}{p}\] for $p = 2$.

Another possible loss function is the so called Minkowski cost, defined as $L(t,y) \defn \abs{t-y}$,
which is the $L^p$ norm with $p=1$.

Sidenote: conditional expectation as the projection of $f$ onto a subspace of $g$ measurable functions in the Hilbert space of square integrable functions with the usual $\int f\bar{g} d\mu$ inner product.

% Remember the measure theory? with its sigma-finite measure \mu with respect to which another measure \nu is absolutely continuous, and there is f\in L^1(\mu) such that \nu=\int f d\mu and \int gd\nu = \int fg d\mu.

% Which approach to use?
The joint randomness of $(X,T)$ leads to the following problem in the case of $L^2$ loss function:
\[\Ex_{X,T} \brac{T-y(X)}^2 \to \min_y\]
Using the tower property of the conditional expectation the optimal $y$ is the conditional expectation $\phi(X)\defn \Ex\brac{\induc{T}X}$.
Indeed, since $\Ex_{\induc{T}X} \brac{\brac{T-\phi(X)} } = 0$ , the mean squared error is
\begin{align*}
	\Ex_{X,T} \brac{T-y(X)}^2 = \Ex_{X} \brac{\Ex_{\induc{T}X} \brac{T-y(X)}^2 } \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{ \brac{T-\phi(X)}^2+\brac{\phi(X)-y(X)}^2} + 2 \Ex_{\induc{T}X} \brac{\brac{T-\phi(X)}\brac{\phi(X)-y(X)} }} \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{T-\phi(X)}^2+\brac{\phi(X)-y(X)}^2 } + 2 \Ex_X \brac{\brac{\phi(X)-y(X)} \Ex_{\induc{T}X} \brac{T-\phi(X)} } \\
		= \Ex_X \brac{ \Ex_{\induc{T}X} \brac{T-\phi(X)}^2 + \Ex_{\induc{T}X} \brac{\phi(X)-y(X)}^2 }
\end{align*}

The latter conditional expectation is vanishes whenever $y=\phi$, whilst the first one is independent of $y$ and can be considered as the irreducible variance due to structural variance.
The mean prediction error is the irreducible variance and the approximation error:
\[\text{error} \defn \Ex_X \Var\brac{\induc{T}X} + \Ex_x \brac{\brac{\phi(X)-y(X)}^2} \]

However it is never possible to achieve $y(x) = \Ex\brac{\induc{T}X=x}$.
Therefore some insight is needed on $\Ex_{\induc{T}X} \brac{\phi(X)-y(X)}^2$.
\begin{description}
	\item[Frequentist] $\pr\brac{\induc{\Dcal}\Theta}$ -- uncertainty is due to dataset
	\item[Bayesian] $\pr\brac{\induc{\Dcal}\Theta} \pi(\Theta)$ -- uncertainty is due parameters, where $\pi(\Theta)$ is the prior distribution.
	The posterior distribution is proportional to
		\[\pr\brac{\induc{\Theta}\Dcal} \sim \pr\brac{\induc{\Dcal}\Theta} \pi(\Theta)\]
\end{description}

Error is for a particular training dataset, on which the $y$ is estimated. In effect the functional form of $y$ depends on the data set $D$:
\[\Ex_\Dcal\brac{\text{Error}_\Dcal} = \text{Noise} + \Ex_\Dcal \Ex_X \brac{ y(x,D)-\phi(x) }^2\]
where noise is defined as $\Ex_X \Var\brac{\induc{T}X}$.

Now put $f(x) \defn \Ex_\Dcal y(x,D)$.
\begin{align*}
	\Ex_\Dcal\brac{ y(x,D)-\phi(x)\pm f(x) }^2 &
		= \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \Ex_\Dcal\brac{ f(x) - \phi(x) }^2 \\ &+ 2 \Ex_\Dcal\brac{ y(x,D) - f(x) }\brac{ f(x) - \phi(x) } \\
		& = \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \brac{ f(x) - \phi(x) }^2 \\&+ 2\brac{ f(x) - \phi(x) } \Ex_\Dcal\brac{ y(x,D) - f(x) } \\
		& = \Ex_\Dcal\brac{ y(x,D) - f(x) }^2 + \brac{ f(x) - \phi(x) }^2 \\
\end{align*}

Therefore the expected value of the loss is 
\[\Ex_\Dcal\brac{\text{Error}_\Dcal} = \text{Noise} + \text{Variance} + \text{Bias}^2\]
with the variance is due to the fact that the instance of $y$ is not static with varying dataset $D$,
and the bias is the measure of average closeness of the approximation $y(\cdot)$ to the real conditional expectation $\Ex\brac{\induc{T}X}$.
This is the decomposition of the loss into systemic noise, bias and variance.

% Non-parametric approach + low dimension: smoothing local data (kNN -- $k$ nearest neighbours).

Parametric approach would require a parametric expression of $y(\cdot)$ as an approximation of $\Ex\brac{\induc{T}X}$ at every $x$ (or at most almost all).

Suppose $\Lcal^2$ is the space in which the best approximation of $\phi(X)$ is searched for.
Projecting $\Ex\brac{\induc{T}X}$ onto $\Lcal^2$ yields $\phi(X) = h^\perp(X) + h(X)$, where $h^\perp \perp \Lcal^2$ (i.e. $\brkt{h^\perp,g} = 0$ for all $g\in \Lcal$) and $h\in \Lcal^2$.

% Here should be a picture of the projection: the conditional expectation is projected onto the subspace of square integrable functions.

% \Ex_\Dcal y(X,D) -- is an element of the 

Since $f-h\in\Lcal^2$ it must be true that
\[\Ex_X \brac{ f(X) - h(X) } h^\perp(X) = \brkt{f-h, h^\perp} = 0 \]
whence the following decomposition must hold:
\begin{align*}
	\text{Bias}^2 &= \Ex_X \brac{ \Ex_\Dcal y(X,D) - \phi(X) }^2 
		= \Ex_X \brac{ f(X) - h^\perp(X) - h(X) }^2 \\
		& = \Ex_X \brac{ f(X) - h }^2 + \Ex_X \brac{ h^\perp(X) }^2 + \Ex_X \brac{ f(X) - h(X) } h^\perp(X) \\
		& = \Ex_X \brac{ f(X) - h(X) }^2 + \Ex_X \brac{ h^\perp(X) }^2
\end{align*}
This means that the squared bias is the sum of how close the model is to the projection of $\Ex\brac{\induc{T}X}$ onto $\Lcal^2$, and how well the projection approximates the conditional expectation itself.

In linear regression problems the space $\Lcal^2$ is taken to be the set of all linear functions of $X$.

\subsection*{Linear regression} % (fold)
\label{sub:linear_regression}

Suppose $X$ is a $p$-dimensional vector of features and that we attempt to approximate the $\Ex\brac{\induc{T}X}$ with $y(x) \defn \beta_0 + \sum_{k=1}^p \beta_k x_k$.

In effect we suppose the following econometric model
\[T = \beta_0 + \sum_{k=1}^p \beta_k X_p + \epsilon;\,\epsilon\sim\Ncal(0,\sigma^2)\]
Therefore conditional upon $X$ the target $T$ is distributed like \[\induc{T}X\sim \Ncal(y(X),\sigma^2)\]

In this setting the following questions arise: \begin{itemize}
	\item How $\beta_{1\times(p+1)}$ is estimated?
	\item Which coefficients $\beta_k$ are significant?
	\item What is their interpretation?
\end{itemize}

In the matrix form, the linear regression problem is stated as follows:
\[\underset{n\times 1}{T} = \underset{n\times (1+p)}{X}\underset{(1+p)\times 1}{\beta} + \underset{n\times 1}{\epsilon}\]
where the vector $X$ of the predictors of the training set has been altered by adding a constant term -- the intercept. The predictors now look like this
\[X = \brac{ \one \defn \underset{n\times 1}{1}, \underset{n\times 1}{x_1}, \ldots, \underset{n\times 1}{x_p} }\]

Two approachs to estimating $\beta$ is possible: the \textbf{L}east \textbf{S}quares\footnotemark  and the \textbf{M}aximum \textbf{L}ikelihood.
Effectively the LS approach to parameter estimation is non-probabilistic.
There are different flavours of least squares: the ordinary, the generalised (with a weighting matrix), two-stage least squares (2SLS), to name a few, and others.
The 2SLS is an extension of the GLS that attempts to battle heteroskedasticity:
first run OLS to get the squared residuals, then GLS with weights that standardise the residuals.

The goal of LS is to solve the following problem: minimize the \textbf{R}esidual \textbf{S}um of \textbf{S}squares
\[\RSS \defn \brac{T-X\beta}'\brac{T-X\beta} \to \min_\beta\]
The first order conditions on the potential minimizer are in the matrix form:
\begin{align*}
	\frac{\partial}{\partial \beta}\RSS &= \frac{\partial}{\partial \beta}\brac{T'T-\beta'X'T - T'X\beta + \beta'X'X\beta}\\
	&= \frac{\partial}{\partial \beta} T'T - \frac{\partial}{\partial \beta} \beta'X'T - \frac{\partial}{\partial \beta} T'X\beta + \frac{\partial}{\partial \beta} \TR\brac{\beta'X'X\beta}\\
	&= - 2 \frac{\partial}{\partial \beta} T'X\beta + \beta'\brac{X'X + \brac{X'X}'}\\
	&= - 2 T'X + 2 \beta'X'X = 0
\end{align*}
If the matrix $X$ is full rank, then the matrix $X'X$ is invertible, and is positive-semidefinite.
Therefore $\frac{\partial^2}{\partial \beta\partial \beta}\RSS = 2 X'X \geq 0$, whence the Least Squares solution $\hat{\beta} \defn \brac{X'X}^{-1} X'T$ is indeed the minimizer of the RSS.

The best linear approximation to $\Ex\brac{\induc{T}X}$ is the projection of $T$ onto the space of linear functionals of $X$.

The matrix $\hat{H} \defn X\brac{X'X}^{-1}X'$ is called the \textbf{hat matrix} and is in fact a projector onto the linear subspace spanned by the column vectors of $X$.

Every projector matrix has the following property: if $v$ is in the spanned linear subspace, then $\hat{H}v = v$.
Indeed, since $v\in \clo{X}$, there must exist $\underset{(p+1)\times 1}{\alpha}\in \Real^{(p+1)}$ such that $v = X\alpha$.
Therefore
\[X\brac{X'X}^{-1}X'v = X\brac{X'X}^{-1}X'X\alpha = X\alpha = v\]

If the columns of $X$ are orthogonal, then $X'X$ is in fact a diagonal matrix with the squared Euclidean norms of columns of $X$ on the diagonal.
In this case each of $\brac{\beta_k}_{k=0}^n$ is the coefficient in the projection of $T$ onto th respective column of $X$.

% subsection* linear_regression (end)

\subsection*{Gram-Schmidt orthogonalisation} % (fold)
\label{sub:gram_schmidt_orthogonalisation}

Suppose there is a set of linearly independent vectors $\brac{f_i}_{i=1}^m$ in some Euclidean space $V$.
\begin{description}
	\item[Initialisation] \hfill \\
		Set $e_1 \defn \frac{1}{\nrm{f_1}} f_1$;
	\item[General step $k=1,\ldots,m-1$] \hfill \\
		Let \[u_{k+1} \defn f_{k+1} - \sum_{i=1}^k \frac{\brkt{e_i, f_{k+1}}}{\brkt{e_i,e_i}} e_i\]
		and put $e_{k+1}\defn \frac{1}{\nrm{u_{k+1}}} u_{k+1}$.
\end{description}
An effective algorithm is given in \emph{Golub \& Van Loan 1996} section 5.2.8.

% subsection* gram_schmidt_orthogonalisation (end)

%% I don't recall what the section below is for...
In a bivariate case $X = (\one,x)$ and
\[\hat{\beta} = \brac{\begin{matrix} \one'\one & \one'x\\ x'\one & x'x \end{matrix}}^{-1} \brac{\begin{matrix} \one't \\ x't \end{matrix}}
= \frac{1}{n^2 \brac{\overline{x^2} - \bar{x}^2}} \brac{\begin{matrix} \one'\one \one't - \one'x x't \\ - x'\one \one't + x'x x't \end{matrix}}
\]
since $\one'\one = n$ and $x'\one = \one'x = n\bar{x}$.

%% p-predictors

Set $\hat{z}_0 \defn \one$. For each $j=1,\ldots,p$ regressing $x_k$ onto $\brac{\hat{z}_j}_{j=0}^{k-1}$ to get the LS coefficients $\brac{\alpha_{jk}}_{j=0}^{k-1}$.
Construct $\hat{z}_k$ as the residuals of $x_k$ from projecting onto $\brac{\hat{z}_j}_{j=0}^{k-1}$ :
\[\hat{z}_k = x_k - \sum_{j=0}^{k-1} \alpha_{jk} \hat{z}_j\]
This is similar to the Gram-Schmidt procedure described previously.

Regressing $t$ on $z_k$ yields the coefficient $\hat{\beta}_k$ which is not the OLS coefficient, but shows new information gained from the $k^\text{th}$ predictor.

Discriminative approach.
Since $\hat{\beta}$ is a linear transformation of $T$ and $\hat{\beta} = \beta + \brac{X'X}^{-1}X'\epsilon$, \textbf{provided the model is specified correctly}, it is therefore true that
\[\hat{\beta}\sim\Ncal\brac{\beta,\brac{X'X}^{-1} \sigma^2}\]

In fact if $\epsilon\sim \Ncal\brac{0,\Sigma}$ -- i.e the noise has some other covariance structure, then $\Var(\hat{\beta}) = \brac{X'X}^{-1}X'\Sigma X\brac{X'X}^{-1}$ -- this is useful in GLS, mentioned above, especially if $\Sigma$ is decomposed into $C'C$ with $C$ -- non-singular lower triangular matrix (Cholesky).

Indeed, if we transform (weigh) the observations according to matrix $W$ with $W'W = \Sigma^{-1}$, then the LS model becomes $WT=WX\beta + W\epsilon$, whence
\[\hat{\beta} = \brac{X'W'WX}^{-1}X'W'WT = \brac{X'\Sigma^{-1}X}^{-1}X'\Sigma^{-1}T\]
whence $\hat{\beta}\sim\Ncal\brac{\beta, \brac{X'\Sigma^{-1}X}^{-1}}$.

\noindent\textbf{Cochran's Theorem} \hfill\\
Suppose $\brac{\xi_i}_{i=1}^n \sim\Ncal(0, 1)$ and that
\[\sum_{i=1}^m \xi_i^2 = \sum_{j=1}^k Q_j\]
where $Q_j = \xi' A_j \xi$ is a positive-semidefinite quadratic form for each $j=1,\ldots,k$.
If $\sum_{j=1}^k r_j = n$, with $r_j = \rank{Q_j}$, then \begin{enumerate}
	\item The random variables $Q_j$ are independent;
	\item each $Q_j\sim \chi^2_{r_j}$.
\end{enumerate}

Returning to the simple model, what is the estimator of $\sigma^2$?

It turns out that $\frac{\RSS}{\sigma^2}\sim \chi^2_{n-p-1}$.
Indeed, provided the model is specified correctly, \begin{align*}
	\RSS &= \brac{T-X\hat{\beta}}'\brac{T-X\hat{\beta}} \\
	& = \brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon}'\brac{T-X\beta - X\brac{X'X}^{-1}X'\epsilon} \\
	& = \epsilon' \brac{I - X\brac{X'X}^{-1}X'}'\brac{I - X\brac{X'X}^{-1}X'}\epsilon \\
	& = \epsilon' \brac{ I - X\brac{X'X}^{-1}X' } \epsilon = \epsilon' \brac{ I - \hat{H} } \epsilon
\end{align*}
Since the trace of an idempotent matrix equals its rank, $\rank(I - \hat{H}) = \TR(I) - \TR(\hat{H}) = n- (p+1)$, whence by Cochran's theorem $\frac{1}{\sigma}\RSS\sim\chi^2_{n-(p+1)}$.
Furthermore, $\hat{\sigma}^2 \perp \hat{\beta}$.

\subsection{Hypothesis testing} % (fold)
\label{sub:hypothesis_testing}

If $V_j \defn \brac{\brac{X'X}^{-1}}_{jj}$ -- the $j^\text{th}$ diagonal element of $\brac{X'X}^{-1}$,
whence $\Var\hat{\beta}_j\defn \sigma^2 V_j$.

To test $H_0:\beta_j=0$ versus $H_1:\beta_j\neq 0$, one should use the following statistic, which under the null is distributed as follows:
\[\frac{\sfrac{\hat{\beta}_j}{\sqrt{\sigma^2 V_j}}}{\sqrt{ \frac{\sfrac{n-p-1}{\sigma^2} \hat{\sigma}^2 }{ n-p-1 } }} \sim \frac{\Ncal(0, 1)}{\sqrt{\chi^2_{n-p-1}}} = t_{n-p-1}\]

The $\alpha$-level confidence interval for $\beta_j$ is defined as
\[\clo{ \hat{\beta}_j - \hat{\sigma} \sqrt{ V_j },  \hat{\beta}_j + \hat{\sigma} \sqrt{ V_j } }\]

For the false rejection rate of the t-test $\alpha = 0.05$, on average $5\%$ of statistically significant results would be false positives.

To test for simultaneous insignificance of $\brac{\beta_j}_{j=1}^p = 0$ one should use the F-test
(more generally an F-test for linear restriction). The following takes place under the null hypothesis that the simpler model is correct :
\[F = \frac{\sfrac{\RSS_0-\RSS_1}{p_1-p_0}}{\sfrac{\RSS_1}{n-p_1-1}}\sim F(p_1-p_0, n-p_1-1)\]

To gauge the model accuracy one can use both the RSS as a measure of lack of fit and the $R^2$ as a measure of goodness-of-fit.

If $\one\in \clo{X}$ then
\[ \text{TSS} = T' \brac{I - \one\brac{\one'\one}^{-1}\one'} T = T' \brac{I - X\brac{X'X}^{-1}X'} T + T' \brac{ X\brac{X'X}^{-1}X' - \one\brac{\one'\one}^{-1}\one'} T = \RSS + \text{ESS}\]

the goodness-of-fit measure $R^2$ is the ratio of the explained variance to the total variance
\[R^2 \defn \frac{\text{ESS}}{\text{TSS}} = 1 - \frac{\RSS}{\text{TSS}} \]
For a simple case of a bivariate regression
\[R^2 = \rho^2 = \frac{\abs{\text{corr}(x,t)}^2}{\Var(x)\Var(t)} = \beta^2 \frac{\Var(x)}{\Var(t)}\]


% subsection hypothesis_testing (end)

% section lecture_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}
% 2015-01-20

The uncertainty on the model for a new input $x_0$ 

Prediction of the value of $T$ at a new input $x_0$ has two sources of uncertainty: \begin{enumerate}
	\item Uncertainty of the first type -- due to the estimation of $y(\cdot)$ ($\hat{\beta}$) in $\Lcal^2_X$;
	\item Uncertainty of the second type -- due to the additive noise term. Even if the true $\beta$ were known, the true values of the response $T$ at $x_0$ would still be unknown.
\end{enumerate}
Predictive intervals incorporate the noise as well as uncertainty due to imprecise estimation of the coefficients.

Variability due to estimation of an approximate model.

\subsection{Shrinkage models} % (fold)
\label{sub:shrinkage_models}

In the ordinary linear squares method we minimized the following fitness function:
\[\text{min} \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2\]

Ridge regression
Minimize the following penalized Residual Sum of Squares as:
\[\RSS\defn \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2 + \lambda \sum_{j=1}^k \beta_j^2\]
the $\lambda>0$ is called the tuning parameter.
It controls the amount of shrinkage:
if $\lambda = 0$ then there is absolutely no shrinkage, and the optimal beta coincide with the OLS.
But if $\lambda\to \infty$ then the optimal coefficient vector tends to zero. 

However with shrinkage, the scale of each predictor $\brac{x_j}_{j=0}^k\in \Real^n$ matter!

Thus the first step is to standardize the variables before fitting the Ridge regression.

Next observation: the intercept $\beta_0$ is indeed excluded from the shrinkage term.

Suppose the $t$ and $X$ are standardized (mean and centre), which means that $\beta_0 \equiv 0$
Then in the matrix form the following is minimized:
\[\brac{t-X\beta}'\brac{t- X\beta} + \lambda \beta'\beta \to \text{min}\]

The first order conditions are:
\begin{align*}
	\frac{\partial \RSS}{\partial \beta } &= -2 X\brac{t-X\beta}+2\lambda \beta\\
	% \frac{\partial \RSS}{\partial \lambda } = \beta'\beta = 0
\end{align*}

The optimal coefficients are given by
\[\hat{\beta}_\text{ridge}\defn \brac{X'X + \lambda I}^{-1}X't\]

% revise matrix derivatives.

% subsection shrinkage_models (end)

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

Limiting the size of the coefficients. Modified RSS with a tuning parameter $\lambda$.
\[\RSS\defn \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ij}}^2 + \lambda \sum_{j=1}^k \beta_j^2\]
important: \begin{itemize}
	\item Consider dimensionless inputs (standardized by standard deviation) -- comparable $\beta_j$.
	\[x_{ij}\]
	\item If the input vectors are centred, then the estimation is separable.
\end{itemize}
Consider \begin{align*}
	\RSS &= \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^k \beta_j \brac{x_{ij}-\bar{x}_j} - \sum_{j=1}^k \beta_j \bar{x}_j}  + \lambda \sum_{j=1}^k \beta_j^2 \\
	\RSS' &= \sum_{i=1}^n \brac{t_i - \beta_0^c - \sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}  + \lambda \sum_{j=1}^k {\beta_j^c}^2
\end{align*}
Notice that $\beta_j^c = \beta_j$ for all $j=1,\ldots,n$ and $\RSS=\RSS'$.
\begin{align*}
	\RSS' &= \lambda \sum_{j=1}^k {\beta_j^c}^2 + \sum_{i=1}^n \brac{t_i - \beta_0^c}^2 \\
	& + \sum_{i=1}^n \brac{\sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}^2 \\
	& - 2 \sum_{i=1}^n \brac{t_i - \beta_0^c} \sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}
\end{align*}
Notice that the last term is zero. Indeed \begin{align*}
	\sum_{i=1}^n \brac{t_i - \beta_0^c} \brac{\sum_{j=1}^k \beta_j^c \brac{x_{ij}-\bar{x}_j}}
	& = \sum_{j=1}^k \beta_j^c \sum_{i=1}^n \brac{t_i - \beta_0^c} \brac{x_{ij}-\bar{x}_j}
\end{align*}
The problem of ridge regression in the matrix form, where $X$ the $n\times k$
is matrix of centred standardized independent variables with no intercept:
\[\RSS = \brac{t-X\beta}'\brac{t-X\beta} + \lambda \beta'\beta\]

The solution is
\[\hat{\beta}_{\text{ridge}}\defn \brac{X'X + \lambda I}^{-1} X'T\]

An equivalent optimization problem:
\begin{align*}
	\hat{\beta}_{\text{ridge}} = \text{argmin}_{\beta} \sum_{i=1}^n \brac{t_i - \sum_{j=1}^k \beta_j x_{ij}^c}
	\text{subject to } \sum_{j=1}^k \beta_j^2 \leq \eta
\end{align*}
$\eta$ -- the budget, $\lambda$ -- the complementary slackness coefficient.

There is a certain universality of the statement of the regression problem in this form:
it is possible to plug in different constraints ($L^1$ constraints -- LASSO).
LASSO is automatically performing feature selection due to $L^1$ norm.

% How to tune $\lambda$? Wait until model selection and cross-validation.

How does the shrinkage work?
Consider two cases of columns of $X$ \begin{itemize}
	\item orthogonal
	\item correlated
\end{itemize}

\subsection*{Orthogonal columns} % (fold)
\label{sub:orthogonal_columns}

Suppose $X$ are orthogonal, in which case $X'X = I$. Therefore
the OLS solution is $\hat{\beta}_{\text{OLS}} = {X'X}^{-1}X'T = X'T$.
The ridge regression solution is given by 
\[\hat{\beta}_{\text{ridge}} = \brac{X'X + \lambda I}^{-1} X'T = \frac{1}{1+\lambda} \hat{\beta}_{\text{OLS}}\]
Hence the shrinkage!

The lasso solution:
\[\text{argmin}_\beta \brac{T - X\beta}'\brac{T - X\beta} + \lambda \sum_{j=1}^k \abs{\beta_j}\]
\begin{align*}
	\brac{T - X\beta}'\brac{T - X\beta} + \lambda \sum_{j=1}^k \abs{\beta_j}
	& = T'T - 2 \beta'X'T + \beta'X'X\beta + \lambda \sum_{j=1}^k \abs{\beta_j}\\
	& = T'T - 2 \beta'\hat{\beta}_{\text{OLS}} + \beta'X'X\beta + \lambda \sum_{j=1}^k \abs{\beta_j}
\end{align*}
The minimization problem is equivalent to the following (in this orthogonal case):
\[\text{argmin}_\beta \sum_{i=1}^k -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}\]
The problem is additively separable. Thus \begin{align*}
	\text{argmin}_{\beta_i\geq 0} -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}\\
	\text{argmin}_{\beta_i\geq 0} -2 \beta_i \beta_{\text{OLS}\,i}+ \beta)i^2 + \lambda \abs{\beta_i}
\end{align*}
The FOC for the first problem is given by:
\[\frac{\partial}{\partial \beta_i} = - \beta_{\text{OLS}\,i} + \beta_i + \frac{\lambda}{2}\]
whence the optimal value is given by:
\[\hat{\beta}_{\text{LASSO}\,i} \defn \brac{ \beta_{\text{OLS}\,i} - \frac{\lambda}{2} }^+\]

Due to soft thresholding in the case of the LASSO there is feature selection.

% subsection* orthogonal_columns (end)

\subsection*{The best subset selection} % (fold)
\label{sub:the_best_subset_selection}
Suppose we have a set of $p$ predictors. Chose the subset with the least
residual sum of squares.

For each $k = 1,\ldots, p$ pick the model with the lowest RSS.
\begin{align*}
\min_{\beta} \sum_{i=1}^n \brac{t_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}}^2 \\
\text{subject to} \sum_{j=1}^p 1_\obj{\beta_j\neq 0} \leq k
\end{align*}

This is equivalent to the following problem:
\[\min_{\beta} \sum_{i=1}^n \brac{ - 2 \beta_j \hat{\beta}_{\text{OLS}\,j} + \beta_j^2 }\]
subject to keeping at most $k$ coefficients nonzero.
\[\hat{\beta}_{\text{BSS}\,i} \defn \beta_{\text{OLS}\,i} 1_{\clop{\hat{\beta}, \infty}}\brac{ \abs{\beta_{\text{OLS}\,i}} }\]
where $\hat{\beta}$ is the $n-k$ th order statistic. This given hard thresholding.

Gram-Schmidt orthogonalization is equivalent to the QR decomposition.

The best idea is to use SVD decomposition of $X = U\Lambda V'$, with $U$ and $V$ orthogonal matrices.

% subsection* the_best_subset_selection (end)

\subsection*{SVD} % (fold)
\label{sub:svd}

Suppose there is a martux $X$ with dimensions $n\times p$ with rank $r$.
SVD decomposition claims that there exist square orthogonal matrices $U_{n\times n}$ and $V_{p\times p}$
and a diagonal matrix $\Lambda_{n\times p}$ of rank $r$, such that $X = U \Lambda V'$.
Orthogonality of $U$: $U'U = I_{n\times n}$.

Orthogonality greatly simplifies the projectors onto the column space of $X$: $\hat{T} = U U' T$.

% subsection* svd (end)

\subsection*{Ridge regression and the SVD} % (fold)
\label{sub:ridge_regression_and_the_svd}

Begin with the expression of the solution:
\[\hat{\beta}_{\text{ridge}} \defn \brac{X'X + \lambda I}^{-1} X'T\]

Since $X'X = V\Lambda U' U \Lambda V' = V\Lambda^2 V'$, it is true that 
\begin{align*}
	\hat{\beta}_{\text{ridge}} & = \brac{X'X + \lambda I}^{-1} X'T\\
	& = \brac{V\Lambda^2 V' + \lambda VV'}^{-1} X'T\\
	& = V \brac{\Lambda^2 + \lambda I}^{-1} V' V\Lambda U' T \\
	& = V \brac{\Lambda^2 + \lambda I}^{-1} \Lambda U' T
\end{align*}

Therefore the ridge regression shrinks the coefficients over the tiniest principal directions.

The sample correlation matrix
\[\frac{1}{n} X'X = \frac{1}{n} V'\Lambda^2 V\]
The principal components are in $V$
the eigenvalue-eigenvector pair for the sample correlation is given by the $\brac{\frac{\lambda_i^2}{n}, V_\clo{i}}$.

The $i$ th principal component is given by $Z_i = X V_\clo{i}$ and $\text{var}(z_i) = \lambda_i^2$

The ridge regression shrinks the coefficients corresponding to the principal directions in the column space of $X$ having the smallest variance.

% subsection* ridge_regression_and_the_svd (end)

\subsection*{The effective degree of freedom} % (fold)
\label{sub:the_effective_degree_of_freedom}

It is a function of the tuning parameter $\lambda$ which governs the complexity of the model.

Complexity id the trace of the hat matrix (the projector) of the ridge regression:
\[\text{df}(\lambda) \defn \text{tr}\brac{X\brac{X'X + \lambda I}^{-1}X'}\]
Using the SVD of $X$ gives us:
\[\text{tr}\brac{X\brac{X'X + \lambda I}^{-1}X'} = \sum_{j=1}^p \frac{\lambda_i^2}{\lambda_i^2 + \lambda^2}\]

If $\lambda = 0$ then $\text{df}(\lambda) = p$, and $\lambda\to +\infty$, then $\text{df}(\lambda) \to 0$.

Correlation between this and this (?)
\begin{align*}
	\text{Var}(T) = \sigma^2 I
\end{align*}
Covariance of $\hat{T}$ and $T$.
% \[\ex(T, \hat{T}) = T' T\] 

% subsection* the_effective_degree_of_freedom (end)

Take a matrix of predictors $X$, its standardized version \[X^\sigma \defn \frac{X}{\sigma}\]
and \[X^c \defn \frac{X-m}{\sigma}\]




% section lecture_3 (end)

\end{document}