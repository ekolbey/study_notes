\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac}
% \usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}

\newcommand{\Xcal}{\mathcal{X}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{\rus{Структурно-классификационные методы интеллектуального анализа}}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section{Lecture \#1} % (fold)
\label{sec:lecture_1}
\selectlanguage{russian}
Сбор и обработка информации была неотъемлемой составляющей протекавших ``цивилизационных процессов'':
в процессе усложнения деятельности человека появлялся всё больший объём данных, усложнялись иерархия, типы и структура данных.
Вобщем-то наблюдался экспоненциальный рост объёмов данных.

Середина 19 века -- качественный скачок благодаря зарождению статистики.
Начало 20 века -- появление возможности автоматизированной обработки информации.

1960 г. Зарождение структурно-классификационного анализа данных.

\begin{description}
	\item[Браверман:] Построение кусочных гиперплоскостей в многомерном пространстве рецепторов персептрона Розенблатта (\eng{Rosenblat's Perceptron})
	\item[Айзерман:] Расширение возможности автоматов -- моделирование способности к обучению и самообучению, характерной биологическим системам.
\end{description}

Дискриминантный анализ, факторный анализ -- синонимы структурно-классификационного анализа и \eng{Data Mining}.

Модель -- куб данных: \[ \brac{ x_{ij}(t) }_{i\in G, j\in M, t\in T} \subseteq X\] где $G$ -- объекты, $M$ -- признаки и $T$ -- временная шкала.
Задача -- построение сжатого содержательно хорошо интерпретируемого описания объектов.
Выявление структуры: \begin{itemize}
	\item объектов в различных подпространствах параметров;
	\item взаимосвязи параметров;
	\item динамических характеристик.
\end{itemize}

Например передача данных по ограниченному каналу: \begin{enumerate}
	\item Разбить данные на непересекающиеся группы;
	\item Выбрать в каждой представителя;
	\item Вместо всего объёма данных передать параметры групп, представителей групп и несколько близких к ним наблюдений.
\end{enumerate}

Можно задать метрику, можно ``меру сходства''.

Задача -- разбить пространство $X$ на $r$ локальных областей так, чтобы ``близкие'' точки попадали в одну группу, ``далёкие'' -- в разные.

\textbf{Гипотеза компактности (Браверман, 1961~г.)}\hfill\\
Отдельные содержательные классы соответствуют компактно расположенным группам точек (локальные области).

Два основных типа алгоритмов: эвристические и формальные.
Формальный подход решает задачу классификации относительно чётко сформулированного критерия и предлагает оптимальную процедуру принятия решений при появлении новой неизвестной точки.
В кластеризации -- построение разделяющих гиперповерхностей (при дискрименантном анализе -- гиперплоскостей). Спрямляющее пространство в случае \eng{SVM}.

Иерархическая кластеризация.

Пусть $\Xcal$ некоторое пространство и $X\subseteq \Xcal$ конечное множество точек (данных).
Пусть $\Xcal$ имеется либо метрика, либо мера близости между точками, обозначенная через $d$.
Имеется мера близости \[K(a,b) \defn \exp\brac{ -\lambda d^p(a,b) }\]
или иная нелинейно убывающая функция $d$.
\begin{description}
\item[Индексация] \hfill \\
Выбираем $a,b\in X$ такие, что $a$ и $b$ максимально удалены друг от друга,
и добавляем $a$ и $b$ к классам $A$ и $B$ соответственно,
устанавливаем $C = \obj{a,b}$. 
Относим очередной $z\in X\setminus C$ к тому классу $S=A$ или $B$, к которому она ``ближе'' согласно $K(z,S)$,
определяемой через \[K(z,S) \defn \frac{1}{\abs{S}} \sum_{m\in S} K(z,m)\]

\item[``Сферизация''] \hfill \\
Имеется порог $T>0$. Каждая группа характеризуется центром.
Принадлежность к группе определяется пороговой близостью.

Первоначально $G = \obj{x}$ для некоторого $x\in X$ и $C=G$.
Для очередного $x\in X\setminus C$ устанавливаем $C = C\cup \obj{x}$ и определеяем группу для $x$:
если нет $g\in G$ такого, что $d(x,g)\leq T$, то $G = G\cup \obj{x}$, иначе добавляем $x$ к группе \[g = \mathop{\text{argmin}}_{g\in G} d(x,g)\]
Обновления центров не производится. (\textbf{УТОЧНИТЬ!})

Низкий порог $T$ приводит к дроблению ``истинных'' классов, в то время как высокое значение $T$ угрубляет классы.

\item[``Объединение'']\hfill \\
Пусть мера близости множеств $C$ и $D$ определеяется через среднюю близость их элементов: \[K(C,D) \defn \frac{ \sum_{i\in C}\sum_{j\in D} K(i,j)}{\abs{C} \abs{D}}\]

Пусть имеется разбиение $\pi$ множества $X$.
Выберем $A,B\in \pi$, $A\cup B = \emptyset$, такие что \[K(A,B) = \min_{P,Q\in \pi, P\neq Q} K(P,Q)\]
и образуем более грубое разбиение $\pi'$ объединением классов $A$ и $B$: \[\pi ' = \pi\setminus\obj{ A, B } \cup \obj{ A\cup B }\]
продолжаем построение последовательности вложенных грубеющих разбиений пока не будет получено разбиение с желаемым числом классов.

Начальное разбиение $\pi$ либо задаётся отдельными элементами $X$ если объём $X$ невелик, либо случайным или иным образом, в случае если $\abs{X}$ велико.

Напирмер, можно использовать алгоритм ``спектр'':
Пусть $I_1 \defn \emptyset$. На шаге $k\geq0$ выбирается одна точка $x_{k+1}\in X\setminus I_k$,
так чтобы близость $x_{k+1}$ и $I_k$ была наибольшей (если $I_k$ пусто то выбирается произвольная)
и устанавливается $I_{k+1} \defn I_k \cup \obj{x_{k+1}}$.
Пусть близость обозначается как $K_{k+1} \defn K(x_{k+1},I_k)$

Рабочая шипотеза состоит в том, что при переходе из одной группы в другую, близость будет скачкообразно уменьшаться.
Группы создаются на основе $K$ наибольших относительных скачков $\Delta_k \defn 1-\frac{K_{k-1}}{K_k}$
следующим образом: точки между двумя последовательыми максимумами $\Delta_k$ относятся к одному множеству начального разбиения.

\item[Упрощённое объединение] \hfill \\
\item[Иерархическая таксономия] \hfill \\
\item[Алгоритм с поиском моды] \hfill \\

\end{description}
Индексация
Пусть 

алгоритму ``распознавания образов с учителем'' на основании потенциальных функций




% section lecture_1 (end)

\end{document}
