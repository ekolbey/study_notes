\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac}
% \usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}

\newcommand{\Xcal}{\mathcal{X}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{\rus{Структурно-классификационные методы интеллектуального анализа}}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section{Lecture \#1} % (fold)
\label{sec:lecture_1}
\selectlanguage{russian}
Сбор и обработка информации была неотъемлемой составляющей протекавших ``цивилизационных процессов'':
в процессе усложнения деятельности человека появлялся всё больший объём данных, усложнялись иерархия, типы и структура данных.
Вобщем-то наблюдался экспоненциальный рост объёмов данных.

Середина 19 века -- качественный скачок благодаря зарождению статистики.
Начало 20 века -- появление возможности автоматизированной обработки информации.

1960 г. Зарождение структурно-классификационного анализа данных.

\begin{description}
	\item[Браверман:] Построение кусочных гиперплоскостей в многомерном пространстве рецепторов персептрона Розенблатта (\eng{Rosenblat's Perceptron})
	\item[Айзерман:] Расширение возможности автоматов -- моделирование способности к обучению и самообучению, характерной биологическим системам.
\end{description}

Дискриминантный анализ, факторный анализ -- синонимы структурно-классификационного анализа и \eng{Data Mining}.

Модель -- куб данных: \[ \brac{ x_{ij}(t) }_{i\in G, j\in M, t\in T} \subseteq X\] где $G$ -- объекты, $M$ -- признаки и $T$ -- временная шкала.
Задача -- построение сжатого содержательно хорошо интерпретируемого описания объектов.
Выявление структуры: \begin{itemize}
	\item объектов в различных подпространствах параметров;
	\item взаимосвязи параметров;
	\item динамических характеристик.
\end{itemize}

Например передача данных по ограниченному каналу: \begin{enumerate}
	\item Разбить данные на непересекающиеся группы;
	\item Выбрать в каждой представителя;
	\item Вместо всего объёма данных передать параметры групп, представителей групп и несколько близких к ним наблюдений.
\end{enumerate}

Можно задать метрику, можно ``меру сходства''.

Задача -- разбить пространство $X$ на $r$ локальных областей так, чтобы ``близкие'' точки попадали в одну группу, ``далёкие'' -- в разные.

\textbf{Гипотеза компактности (Браверман, 1961~г.)}\hfill\\
Отдельные содержательные классы соответствуют компактно расположенным группам точек (локальные области).

Два основных типа алгоритмов: эвристические и формальные.
Формальный подход решает задачу классификации относительно чётко сформулированного критерия и предлагает оптимальную процедуру принятия решений при появлении новой неизвестной точки.
В кластеризации -- построение разделяющих гиперповерхностей (при дискрименантном анализе -- гиперплоскостей). Спрямляющее пространство в случае \eng{SVM}.

Иерархическая кластеризация.

Пусть $\Xcal$ некоторое пространство и $X\subseteq \Xcal$ конечное множество точек (данных).
Пусть $\Xcal$ имеется либо метрика, либо мера близости между точками, обозначенная через $d$.
Имеется мера близости \[K(a,b) \defn \exp\brac{ -\lambda d^p(a,b) }\]
или иная нелинейно убывающая функция $d$.
\begin{description}
\item[Индексация] \hfill \\
Выбираем $a,b\in X$ такие, что $a$ и $b$ максимально удалены друг от друга,
и добавляем $a$ и $b$ к классам $A$ и $B$ соответственно,
устанавливаем $C = \obj{a,b}$. 
Относим очередной $z\in X\setminus C$ к тому классу $S=A$ или $B$, к которому она ``ближе'' согласно $K(z,S)$,
определяемой через \[K(z,S) \defn \frac{1}{\abs{S}} \sum_{m\in S} K(z,m)\]

\item[``Сферизация''] \hfill \\
Имеется порог $T>0$. Каждая группа характеризуется центром.
Принадлежность к группе определяется пороговой близостью.

Первоначально $G = \obj{x}$ для некоторого $x\in X$ и $C=G$.
Для очередного $x\in X\setminus C$ устанавливаем $C = C\cup \obj{x}$ и определеяем группу для $x$:
если нет $g\in G$ такого, что $d(x,g)\leq T$, то $G = G\cup \obj{x}$, иначе добавляем $x$ к группе \[g = \mathop{\text{argmin}}_{g\in G} d(x,g)\]
% Обновления центров не производится. (\textbf{УТОЧНИТЬ!})

Обновление центров сфер производится после того, как будут покрыты все точки из исходного набора данных -- при этом выбытия точек из группы не происходит.
Итерации производятся до тех пор пока не будет достигнута устойчивость групп.

Низкий порог $T$ приводит к дроблению ``истинных'' классов, в то время как высокое значение $T$ ``угрубляет'' классы.

\item[``Объединение'']\hfill \\
Пусть мера близости множеств $C$ и $D$ определеяется через среднюю близость их элементов: \[K(C,D) \defn \frac{ \sum_{i\in C}\sum_{j\in D} K(i,j)}{\abs{C} \abs{D}}\]

Пусть имеется разбиение $\pi$ множества $X$.
Выберем $A,B\in \pi$, $A\cup B = \emptyset$, такие что \[K(A,B) = \min_{P,Q\in \pi, P\neq Q} K(P,Q)\]
и образуем более грубое разбиение $\pi'$ объединением классов $A$ и $B$: \[\pi ' = \pi\setminus\obj{ A, B } \cup \obj{ A\cup B }\]
продолжаем построение последовательности вложенных грубеющих разбиений пока не будет получено разбиение с желаемым числом классов.

Начальное разбиение $\pi$ либо задаётся отдельными элементами $X$ если объём $X$ невелик, либо случайным или иным образом, в случае если $\abs{X}$ велико.

Напирмер, можно использовать алгоритм ``спектр'':
Пусть $I_1 \defn \emptyset$. На шаге $k\geq0$ выбирается одна точка $x_{k+1}\in X\setminus I_k$,
так чтобы близость $x_{k+1}$ и $I_k$ была наибольшей (если $I_k$ пусто то выбирается произвольная)
и устанавливается $I_{k+1} \defn I_k \cup \obj{x_{k+1}}$.
Пусть близость обозначается как $K_{k+1} \defn K(x_{k+1},I_k)$

Рабочая шипотеза состоит в том, что при переходе из одной группы в другую, близость будет скачкообразно уменьшаться.
Группы создаются на основе $K$ наибольших относительных скачков $\Delta_k \defn 1-\frac{K_{k-1}}{K_k}$
следующим образом: точки между двумя последовательными максимумами $\Delta_k$ относятся к одному множеству начального разбиения.

\item[Упрощённое объединение] \hfill \\
\item[Иерархическая таксономия] \hfill \\
\item[Алгоритм с поиском моды] \hfill \\

\end{description}
Индексация
Пусть 

алгоритму ``распознавания образов с учителем'' на основании потенциальных функций


% section lecture_1 (end)


\section{Lecture \#2} % (fold)
\label{sec:lecture_2}

Структура объектов: Объекты имеют набор информативных параметров.

\textbf{Гипотеза компактности}\hfill\\
содержательным классам объектов соответствуют компактно расположенные группы (локальные области).

Структуризация и выбор информативных параметров.


Разбиение графа соответствующего матрице близости $A$ на сильно связные подграфы.

Алгоритм с поиском моды
максимум функции распределения плотности

Идея: есть эмпирическая функция плотности некоторого распределения, найти по выборке максимум плотности.
Берём сферу (интервал) с некоторым центром, ищем градиент центра сферы и передвигаем сферу в том направлении:
поиск моды происходит за счёт перекоса плотности.
Малый радиус -- неустойчивость моды, Большой радиус -- медленная сходимость.

\textbf{Вариационный подход}\hfill\\
Сформулировать критерий качества структуризации $I$ зависящего от разбиения пространства $\mathcal{X}$
на области, что интуитивное представление о ``хорошести'' разбиения пространства на компактные области.

Переменные -- меры близости между группами точек (и самими точками).

Если задано $X\subseteq \mathcal{X}$ конечное, то можно перебором оптимизировать функционал.

Если же задача поставлена в ``нормальном режиме'', то наблюдения поступают последовательно и поэтому необходимо заранее конструировать оптимальные разбиения:
множество $X = \brac{x_k}_{k\geq 1}\in \mathcal{X}$ -- счётное.

Например, задача расщепления смеси распределений.

Метод математического программирования Беллмана.

Группа работ в которых задача поставлена точно
\begin{itemize}
	\item Schlezinger 1965
	\[R \defn \sum_{j=1}^r p_j \iint S(x,y) \Pr\brac{\induc{x}\,x\in C_j}\Pr\brac{\induc{y}\,y\in C_j} dxdy\]
	где $S$ -- мера близости между точками, $\Pr\brac{\induc{x}\,x\in C_j}$ -- условная вероятность $x$ если $x$ принадлежит классу $C_j$.

	В случае квадратичной близости, критерий вырождается в \[R = \sum_{j=1}^r p_j \brac{x-C_j}^2\] алгоритм $k$-средних.

	\begin{enumerate}
		\item Разбиваются точки выборки на два непустых классов случайной гиперплоскостью; 
		\item Пересчитываются центры классов;
		\item Строится новая разделяющая гиперплоскость, ортогональная прямой между центрами новых классов;
		\item Повторяем сначала.
	\end{enumerate}

	Пусть $\mathcal{X}$ -- Евклидово пространство и есть $X\subseteq \mathcal{X}$.
	Пусть $u\in \mathcal{X}$ задаёт некоторую плоскость как ортогональное пространство к $u$:
	\[u^\perp \defn \obj{\induc{x\in \mathcal{X}}\, \brkt{u,x} = 0}\]
	Разделяющая гиперплоскость: $x\in C_1$ когда $\brkt{u,x}>0$ и $x\in C_2$ если $\brkt{u,x}<0$.
	Можно добавить $\pm\epsilon$ область неопределённости.

	\item \hfill \\
	\[\Phi_1 \defn \sum_{k=1}^r \frac{1}{n_k} \sum_{x,y\in A_k} d(x,y)\]
	Можно интерпретировать как средняя мера удалённости точек в группах.

	% Пропущен слайд~19

	Матрица близости

\end{itemize}

Критерия на базе ковариационных матриц членов классов (эллипсоид рассеяния).

Оптимальная разделяющая гиперплоскость ортогональна собственному вектору матрицы ковариации с наибольшим собственным значением.

дивергенция Кульбака.
\[\text{div}(A,B) \defn \int \log\frac{p_B}{p_A} \brac{p_A-p_B} d\mu\]

% слайд~23

\[F\defn \log\frac{\rho l}{\brac{1+\lambda}\brac{1+d}}\]
где $d\defn \sum_{a\in A} \frac{d_a}{N-\abs{A}}$ и $d_a$ -- длина кратчайшего незамкнутого пути, соединяющего проходящего через все точки группы $a\in A$.

Критерий средней близости по группам в пределах каждой группы:
\begin{align*}
	I_1&\defn \frac{1}{r}\sum_{i=1}^r K(A_i, A_i)\\
	I_2&\defn \frac{1}{r(r-1)}\sum_{i=1}^r\sum_{j\neq i} K(A_i, A_j)
\end{align*}
где \[K(A, B)\defn \frac{\abs{A}\abs{B}} \sum_{x\in A}\sum_{y\in B} K(x,y)\]
При этом если $A=B$, но суммируется по парам $x\neq y$ и корректируется знаменатель $\abs{B}\to\abs{B}-1$.

Функция $K$ от близости должна быть ``сжатием'': $K(r)\leq r$.

Минимизируем $I_2$ и максимизируем $I_1$.

Последовательная дихотомия: делим на два класса, затем наихудщий на два класса и так далее.

Варьирование количества классов $r$ сводится к целочисленному программированию, что сложно.

Возникла идея комбинирования $I_1$ и $I_2$: 
\begin{align*}
	I_3 &\defn I_1 - \lambda I_2\\
	I_4 &\defn \frac{I_1}{I_2}\\
	I_5 &\defn \frac{I_1 - \lambda I_2}{I_1 + \lambda I_2}
\end{align*}

Если брать только $I_1$, то он монотонно возрастает и оптимальными будет универсальный класс, а если брать $I_2$ то чем больше классов, тем ниже критерий $I_2$. Поэтому взвешивание (? обосновать) -- компромисс интересов.

$1$-локальная оптимизация для получения глобального экстремума

Пусть $A_k$-- $k$-й класс точек, $k=1,2$.
Берём последовательность точек $\brac{x_i}_{i=1}^n$ и зацикливаем её.
Пусть $\rho_i = 1_{A_1}(x_i)$.
Рассчитываем $\Delta_i \defn I\brac{A_1\cup \obj{x_i}}-I\brac{A_2\cup \obj{x_i}}$ -- либо оставляем точку в её классе либо перебрасываем.

Повторяем до тех пор, пока разбиение не стабилизировалось за $n$  шагов (да-да то же самое $n$).

$m$-локальная оптимизация, где $m$ -- глубина перебора.

Исследуем необходимость переброса $s$ точек из перечисления всех $s$ подмножеств $n$
(да-да, часть точек переходи из $1$ в $2$ и другие обратно $2\to1$).

Сокращённый перебор: следует брать точки близкие к границе класса.
см.~слайд~31

Перебросив $m$ точек, пербрасываем $m+1$ точек -- прицип Беллмана (? обдумать).

% слайд~32: Линейное упорядочивание классов, совпадающее с порядком на точках

% Слайды~40-43
Вариационная постановка задачи классификации (бесконечный вариант)
\[R(A,B,\alpha,\beta)\defn \int_A F_A(x,\alpha,\beta) p(x) d\mu + \int_B F_B(x,\alpha,\beta) p(x) d\mu\]
где $\alpha$ и $\beta$ -- векторные параметры классов, $F_A$ и $F_B$ -- потери при отнесении $x$ к соответствующему классу ($A$ и $B$).

Алгоритм Цыпкина-Кельманса.
Равенство нулю первой вариации по параметрам и области интегрирования.

% Формула Грина (?)
Опечатка в формуле 42.3 должно быть $x\in \Lambda$.

Предположения об ансамбле $\Pr(x)$ для робастности.





% section lecture_2 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

Постановка задчи расщепления смеси рапсредлений.

Положим что выборка $\brac{x_k}_{k=1}^n\sim G$, где $G(x;\theta_1,\theta_2) = \alpha F_1\big(x;\theta_1\big) + (1-\alpha)F_2\big(x;\theta_2\big)$ при некотором $\alpha\in \brac{0,1}$.

Задача состоит в том, чтобы определить $\theta_i$ и $\alpha$.

Самые многочисленные работы в область расщепления смеси нормальных распределений.

Для контроля качества расщепления необходимо вводить функцию потерь.

Важное замечание: задча расщепления более избыточная нежели чем обычная кластеризация. Если знаем $F_i$ то можно определить дискримиантную разделяющую поверхность. Однако знать эти функции заранее почти невозможно.

Используется в специфических случаях: когда необходимо знать ещё и параметры распределений.

Задача о разладке








% section lecture_7 (end)


\end{document}
