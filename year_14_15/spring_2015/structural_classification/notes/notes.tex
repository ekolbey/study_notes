\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Pwr}{\mathcal{P}}

\newcommand{\Xcal}{\mathcal{X}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{\rus{Структурно-классификационные методы интеллектуального анализа}}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

\section{Lecture \#1} % (fold)
\label{sec:lecture_1}
\selectlanguage{russian}
Сбор и обработка информации была неотъемлемой составляющей протекавших ``цивилизационных процессов'':
в процессе усложнения деятельности человека появлялся всё больший объём данных, усложнялись иерархия, типы и структура данных.
Вобщем-то наблюдался экспоненциальный рост объёмов данных.

Середина 19 века -- качественный скачок благодаря зарождению статистики.
Начало 20 века -- появление возможности автоматизированной обработки информации.

1960 г. Зарождение структурно-классификационного анализа данных.

\begin{description}
	\item[Браверман:] Построение кусочных гиперплоскостей в многомерном пространстве рецепторов персептрона Розенблатта (\eng{Rosenblat's Perceptron})
	\item[Айзерман:] Расширение возможности автоматов -- моделирование способности к обучению и самообучению, характерной биологическим системам.
\end{description}

Дискриминантный анализ, факторный анализ -- синонимы структурно-классификационного анализа и \eng{Data Mining}.

Модель -- куб данных: \[ \brac{ x_{ij}(t) }_{i\in G, j\in M, t\in T} \subseteq X\] где $G$ -- объекты, $M$ -- признаки и $T$ -- временная шкала.
Задача -- построение сжатого содержательно хорошо интерпретируемого описания объектов.
Выявление структуры: \begin{itemize}
	\item объектов в различных подпространствах параметров;
	\item взаимосвязи параметров;
	\item динамических характеристик.
\end{itemize}

Например передача данных по ограниченному каналу: \begin{enumerate}
	\item Разбить данные на непересекающиеся группы;
	\item Выбрать в каждой представителя;
	\item Вместо всего объёма данных передать параметры групп, представителей групп и несколько близких к ним наблюдений.
\end{enumerate}

Можно задать метрику, можно ``меру сходства''.

Задача -- разбить пространство $X$ на $r$ локальных областей так, чтобы ``близкие'' точки попадали в одну группу, ``далёкие'' -- в разные.

\textbf{Гипотеза компактности (Браверман, 1961~г.)}\hfill\\
Отдельные содержательные классы соответствуют компактно расположенным группам точек (локальные области).

Два основных типа алгоритмов: эвристические и формальные.
Формальный подход решает задачу классификации относительно чётко сформулированного критерия и предлагает оптимальную процедуру принятия решений при появлении новой неизвестной точки.
В кластеризации -- построение разделяющих гиперповерхностей (при дискрименантном анализе -- гиперплоскостей). Спрямляющее пространство в случае \eng{SVM}.

Иерархическая кластеризация.

Пусть $\Xcal$ некоторое пространство и $X\subseteq \Xcal$ конечное множество точек (данных).
Пусть $\Xcal$ имеется либо метрика, либо мера близости между точками, обозначенная через $d$.
Имеется мера близости \[K(a,b) \defn \exp\brac{ -\lambda d^p(a,b) }\]
или иная нелинейно убывающая функция $d$.
\begin{description}
\item[Индексация] \hfill \\
Выбираем $a,b\in X$ такие, что $a$ и $b$ максимально удалены друг от друга,
и добавляем $a$ и $b$ к классам $A$ и $B$ соответственно,
устанавливаем $C = \obj{a,b}$. 
Относим очередной $z\in X\setminus C$ к тому классу $S=A$ или $B$, к которому она ``ближе'' согласно $K(z,S)$,
определяемой через \[K(z,S) \defn \frac{1}{\abs{S}} \sum_{m\in S} K(z,m)\]

\item[``Сферизация''] \hfill \\
Имеется порог $T>0$.

Точка $z\in X\setminus C$ относится к 

\end{description}
Индексация
Пусть 

алгоритму ``распознавания образов с учителем'' на основании потенциальных функций




% section lecture_1 (end)

\end{document}
