\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
%\usepackage{fullpage}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, xfrac}
\usepackage{mathtools}

\usepackage{tikz}
\usetikzlibrary{shapes}
\colorlet{light blue}{blue!50}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

\newcommand{\abs}[1]{\lvert #1\rvert}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}

\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ical}{\mathcal{I}}

\newcommand{\ex}[0]{{\mathbb{E}}}
\newcommand{\pr}[0]{{\mathbb{P}}}
\newcommand{\var}[0]{\text{var}}

\newcommand{\defn}{\overset{\Delta}{=}}

\newcommand{\re}{\operatorname{Re}\nolimits}
\newcommand{\im}{\operatorname{Im}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{\rus{Зачётная работа по курсу \\ ``Теоретико-вероятностные методы в статистике''}}
\author{\rus{Назаров Иван,} \rus{101мНОД(ИССА)}}

\begin{document}
\selectlanguage{russian}
\maketitle

\section{У2.12} % (fold)
\label{sec:problem_2_12}

Итак необходимо изучить моменты любого порядка случайной величины с плотностью
\[
p(x\rvert a) = C \frac{e^{-\frac{1}{2}\ln^2 x}}{x}\bigl(1 + a\sin(2\pi \ln x)\bigr)
\]
при $\lvert a\rvert \leq 1$. Именно при этих $a$ функция $p(x\rvert a)$ всюду неотрицательна.

Итак необходимо вычислить при целых $k$ выражение
\[
\ex_{p(x\rvert a)} X^k
= C \int_0^\infty x^k \frac{e^{-\frac{1}{2}\ln^2 x}}{x}\bigl(1 + a\sin(2\pi \ln x)\bigr) dx
\]

Проведём замену переменных в интеграле $y = \ln x$ ($x = e^y$, $dy = \frac{dx}{x}$) и получим:
\[
\ex_{p(x\rvert a)} X^k
= C \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2}\bigl(1 + a\sin(2\pi y)\bigr) dy
\]

Подынтегральное выражение есть сумма
\[e^{ky} e^{-\frac{1}{2}y^2} + a e^{ky} e^{-\frac{1}{2}y^2}\sin(2\pi y)\]
в силу линейности, искомый интеграл есть сумма интегралов каждого из слагаемых.

Первое слагаемое (константа $C$ опущена):
\begin{align*}
	\int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2}
	&= \int_{-\infty}^\infty e^{-\frac{1}{2}( y^2 - 2ky + k^2 - k^2)} dy\\
	&= e^{\frac{1}{2}k^2} \sqrt{2\pi} \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}( y-k)^2 } dy\\
	&= e^{\frac{1}{2}k^2} \sqrt{2\pi}
\end{align*}

Для работы со вторым слагаемым заменим $2\pi$ под синусом на параметр $t$.
Также заметим, что для любого $\theta\in \Real$
\[\sin \theta = \frac{1}{2i} \bigl( e^{i\theta} - e^{-i\theta} \bigr)\]
Тогда второе слагаемое (опять-таки константа $C$ опущена) принимает вид:
\begin{align*}
	a \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} \sin(t y) dy
	&= a \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} \sin(t y) dy\\
	&= \frac{a}{2i} \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} \bigl(e^{t i y} - e^{-t i y}\bigr) dy
\end{align*}

Здесь опять интеграл разбивается на два слагаемых, причём одно отличается
от другого знаком при $t$ и перед интегралом -- рассмотрев подробнее одно,
автоматически получим из него другое. Итак
\begin{align*}
	\int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} e^{it y} dy
	&= \int_{-\infty}^\infty e^{-\frac{1}{2}y^2 + ky - \frac{1}{2}k^2 + \frac{1}{2}k^2 } e^{it (y-k) + it k} dy\\
	&= e^{\frac{1}{2}k^2} e^{it k} \int_{-\infty}^\infty e^{-\frac{1}{2}(y-k)^2} e^{it (y-k)} dy\\
	&= \bigl[ x = y-k \bigr] = e^{\frac{1}{2}k^2} e^{it k} \int_{-\infty}^\infty e^{-\frac{1}{2}x^2} e^{it x} dx
\end{align*}

Исследуем интеграл
\[\int_{-\infty}^\infty e^{-b x^2} e^{it x} dx\]
при произвольном $b>0$ и преобразуем его в следующее выражение:
\begin{align*}
	\int_{-\infty}^\infty e^{-b x^2} e^{it x} dx
	&= \int_{-\infty}^\infty e^{-b (x^2 - 2\frac{it}{2b} x - \frac{t^2}{4b^2} + \frac{1}{4b^2}t^2 )} dx\\
	&= \int_{-\infty}^\infty e^{-b (x - \frac{it}{2b})^2} e^{-\frac{1}{4b}t^2} dx\\
	&= e^{-\frac{1}{4b}t^2} \int_{-\infty}^\infty e^{-b (x - \frac{it}{2b})^2} dx
\end{align*}

Для нахождения
\[\int_{-\infty}^\infty e^{-b (x - \frac{it}{2b})^2} dx\]
заменим $\frac{t}{2b}$ на параметр $\eta$ и рассмотрим всюду аналитическую функцию
комплексного аргумента $f(z) = e^{-b (z - i\eta)^2}$, $z\in \Cplx$. Нужно вычислить
значение 
\[\int_\Real e^{-b (z - i\eta)^2} dz
%% сведение к пределу теоремой о доминируемой сходимости
= \lim_{R\to\infty} \int_{-R}^R e^{-b (z - i\eta)^2} dz \]

По теореме Коши, для любой функции $f(z)$ аналитической в области $R$ и содержащей
замкнутый контур $\Ccal$, такой что $\Ccal\sim 0$ гомеоморфным преобразованием
сводящийся к одной точке, справедливо
\[\oint_\Ccal f(z) dz = 0\]

Возьмём произвольное $R>0$ и рассмотрим прямоугольный контур $\Box$ в $\Cplx$ с
узлами $\pm R$ и $\pm R+i\eta$. Поскольку $f(z)$ аналитична всюду, то для любого
$R>0$ внутри контура не содержатся особенности $f(z)$. Поскольку $\Box\sim 0$ в
$\Cplx$ то
\[\oint_\Box f(z) dz = 0\]
Но был специально выбран так, чтобы состоять из прямых путей в $\Cplx$. Поэтому
в вольной записи
\[
\oint_\Box f(z) dz
= \Bigl( \int_{-R}^R + \int_R^{R+i\eta} + \int_{R+i\eta}^{-R+i\eta} + \int_{-R+i\eta}^{-R} \Bigr) f(z) dz
\]
Искомый интеграл равен
\[\int_{-R}^R e^{-b (z - i\eta)^2} dz
= - \Bigl(\int_R^{R+i\eta} + \int_{R+i\eta}^{-R+i\eta} + \int_{-R+i\eta}^{-R} \Bigr) e^{-b (z - i\eta)^2} dz\]
Итак на ``отрезке'' $[-R+i\eta,R+i\eta]$ функция $f(z)$ легко интегрируется
\begin{align*}
	\int_{R+i\eta}^{-R+i\eta} e^{-b (z - i\eta)^2} dz
	&= \int e^{-b (z - i\eta)^2} 1_{[-R+i\eta,R+i\eta]}(z) dz\\
	&= \bigl[x = z-i\eta\bigr] = \int_R^{-R} e^{-b x^2} dx = -\int_{-R}^R e^{-b x^2} dx
\end{align*}
На $[R,R+i\eta]$ нужен анализ: 
\begin{align*}
	\int_R^{R+i\eta} e^{-b (z - i\eta)^2} dz
	&= \int e^{-b (z - i\eta)^2} 1_{[R,R+i\eta]}(z) dz\\
	&= \bigl[z = R+is \bigr] = \int e^{-b (R + is - i\eta)^2} 1_{[0,\eta]}(s) ids\\
	&= \int_0^\eta i e^{-b \bigr(R + i(s - \eta)\bigl)^2} ds\\
	&= \bigl[\xi = s-\eta \bigr] = \int_{-\eta}^0 i e^{-b(R + i\xi)^2} d\xi\\
	&= \int_{-\eta}^0 i e^{-b (R^2 - \xi^2 + 2 i R\xi)} d\xi
	= e^{-bR^2} \int_{-\eta}^0 i e^{b\xi^2} e^{- i 2bR\xi} d\xi
\end{align*}
Заметим, что этот интеграл имеет очень хорошее свойство:
\begin{align*}
	\biggl\lvert \int_R^{R+i\eta} e^{-b (z - i\eta)^2} dz \biggr\rvert
	&= e^{-bR^2} \biggl\lvert \int_{-\eta}^0 i e^{b\xi^2} e^{- i 2bR\xi} d\xi \biggr \rvert\\
	&\leq e^{-bR^2} \int_{-\eta}^0 \Bigl\lvert i e^{b\xi^2} e^{- i 2bR\xi} \Bigr\rvert d\xi\\
	&= e^{-bR^2} \int_{-\eta}^0 e^{b\xi^2} \Bigl\lvert e^{- i 2bR\xi} \Bigr\rvert d\xi\\
	&= e^{-bR^2} \int_{-\eta}^0 e^{b\xi^2} d\xi \leq e^{-bR^2} \eta e^{b\eta^2}
\end{align*}
поскольку как ни ``крути'' $\theta$ всегда выполняется $\abs{e^{i\theta}} = 1$.
Поэтому этот интеграл бесконечно мал при $R\to\infty$.

\noindent На $[-R+i\eta, R]$ аналогичные выкладки приводят к
\[
\int_{-R+i\eta}^R e^{-b (z - i\eta)^2} dz
= e^{-bR^2} \int_0^{-\eta} i e^{b\xi^2} e^{i 2bR\xi} d\xi
\]
откуда опять-таки 
\[
\biggl\lvert \int_{-R+i\eta}^R e^{-b (z - i\eta)^2} dz \biggr\rvert
\leq e^{-bR^2} \eta e^{b\eta^2}
\]

\noindent Возвращаясь к интегралу $\int_{-R}^R e^{-b (z - i\eta)^2} dz$ имеем
\begin{multline*}
	\biggl\lvert \int_{-R}^R e^{-b (z - i\eta)^2} dz +
		\int_{R+i\eta}^{-R+i\eta} e^{-b (z - i\eta)^2} dz \biggr\rvert = \\
	= \biggl\lvert \int_{-R}^R e^{-b (z - i\eta)^2} dz -
		\int_{-R}^R e^{-b x^2} dx \biggr\rvert \leq 2 e^{-bR^2} \eta e^{b\eta^2}
\end{multline*}
Поскольку $\eta$ -- это конечное число, отсюда вытекает 
\[
\lim_{R\to\infty}\int_{-R}^R e^{-b (z - i\eta)^2} dz
= \lim_{R\to\infty} \int_{-R}^R e^{-b x^2} dx
= \int_{-\infty}^\infty e^{-b x^2} dx
\]
Однако
\[
\int_{-\infty}^\infty e^{-b x^2} dx
= \bigl[ x = \frac{1}{\sqrt{2b}}s\bigr]
= \frac{1}{\sqrt{2b}} \int_{-\infty}^\infty e^{-\frac{s^2}{2}} ds
= \sqrt{\frac{2\pi}{2b}}
\]
Таким образом справедливо, что 
\[
\int_{-\infty}^\infty e^{-b (z - i\eta)^2} dz
= \lim_{R\to\infty} \int_{-R}^R e^{-b (z - i\eta)^2} dz
= \sqrt{\frac{2\pi}{2b}}
\]

\noindent Эти рассуждения позволяют получить следующее равенство:
\[
\int_{-\infty}^\infty e^{-b x^2} e^{it x} dx
= e^{-\frac{1}{4b}t^2} \sqrt{\frac{\pi}{b}}
\]
В свою очередь, при $b=\frac{1}{2}$ отсюда следует, что
\[
\int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} e^{it y} dy
= e^{\frac{1}{2}k^2} e^{it k} \int_{-\infty}^\infty e^{-\frac{1}{2}x^2} e^{it x} dx
= e^{\frac{1}{2}k^2} \sqrt{2\pi} e^{-\frac{1}{2}t^2} e^{it k}
\]
Заменив $t$ на $-t$, получаем
\[
\int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} e^{-it y}
= e^{\frac{1}{2} k^2} \sqrt{2\pi} e^{-\frac{1}{2}t^2} e^{-it k}
\]

\noindent Далее
\begin{multline*}
	a \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} \sin(t y) dy \\
	= \frac{a}{2i} \Bigl( \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} e^{it y} dy
		- \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} e^{-it y} dy \Bigr)\\
	= a e^{\frac{1}{2}k^2} \sqrt{2\pi} e^{-\frac{1}{2}t^2} \frac{1}{2i} \bigl( e^{it k} - e^{-it k}\bigr)\\
	= a e^{\frac{1}{2}k^2} \sqrt{2\pi} e^{-\frac{1}{2}t^2} \sin(t k)
\end{multline*}
Подставляя $t = 2\pi$ получаем
\[
a \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} \sin(2\pi y) dy
= a e^{\frac{1}{2}k^2} \sqrt{2\pi} e^{-\frac{1}{2}t^2} \sin( 2\pi k)
= 0
\]
поскольку все $k$ -- целые.

Итак $k$-й момент распределения равен
\begin{align*}
	\ex_{p(x\rvert a)} X^k
	&= C \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2}\bigl(1 + a\sin(2\pi y)\bigr) dy\\
	&= C \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2}
		+ C a \int_{-\infty}^\infty e^{ky} e^{-\frac{1}{2}y^2} \sin(2\pi y) dy\\
	&= C e^{\frac{1}{2}k^2} \sqrt{2\pi} + 0
\end{align*}
В частности для $k=0$ из условия $\int p(x\rvert a) dx = 1$ следует, что $C = \frac{1}{\sqrt{2\pi}}$.
Получается, что моменты распределения не зависят от параметра $a$.

Из проведённого анализа можно сделать вывод, что знания даже счётного числа моментов
распределения не всегда достаточно для восстановления его параметров.

% section problem_2_12 (end)

\section{У2.18} % (fold)
\label{sec:problem_2_18}

Итоговая случайная величина $X$ при известном $D$ имеет нормальное распределение
с плотностью
\[p_{X\rvert D}(x\rvert d) = \frac{1}{\sqrt{ 2\pi d}} e^{-\frac{x^2}{2d}} \]
Совместная функция плотности $(X,D)$ может быть разложена в следующее произведение
\[p_{X,D}(x,d) = p_{X\rvert D}(x\rvert d) p_D(d)\]
Отсюда следует, что 
\[
p_X(x)
= \int_0^\infty p_{X,D}(x,t) dt
= \int_0^\infty p_{X\rvert D}(x\rvert t) p_D(t) dt
\]

Рассмотрим подробнее случайную величину $D$ с плотностью
\[p_D(x) = \frac{\lambda}{\sqrt{2\pi x^3}} e^{-\frac{\lambda^2}{2x}}\]
Пусть случайная величина $Z = \frac{\lambda^2}{D}$. Тогда её плотность равна
\[p_Z(z) = p_D(d) \frac{1}{\lvert -\frac{\lambda^2}{d^2}\rvert}\big\rvert_{d=\frac{\lambda^2}{z}}\]
откуда получается
\[
p_Z(z)
= \frac{\lambda z^\frac{3}{2}}{\sqrt{2\pi \lambda^6}} e^{-\frac{1}{2}z} \frac{\lambda^2}{z^2}
= \frac{1}{\sqrt{2\pi}} z^{\frac{1}{2}-1} e^{-\frac{1}{2}z}
\]
Таким образом $Z$ имеет распределение $\Gamma\bigl(\frac{1}{2},\frac{1}{2}\bigr)$, которое на
самом деле есть $\chi^2_1$.

Учитывая данное наблюдение, модель итоговой случайной величины принимает вид:
\[X\rvert Z\sim \Ncal\bigl(0,\frac{\lambda^2}{Z}\bigr)\text{ где } Z\sim \Gamma\bigl(\frac{1}{2},\frac{1}{2}\bigr)\]
Заметим, что условная плотность $X$ при заданном $Z=z$ равна
\[p_{X\rvert Z}(x\rvert z) = \frac{z^\frac{1}{2}}{\sqrt{ 2\pi \lambda^2}} e^{-\frac{x^2}{2\lambda^2}z}\]
Тогда искомая безусловная плотность $X$ равна
\[
p_X(x)
= \int_0^\infty p_{X\rvert Z}(x\rvert z) p_Z(z) dz
= \int_0^\infty \frac{z^\frac{1}{2}}{\sqrt{ 2\pi \lambda^2}} e^{-\frac{x^2}{2\lambda^2}z}
	\frac{1}{\sqrt{2\pi}} z^{\frac{1}{2}-1} e^{-\frac{1}{2}z} dz
\]
Интеграл упрощается следующим образом:
\begin{align*}
	p_X(x)
	&= \int_0^\infty \frac{z^\frac{1}{2}}{\sqrt{ 2\pi \lambda^2}} e^{-\frac{x^2}{2\lambda^2}z}
		\frac{1}{\sqrt{2\pi}} z^{\frac{1}{2}-1} e^{-\frac{1}{2}z} dz \\
	&= \frac{1}{2\pi \lambda} \int_0^\infty z^\frac{1}{2} e^{-\frac{x^2}{2\lambda^2}z}
		z^{\frac{1}{2}-1} e^{-\frac{1}{2}z} dz \\
	&= \frac{1}{2\pi \lambda} \int_0^\infty e^{-\frac{1}{2} \bigl(\frac{x^2}{\lambda^2}+1\bigr)z} dz \\
	&=\Bigl[ \text{заменим }\beta = \frac{1}{2} \bigl(\frac{x^2}{\lambda^2}+1\bigr) \Bigr]\\
	&= \frac{1}{2\pi \lambda} \int_0^\infty e^{-\beta z} dz = \frac{1}{2\pi \lambda} \frac{1}{\beta}
\end{align*}
Итак искомая плотность имеет вид:
\[
p_X(x)
= \frac{1}{2\pi \lambda} \frac{2 \lambda^2}{x^2+\lambda^2}
= \frac{1}{\pi} \frac{\lambda}{x^2+\lambda^2}
\]
Функция распределения данной случайной величины задаётся
\[
P_X(x)
= \frac{1}{\pi}\int_{-\infty}^x\frac{\lambda}{t^2+\lambda^2} dt
= \frac{1}{\pi}\int_{-\infty}^\frac{x}{\lambda}\frac{1}{z^2+1} dt
= \frac{1}{\pi} \mathop{\text{arctg}}\bigl(\sfrac{x}{\lambda}\bigr)+\frac{1}{2}
\]
Таким образом, случайная величина $X$ распределена согласно распределению Коши
с параметром масштаба $\lambda$.

% section problem_2_18 (end)

\section{У2.13} % (fold)
\label{sec:problem_2_13}

Пусть $F_{X,Y}(x,y)$ совместная функция распределения пары случайных величин $(X,Y)$
с частными функциями распределения $F_X(x)$ и $F_Y(y)$ соответственно. Тогда
справедливо следующее
\[
F_{X,Y}(x,y)
= \Pr\bigl(X\leq x, Y\leq y\bigr)
= \Pr\bigl((X,Y)\in (-\infty,x]\times (-\infty,y]\bigr)
\]
Следует заметить, что $\Pr\bigl((X,Y)\in (-\infty,x]\times \Real\bigr) = \Pr(X\leq x) = F_X(x)$,
и аналогично для $Y$. При этом выполняется
\[(-\infty,x]\times (-\infty,y]\subseteq (-\infty,x]\times \Real,\Real\times (-\infty,y]\]
Таким образом, поскольку $\Pr$ есть вероятностная мера на $\Real\times \Real$, то
\[F_{X,Y}(x,y)\leq F_X(x), F_Y(y)\]
откуда следует верхняя граница Фреше-Хёфдинга.
\[F_{X,Y}(x,y)\leq \min\bigl\{F_X(x), F_Y(y)\bigr\}\]

Нижняя граница выводится их похожих соображений. Действительно несложно заметить,
что
\[
\Real\times \Real \setminus (-\infty,x]\times (-\infty,y]
\subseteq \bigl((x,+\infty)\times\Real\bigr) \cup \bigl(\Real\times(y,+\infty)\bigr)
\]
откуда в силу субаддитивности мер вытекает, что
\begin{align*}
	\Pr\bigl( \Real\times \Real \setminus (-\infty,x]\times (-\infty,y] \bigr)
	&\leq \Pr\Bigl( \bigl((x,+\infty)\times\Real\bigr) \cup \bigl(\Real\times(y,+\infty)\bigr) \Bigr)\\
	&\leq \Pr\bigl( (x,+\infty)\times \Real \bigr) + \Pr\bigl( \Real\times (y,+\infty) \bigr)
\end{align*}
При этом, поскольку $F_{X,Y}$ -- совместная функция распределения, то
\begin{align*}
	1-F_{X,Y}(x,y) &= \Pr\bigl( \Real\times \Real \setminus (-\infty,x]\times (-\infty,y] \bigr)\\
	1-F_X(x) &= \Pr\bigl( (x,+\infty)\times \Real \bigr)\\
	1-F_Y(y) &= \Pr\bigl( \Real\times (y,+\infty) \bigr)
\end{align*}
Таким образом справедливо неравенство
\begin{multline*}
	1-F_{X,Y}(x,y) \leq 1-F_X(x) + 1-F_Y(y) \\ \Leftrightarrow F_X(x) + F_Y(y) - 1 \leq F_{X,Y}(x,y)
\end{multline*}
Из неотрицательности распределения получается нижняя граница Фреше-Хёфдинга
\[ \max\bigl\{0,F_X(x) + F_Y(y) - 1\bigr\} \leq F_{X,Y}(x,y) \]

% section problem_2_13 (end)

\section{У2.14} % (fold)
\label{sec:problem_2_14}

Рассмотрим функцию двух переменных
\[C_\theta(u,v) = uv \bigl(1-\theta(1-u)(1-v)\bigr)\]
и пусть $H(x,y) = C_\theta\bigl(F_X(x), F_Y(y)\bigr)$.

Рассмотрим произвольную последовательность $(s_n,t_n) \downarrow (x,y)$, то есть
$(x_n,y_n) \overset{\Real^2}{\to}(x,y)$ и при этом $x_n\geq x$ и $y_n\geq y$. Тогда
непрерывность справа и неубывание частных функций распределения $F_X$ и $F_Y$ влечёт
то, что при $u_n=F_X(x_n)$ и $v_n=F_Y(y_n)$ справедливо, что $(u_n,v_n)\downarrow (u,v)$,
где $u=F_X(x)$ и $v=F_Y(y)$. Поэтому сходимость $H(x_n,y_n)$ к $H(x,y)$ равносильна
сходимости $C_\theta(u_n,v_n)$ к $C_\theta(u,v)$. Легко увидеть, что $C_\theta$ есть
композиция непрерывных функций $(u,v)\to u\cdot v$ и $u\to 1-u$. Поэтому $C_\theta$
непрерывна и выполнено $\lim_{n\to \infty} C_\theta(u_n,v_n) = C_\theta(u,v)$.
Тк последовательность $(x_n,y_n)$ произвольна, то справедливо, что
\[\lim_{s\downarrow x,t\downarrow y} H(s,t) = H(x,y)\]

Пусть $y$--фиксировано, тогда при $x\to \infty$ справедливо $F_X(x)\to 1$, откуда в
силу непрерывности $C_\theta$ вытекает, что
\[
\lim_{x\to\infty} H(x,y)
= \lim_{x\to\infty} C_\theta(F_X(x),F_Y(y))
= C_\theta(1,F_Y(y))
= F_Y(y)
\]
аналогично при фиксированном $x$ и $y\to \infty$
\[\lim_{y\to\infty} H(x,y) = F_X(x)\]
Таким образом $\lim_{x\to\infty} \lim_{y\to\infty} H(x,y) = 1$.

Далее, пусть $x$ -- фиксировано, а $y\to -\infty$. Тогда $F_Y(y)\to 0$ по свойствам
одномерной функции распределения. Однако поскольку $C_\theta:[0,1]\times [0,1]\to \Real$
непрерывна, то
\[
\lim_{y\to-\infty} H(x,y)
= \lim_{y\to-\infty} C_\theta(F_X(x),F_Y(y))
= C_\theta(F_X(x),0)
= 0
\]
Опять-таки, для фиксированного $y$ при $x\to-\infty$ ситуация идентична.

Теперь следует показать свойство ``обобщённой'' монотонности функции $H$, которое
позволит ей быть мерой на измеримых прямоугольниках $\Real^2$:
\begin{multline*}
\mu\bigl((x_1,x_2]\times (y_1,y_2]\bigr)
= \mu\bigl((-\infty,x_2]\times (-\infty,y_2]\bigr)
- \mu\bigl((-\infty,x_1]\times (-\infty,y_2]\bigr) \\
- \Bigl( \mu\bigl((-\infty,x_2]\times (-\infty,y_1]\bigr)
	- \mu\bigl((-\infty,x_1]\times (-\infty,y_1]\bigr) \Bigr)
\end{multline*}
где $\mu\bigl((-\infty,x]\times (-\infty,y]\bigr) = H(x,y)$.

Итак, рассмотрим $x_1\geq x_2$ и $x_1\geq x_2$. Пусть $u_i=F_X(x_i)$ и $v_i=F_Y(y_i)$,
$i=1,2$. Тогда
\begin{multline*}
H(x_2,y_2) - H(x_1,y_2) - H(x_2,y_1) + H(x_1,y_1)
=  \\ C_\theta(u_2,v_2) - C_\theta(u_1,v_2) - C_\theta(u_2,v_1) + C_\theta(u_1,v_1)
\end{multline*}
Каждая $C_\theta(u,v)$ состоит из двух слагаемых :
\[C_\theta(u,v) = uv - \theta u(1-u) v(1-v)\]
Таким образом рассматриваемое выражение равно
\begin{multline*}
	C_\theta(u_2,v_2) - C_\theta(u_1,v_2) - C_\theta(u_2,v_1) + C_\theta(u_1,v_1) = \\
	( u_2v_2 - u_1v_2 - u_2v_1 + u_1v_1 ) - \\
		\theta ( u_2(1-u_2) v_2(1-v_2) - u_1(1-u_1) v_2(1-v_2) - \\ u_2(1-u_2) v_1(1-v_1) + u_1(1-u_1) v_1(1-v_1) )
\end{multline*}
после упрощения получаем следующее выражение
\[\ldots = (u_2-u_1)(v_2-v_1) \bigl( 1 - \theta \Bigl( 1 - (u_2 + u_1) \bigr)\bigl( 1 - (v_2 + v_1) \bigr) \Bigr)\]
Итак
\[C_\theta(u_2,v_2) - C_\theta(u_1,v_2) - C_\theta(u_2,v_1) + C_\theta(u_1,v_1) \geq 0\]
тогда и только тогда, когда
\[
\theta \Bigl( 1 - (u_2 + u_1) \bigr)\bigl( 1 - (v_2 + v_1) \bigr)\leq 1
\]
Рассмотрим функцию $[-1,1]^2\to\Real$ вида $\theta s t$, поскольку для всяких
$u_1,u_2\in[0,1]$ разность
\[1 - (u_2 + u_1) = (\frac{1}{2}-u_1)+(\frac{1}{2}-u_2)\in [-1,1]\]
Для выполнения условия $\theta st\leq 1$ всюду на $[-1,1]\times [-1,1]$ достаточно,
чтобы $\lvert \theta\rvert\leq1$, поскольку $st$ изменяется в пределах $[-1,1]$.

Таким образом при $\theta\in [-1,1]$ функция $H(x,y)=C_\theta(F_X(x),F_Y(y))$
является совместной функцией распределения величин $X,Y$.

В общем случае, достаточно чтобы функция $C:[0,1]\times[0,1]\to[0,1]$ была
совместной функцией распределения на единичном квадрате пары индивидуально
равномерно распределённых случайных величин $(U,V)$. Действительно благодаря
тому, что частные одномерные распределения являются неотрицательными, неубывающими
и непрерывными справа функциями, композиция $C\circ(F_X,F_Y)$ будет двумерной
функцией распределения.

Действительно, если выполнено
\[\lim_{s\downarrow u}\lim_{t\downarrow v} C(s,t) = C(u,v)\]
то автоматически выполнено
\[
\lim_{s\downarrow x}\lim_{t\downarrow y} H(s,t)
= \lim_{s\downarrow x}\lim_{t\downarrow y} C\bigl(F_X(s),F_Y(t)\bigr)
= C\bigl(F_X(x),F_Y(y)\bigr)
= H(x,y)
\]
Если $\lim_{u\to0} C(u,v) = \lim_{v\to0} C(u,v) = 0$ то и выполнено 
\[\lim_{x\to0} H(x,y) = \lim_{y\to0} H(x,y) = 0\]
Также, если $\lim_{u\to1} C(u,v) = v$ и $\lim_{v\to1} C(u,v) = u$, то справедливо,
что
\[\lim_{x\to\infty}H(x,y) = F_Y(y)\text{ и }\lim_{y\to\infty}H(x,y) = F_X(x)\]
откуда $\lim_{x\to\infty}\lim_{y\to\infty}H(x,y) = 1$.
И в завершение, при выполнении для любых $u_1\leq u_2$ и $v_1\leq v_2$
\[C(u_2,v_2) - C(u_1,v_2) - C(u_2,v_1) + C(u_1,v_1)\geq 0\]
из свойств $F_X$ и $F_Y$ вытекает, что
\[H(x_2,y_2) - H(x_1,y_2) - H(x_2,y_1) + H(x_1,y_1)\geq 0\]

Итак действительно того факта, что $C(u,v)$ распределение на единичном квадрате
случайных величин, каждая из которых имеет частное равномерное распределение,
вытекает, что $H=C\circ(F_X,F_Y)$ -- совместное распределение.

% section problem_2_14 (end)

\section{У4.2} % (fold)
\label{sec:problem_4_2}

Пусть $X=(X_k)_{k=1}^n$ независимые случайные величины распределённые согласно
нормальному закону со средними $\mu = (\mu_k)_{k=1}^n$ и дисперсиями $(\sigma^2_k)_{k=1}^n$.
Тогда совместная функция плотности равна
\[p_X(x) = \prod_{k=1}^n \frac{1}{\sqrt{2\pi \sigma_k^2}} e^{-\frac{(x_k-\mu_k)^2}{2\sigma^2_k}}\]
На самом деле вектор $X$ можно рассматривать как случайный вектор многомерного
нормального распределения $\Ncal_n\bigl(\mu,\Sigma\bigr)$, где
$\Sigma = \text{diag}\bigl((\sigma^2_k)_{k=1}^n\bigr)$.
Плотность $\Ncal_n(\mu,\Sigma)$:
\[
p_X(x)
= \frac{1}{(2\pi)^\frac{n}{2} \lvert \Sigma \rvert^\frac{1}{2}}
	e^{-\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu)}
\]

Итак рассмотрим квадрат длины вектора $X\sim \Ncal_n(\mu,D)$ :
\[L = \sum_{k=1}^n X_k^2\]
Характеристическая функция $L$ задаётся выражением
\[
\phi_L(s)
= \ex\bigl(e^{itL}\bigr)
= \ex\Bigl(\prod_{k=1}^n e^{i t X_k^2}\Bigr)
\]
Поскольку ковариационная матрица нормального вектора $X$ диагональна, то компоненты
$X_k$ вектора являются независимыми случайными величинами с распределением
$\Ncal(\mu_k,\sigma_k^2)$. Поэтому характеристическая функция $L$ есть произведение
характеристических функций квадратов независимых нормально распределённых
случайных величин
\[\phi_{X_k^2}(t) = \ex\bigl(e^{it X_k^2}\bigr)\]
Итак
\[
\int_{-\infty}^\infty e^{itx^2} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x-\mu)^2} dx
	= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{it(z\sigma+\mu)^2} e^{-\frac{1}{2}z^2} dz
\]
В показателе степени можно выделить полный квадрат по $z$ :
\begin{align*}
it(z\sigma+\mu)^2 - \frac{1}{2}x^2
	&= it z^2\sigma^2 + 2itz\sigma\mu + it\mu^2 -\frac{1}{2}z^2 \\
	&=  -\frac{1}{2}\bigl(z^2(1 - 2 it \sigma^2) - 4itz\sigma\mu\bigr) + it\mu^2\\
	&=  -\frac{1}{2}(1 - 2 it \sigma^2)\bigl(z^2 - 2z\frac{2it\sigma\mu}{1 - 2 it \sigma^2}\bigr) + it\mu^2\\
	&=  -\frac{1}{2}(1 - 2 it \sigma^2)\Bigl(
			z^2 - 2z\frac{2it\sigma\mu}{1 - 2it\sigma^2} -\frac{4t^2\sigma^2\mu^2}{(1 - 2it\sigma^2)^2}
		\Bigr) \\
	&\quad\quad- \frac{2t^2\sigma^2\mu^2}{1 - 2it\sigma^2} + it\mu^2\\
	&=  -\frac{1}{2}(1 - 2it\sigma^2)\bigl(z - \frac{2it\sigma\mu}{1 - 2it\sigma^2}\bigr)^2
		+ \frac{it\mu^2}{1 - 2it\sigma^2}
\end{align*}
Из этого вытекает, что 
\[
\phi_{X_k^2}(t)
= e^\frac{it\mu^2}{1 - 2it\sigma^2} \frac{1}{\sqrt{2\pi}}
	\int_{-\infty}^\infty e^{-\frac{1}{2}(1 - 2it\sigma^2)\bigl(z - \frac{2it\sigma\mu}{1 - 2it\sigma^2}\bigr)^2} dz
= \frac{e^\frac{it\mu^2}{1 - 2it\sigma^2}}{\sqrt{1 - 2it\sigma^2}}
\]
Таким образом характеристическая функция квадрата длины нормального вектора равнв
\[\phi_L(s) = \prod_{k=1}^n \frac{e^\frac{it\mu_k^2}{1 - 2it\sigma_k^2}}{\sqrt{1 - 2it\sigma_k^2}}\]

Если рассматривать длину не исходного вектора, а вектора с элементами нормированными
до характерного масштаба каждой компоненты (по сути безразмерных), то вид
характеристической функции упрощается. Действительно, если
$L = \sum_{k=1}^n \Bigl(\frac{X_k}{\sigma_k}\Bigr)^2$, то
\[\phi_L(s) = \prod_{k=1}^n \frac{e^\frac{it\frac{\mu_k^2}{\sigma^2_k}}{1 - 2it}}{\sqrt{1 - 2it}}\]
поскольку $\frac{X_k}{\sigma_k} \overset{\Dcal}{=} Z_k$ где
\[Z_k \sim\Ncal\bigl(\frac{\mu_k}{\sigma_k},1\bigr)\]
а в этом случае
\[
\phi_{Z_k^2}(t) = \frac{e^\frac{it\frac{\mu^2}{\sigma^2}}{1 - 2it}}{\sqrt{1 - 2it}}
\]

Итак в изменённых условиях задачи 
\[\phi_L(s) = e^\frac{it \lambda}{1 - 2it} \bigl(1 - 2it\bigr)^{-\frac{n}{2}}\]
где $\lambda = \sum_{k=1}^n \bigl(\frac{\mu_k}{\sigma_k}\bigr)^2$. Согласно справочника
характеристических функций полученное выражение соответствует характеристической
функции $\chi^2_n$ распределения с параметром нецентральности $\lambda$. Функция
плотности такого распределения равна
\[
f(x;n,\lambda)
= \sum_{j\geq 0} \frac{e^\frac{\lambda}{2}}{ \bigl(\frac{\lambda}{2}\bigr)^j}{j!} f_{n+2j}(x)
\]
где $f_m(x)$ -- функция плотности $\chi^2_m$ c $m$ степенями свободы:
\[f_m(x) = \frac{2^{-\frac{m}{2}}}{\Gamma(\frac{m}{2})} e^{-\frac{1}{2}x} x^{\frac{m}{2}-1}\]
Стоит заметить, что если $\lambda = 0$, что возможно только в случае если средние
значения вектора $X$ равны нулю, то распределение квадрата длины вектора
нормированных компонент вырождается в обычное $\chi^2$ распределение с $n$ степенями
свободы.

Итак что касается функции плотности распределения длины $l$ (квадратного корня из $L$)
вектора $(\bigl(\frac{x_k}{\sigma_k}\bigr)$, то она через плотность $L$ следующим
образом: поскольку $l = \sqrt{L} = g(L)$, то
\[f_l(y) = f_L\bigl(g^{-1}(y)\bigr) \frac{1}{\bigl\lvert g'(L)\bigr\rvert_{L=g^{-1}(y)}}\]
Таким образом длина вектора безразмерных компонент распределена согласно
\[f_l(y) = \sum_{j\geq 0} \frac{e^\frac{\lambda}{2}}{ \bigl(\frac{\lambda}{2}\bigr)^j}{j!} 2 f_{n+2j}(y^2) y\]
В случае $\lambda = 0$ распределение вырождается в 
\[
f_l(y)
= 2 f_{n+2j}(y^2) y
= \frac{2^{1-\frac{n}{2}}}{\Gamma\bigl(\frac{n}{2}\bigr)} y^{n-1} e^{-\frac{y^2}{2}}
\]
так называемое $\chi_n$ распределение (без квадрата).

% section problem_4_2 (end)

\section{У4.11} % (fold)
\label{sec:problem_4_11}

Рассмотрим $X\in \Real{n\times 1}$ с $n$-мерным нормальным распределением со средним
$\mu\in \Real^{n\times 1}$ и положительно определённой ковариационной матрицей
$\Sigma \in \Real{n\times n}$
\[
p(X;\mu,\Sigma)
= (2\pi)^{-\frac{n}{2}} \lvert \Sigma \rvert^{-\frac{1}{2}}
	\text{exp}\bigl\{-\frac{1}{2}(X-\mu)'\Sigma^{-1}(X-\mu)\bigr\}
\]
Пусть вектор $X$ разбит на два подвектора $X_1\in \Real^{p\times 1}$ и $X_2\in \Real^{(n-p)\times 1}$:
\[X = \Bigl(\begin{smallmatrix} X_1 \\ X_2 \end{smallmatrix}\Bigr)\]
а в матрице $\Sigma$ выделена следующая блочная структура:
\[
\Sigma
= \biggl(\begin{matrix}\Sigma_{11} & \Sigma_{12}\\\Sigma_{21} & \Sigma_{22}\end{matrix}\biggr)
\sim \biggl(\begin{matrix}p\times p & p\times (n-p)\\(n-p)\times p & (n-p)\times (n-p)\end{matrix}\biggr)
\]
причём $\Sigma_{12} = \Sigma_{21}'$. Поскольку $\Sigma$ положительно определена,
то она обязательно невырождена, откуда следует что существует единственная матрица
аналогичной блочной структуры $B$ такая что
\[
\biggl(\begin{matrix}\Sigma_{11} & \Sigma_{12}\\\Sigma_{21} & \Sigma_{22}\end{matrix}\biggr)
\biggl(\begin{matrix}B_{11} & B_{12}\\B_{21} & B_{22}\end{matrix}\biggr)
=
\biggl(\begin{matrix}I_p & 0\\0 & I_{n-p}\end{matrix}\biggr)
\]
Поскольку $\Sigma B = I_n$, то $B'\Sigma' = I_n'$ и $B'\Sigma = I_n$. Из этого следует,
что обратная к симметричной матрице симметрична:
\[B' = B' I_n = B' \Sigma B = I_n B = B\]

Поскольку ковариационная матрица является положительно определенной, то блоки
$\Sigma_{11}$ и $\Sigma_{22}$ являются положительно определёнными. Это означает,
что каждый диагональный блок $\Sigma$ невырожден.

\subsection*{условное распределение $X_1$ по $X_2$} % (fold)
\label{sub:conditional_x_1_x_2}

Для блоков обратной матрицы $B$ можно получить следующие уравнения:
\[
\biggl(\begin{matrix}
	\Sigma_{11}B_{11} + \Sigma_{12}B_{21} & \Sigma_{11}B_{12} + \Sigma_{12}B_{22} \\
	\Sigma_{21}B_{11} + \Sigma_{22}B_{21} & \Sigma_{21}B_{12} + \Sigma_{22}B_{22}
\end{matrix}\biggr)
= \biggl(\begin{matrix}I_p & 0\\0 & I_{n-p}\end{matrix}\biggr)
\]
Это равенство эквивалентно следующим уравнениям
\begin{align*}
	\Sigma_{11}B_{11} + \Sigma_{12}B_{21} &= I_p \\
	\Sigma_{21}B_{11} + \Sigma_{22}B_{21} &= 0 \\
	\Sigma_{11}B_{12} + \Sigma_{12}B_{22} &= 0 \\
	\Sigma_{21}B_{12} + \Sigma_{22}B_{22} &= I_{n-p}
\end{align*}
В силу обратимости диагональных блоков $\Sigma$ справедливо
\begin{align*}
	B_{21} &= - \Sigma_{22}^{-1} \Sigma_{21}B_{11} \\
	B_{11}^{-1} &= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
\end{align*}

Рассмотрим выражение в экспоненте в плотности многомерного нормального распределения
$Y'\Sigma^{-1}Y$, где для удобства выполнена замена $Y = X-\mu$. Данное скалярное
произведение с учётом выделенной блочной структуры принимает вид
\[ Y_1'B_{11}Y_1 + Y_2'B_{21}Y_1 + Y_1'B_{12}Y_2 + Y_2'B_{22}Y_2 \]
Поскольку $B_{11}$ обратима, то в этом выражении можно выделить ``квадратичную
форму'' (которая в общем случае находится методом ``пристального вглядывания'')
\begin{multline*}
  Y_1'B_{11}\bigl( Y_1 + B_{11}^{-1}B_{12}Y_2 \bigr)\\
	+ Y_2'B_{21}B_{11}^{-1} B_{11} \bigl( Y_1 + B_{11}^{-1} B_{12} Y_2\bigr)\\
	+ Y_2'\bigl( B_{22} - B_{21}B_{11}^{-1} B_{12} \bigr)Y_2
\end{multline*}
Данное выражение упрощается в 
\[
  \bigl( Y_1 + B_{11}^{-1}B_{21}'Y_2 \bigr)' B_{11} \bigl( Y_1 + B_{11}^{-1}B_{12}Y_2 \bigr)\\
	+ Y_2'\bigl( B_{22} - B_{21}B_{11}^{-1} B_{12} \bigr)Y_2
\]
Заметим, что во-первых, выражение $B_{22} - B_{21}B_{11}^{-1} B_{12}$ можно упростить:
\begin{align*}
	B_{22} - B_{21}B_{11}^{-1} B_{12}
	&= \Sigma_{22}^{-1}\bigl( \Sigma_{22} B_{22} - \Sigma_{22} B_{21}B_{11}^{-1} B_{12} \bigr)\\
	&= \Bigl[ B_{21} = - \Sigma_{22}^{-1} \Sigma_{21}B_{11} \Bigr]\\
	&= \Sigma_{22}^{-1}\bigl( \Sigma_{22} B_{22} + \Sigma_{22} \Sigma_{22}^{-1} \Sigma_{21} B_{11} B_{11}^{-1} B_{12} \bigr)\\
	&= \Sigma_{22}^{-1}\bigl( \Sigma_{22} B_{22} + \Sigma_{21} B_{12} \bigr)\\
	&= \Bigl[ \Sigma_{21}B_{12} + \Sigma_{22}B_{22} = I_{n-p} \Bigr]\\
	&= \Sigma_{22}^{-1}
\end{align*}
Во-вторых, благодаря симметрии $B$ справедливо, что
\begin{align*}
	B_{11}^{-1}B_{21}' &= - B_{11}^{-1} B_{11} \Sigma_{12}\Sigma_{22}^{-1} = - \Sigma_{12}\Sigma_{22}^{-1}\\
	B_{11}^{-1}B_{12}  &= B_{11}^{-1}B_{21}' = - \Sigma_{12}\Sigma_{22}^{-1}
\end{align*}
откуда получается
\[
Y'\Sigma^{-1}Y
= \bigl( Y_1 - \Sigma_{12}\Sigma_{22}^{-1}Y_2 \bigr)' B_{11} \bigl( Y_1 - \Sigma_{12}\Sigma_{22}^{-1} Y_2 \bigr)
	+ Y_2'\Sigma_{22}^{-1}Y_2
\]
где $B_{11} = \bigl(\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}\bigr)^{-1}$.

В-третьих, поскольку $\Sigma_{22}$ невырождена, то определитель $\lvert \Sigma\rvert$
равен произведению
\[
\lvert \Sigma\rvert
= \lvert \Sigma_{22}\rvert \cdot \lvert \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}\rvert
\]

Таким образом,
\begin{multline*}
p(X;\mu,\Sigma)
= (2\pi)^{-\frac{p}{2}} \lvert \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \rvert^{-\frac{1}{2}}
	\text{exp}\Bigl\{-\frac{1}{2}\bigl( (X_1-\mu_1) - \Sigma_{12}\Sigma_{22}^{-1}(X_2-\mu_2) \bigr)'\\
		\bigl(\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}\bigr)^{-1}
		\bigl( (X_1-\mu_1) - \Sigma_{12}\Sigma_{22}^{-1} (X_2-\mu_2) \bigr)\Bigr\}\\
	\times (2\pi)^{-\frac{n-p}{2}} \lvert \Sigma_{22} \rvert^{-\frac{1}{2}}
	\text{exp}\bigl\{-\frac{1}{2}(X_2-\mu_2)'\Sigma_{22}^{-1}(X_2-\mu_2)\bigr\}
\end{multline*}
Откуда по формуле Байеса получается, что 
\[
X_1\rvert X_2
\sim \Ncal_p\bigl(
	\mu_1+\Sigma_{12}\Sigma_{22}^{-1} (X_2-\mu_2),
	\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
\bigr)
\]

Теперь, в двумерном случае согласно данной формуле, получается, что
\begin{multline*}
x_1\rvert x_2
\sim \Ncal_p\bigl(
	\mu_1+\rho\sigma_1\sigma_2\frac{1}{\sigma^2_2} (x_2-\mu_2),
	\sigma^2_1 - \rho\sigma_1\sigma_2 \frac{1}{\sigma^2_2} \rho\sigma_1\sigma_2
\bigr)\\
= \Ncal_p\bigl(
	\mu_1+\rho\frac{\sigma_1}{\sigma_2} (x_2-\mu_2),
	\sigma^2_1(1 - \rho^2)
\bigr)
\end{multline*}

% subsection* conditional_x_1_x_2 (end)

% section problem_4_11 (end)

\section{У4.12} % (fold)
\label{sec:problem_4_12}

Если $X\sim \Ncal_n(\mu,\Sigma)$, то условное распределение $X_1$ при $X_2$ равно
\[
X_1\rvert X_2
\sim \Ncal_p\bigl(
	\mu_1+\Sigma_{12}\Sigma_{22}^{-1} (X_2-\mu_2),
	\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
\bigr)
\]
Отсюда, частности следует, что 
\[\ex\bigl( X_1 \rvert X_2 \bigr) = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1} (X_2-\mu_2)\]
Безусловная дисперсия условного математического ожидания равна
\begin{multline*}
\var\Bigl(\ex\bigl( X_1 \rvert X_2 \bigr)\Bigr)
= \var\Bigl(\Sigma_{12}\Sigma_{22}^{-1} (X_2-\mu_2)\Bigr)\\
= \Sigma_{12}\Sigma_{22}^{-1} \var\Bigl((X_2-\mu_2)\Bigr) \Sigma_{22}^{-1} \Sigma_{21}
= \Sigma_{12}\Sigma_{22}^{-1} \Sigma_{22} \Sigma_{22}^{-1} \Sigma_{21}
= \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
\end{multline*}

С другой стороны, условная дисперсия $X_1 \rvert X_2$ равна
\[\var\bigl( X_1 \rvert X_2 \bigr) = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}\]
которое совпадает с безусловным математическим ожиданием.

Сложив рассчитанные выражения получаем:
\begin{multline*}
\var\Bigl(\ex\bigl( X_1 \rvert X_2 \bigr)\Bigr) + \var\bigl( X_1 \rvert X_2 \bigr)\\
= \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} + \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
= \Sigma_{11}
= \var(X_1)
\end{multline*}

В общем случае условная дисперсия $X_1$ по $X_2$ равна
\begin{align*}
	\var(X_1 \lvert X2)
	&= \ex\bigl( (X_1 - \ex(X_1 \lvert X_2))^2 \big\lvert X_2\bigr) \\
	&= \ex\bigl( X_1^2 - 2 X_1\ex(X_1 \lvert X_2) + \ex(X_1 \lvert X_2)^2 \big\lvert X_2\bigr) \\
	&= \ex( X_1^2 \lvert X_2) - 2 \ex\bigl( X_1 \ex(X_1 \lvert X_2) \big\lvert X_2\bigr) + \ex\bigl( \ex(X_1 \lvert X_2)^2 \big\lvert X_2\bigr) \\
	&= \ex( X_1^2 \lvert X_2) - 2 \ex(X_1 \lvert X_2)^2 + \ex(X_1 \lvert X_2)^2 \\
	&= \ex( X_1^2 \lvert X_2) - \ex(X_1 \lvert X_2)^2
\end{align*}
Следовательно
\[
\ex \var(X_1 \lvert X2)
= \ex\bigl( \ex( X_1^2 \lvert X_2) - \ex(X_1 \lvert X_2)^2 \bigr)
= \ex(X_1^2) - \ex \ex(X_1 \lvert X_2)^2
\]
Однако с другой стороны в силу $\var{Y} = \ex Y^2 - (\ex Y )^2$:
\[
\var \ex(X_1 \lvert X_2)
= \ex \bigl(\ex(X_1 \lvert X_2)^2\bigr) - \Bigl(\ex \bigl(\ex(X_1 \lvert X_2)\bigr)\Bigr)^2
= \ex \bigl(\ex(X_1 \lvert X_2)^2\bigr) - \ex(X_1)^2
\]
Выразив матожидание квадрата условного матожидания из последнего выражения, получаем
\[
\ex \var(X_1 \lvert X2)
= \ex(X_1^2) - \ex(X_1)^2 - \var \ex(X_1 \lvert X_2)
= \var(X_1) - \var \ex(X_1 \lvert X_2)
\]
Отсюда следует, что
\[\var(X_1) = \ex \var(X_1 \lvert X2) + \var \ex(X_1 \lvert X_2)\]
при условии, что у случайной величины $X_1$ существует хотя бы второй момент.

% section problem_4_12 (end)

\section{У4.16} % (fold)
\label{sec:problem_4_16}

Пусть $X^n = (X_i)_{i=1}^n$ выборка независимых и одинаково распределённых случайных
величин с законом $F(x)$. Известно, что если $U_i = F(X_i)$, то
$(U_i)_{i=1}^n\sim\mathcal(U)[0,1]$, и при этом в силу неубывания $F$ выполняется
$X_{(i)} = F^{-1}(U_{(i)})$ для всех $i=1,\ldots,n$, где $X_{(i)}$ есть $i$-я
порядковая статистика.

Согласно теореме Пайка произвольная порядковая статистика равномерного распределения
совпадает по распределению с отношением накопленных сумм независимых экспоненциально
распределённых случайных величин:
\[U_{(i)}\sim \frac{\sum_{k=1}^i \xi_k}{\sum_{k=1}^{n+1} \xi_k}\]
где $(\xi_k)_{k=1}^{n+1} \sim \text{Exp}(1)$ независимые случайные величины. При
этом для любого $m\geq 1$
\[G_m = \sum_{k=1}^m \xi_k \sim \Gamma(1, m)\]
где $\Gamma(\beta,n)$ -- Гамма распределение с плотностью 
\[f(x) = \frac{\beta^n}{\Gamma(n)} x^{n-1} e^{-\beta x}\]
Более того известно, что 
\[\frac{g_m}{g_m+g_{n-m+1}} \sim B(m, n-m+1)\]
где $B(\alpha,\beta)$ -- бета распределение с параметрами $\alpha$ и $\beta$, чья
функция плотности задана
\[f(x) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)} x^{\alpha-1}(1-x)^{\beta-1}\]
Отмечу, что 
\begin{align*}
	\ex\bigl(B(\alpha,\beta)\bigr) &= \frac{\alpha}{\alpha+\beta}\\
	\var\bigl(B(\alpha,\beta)\bigr) &= \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
\end{align*}
Таким образом для любого $i$ справедливо, что
\[U_{(i)} \sim B(i,n+1-i)\]

Итак выберем некоторый $p\in (0,1)$ и рассмотрим вопрос сходимости
\[U_{([np])} - p \overset{\pr}{\to} 0\]
где $[np]$ -- ближайшее целое к $np$. Наблюдения выше позволяют сделать вывод о том,
что $U_{([np])}\sim B([np], n+1-[np])$ и
\begin{align*}
	\ex\bigl(U_{([np])}\bigr)
		&= \frac{[np]}{n+1}\\
	\var\bigl(U_{([np])}\bigr)
		&= \frac{[np](n+1-[np])}{(n+1)^2(n+2)}
		= \frac{\frac{[np]}{n+1}(1-\frac{[np]}{n+1})}{n+2}
\end{align*}
Таким образом ожидание квадрата разности $U_{([np])} - p$:
\begin{align*}
	\ex\bigl(U_{([np])} - p\bigr)^2
	&= \ex\bigl(U_{([np])} - \frac{[np]}{n+1}\bigr)^2 + \bigl(\frac{[np]}{n+1}-p\bigr)^2 \\
	&\quad\quad + \ex\bigl(\frac{[np]}{n+1}-p\bigr)\bigl(U_{([np])} - \frac{[np]}{n+1}\bigr)\\
	&= \var\bigl(U_{([np])}\bigr) + \bigl(\frac{[np]}{n+1}-p\bigr)^2 \\
	&= \frac{\frac{[np]}{n+1}(1-\frac{[np]}{n+1})}{n+2} + \bigl(\frac{[np]}{n+1}-p\bigr)^2
\end{align*}
Используя неравенство Маркова получаем, что для любого $\epsilon>0$ выполнено
\[
\pr\bigl( \lvert U_{([np])} - p\rvert > \epsilon \bigr)
\leq \frac{\mu_{n,p}(1-\mu_{n,p})}{\epsilon^2(n+2)} + \frac{(\mu_{n,p}-p)^2}{\epsilon^2}
\]
где $\mu_{n,p} = \frac{[np]}{n+1}$. Однако по определению ближайшего целого
\[\bigl\lvert[np]-np\bigr\rvert \leq \frac{1}{2}\]
\[
\lvert \mu_{n,p}-p \rvert
= \Bigl\lvert \frac{[np]-np}{n+1}-\frac{p}{n+1} \Bigr\rvert
\leq \frac{\lvert[np]-np\rvert}{n+1}+\frac{p}{n+1}
\leq \frac{1+2p}{2(n+1)}
\]
Отсюда следует, что $\mu_{n,p} - p = o(\frac{1}{n})$ при $n\to\infty$. Теперь
\[
\limsup_{n\to \infty} \pr\bigl( \lvert U_{([np])} - p\rvert > \epsilon \bigr)
\leq \limsup_{n\to \infty} \bigl(\frac{\mu_{n,p}(1-\mu_{n,p})}{\epsilon^2(n+2)}
	+ \frac{(\mu_{n,p}-p)^2}{\epsilon^2}\bigr)
= 0
\]
Следовательно,
\[U_{([np])} - p \overset{\pr}{\to} 0\]

Рассмотрим вопрос предельного распределения $\sqrt{n}(U_{([np])} - p)$.
\begin{align*}
	\sqrt{n} \bigl( U_{([np])} - p \bigr)
	&= \frac{\frac{1}{\sqrt{n}}}{\frac{1}{n}}\Bigl(
		\frac{\sum_{k=1}^{[np]} \xi_k}{\sum_{k=1}^{[np]} \xi_k + \sum_{k=[np]+1}^{n+1} \xi_k} - p \Bigr)\\
	&= \frac{1}{\sqrt{n}}\frac{(1-p)\sum_{k=1}^{[np]} \xi_k - p \sum_{k=[np]+1}^{n+1} \xi_k}{\frac{\sum_{k=1}^{n+1} \xi_k}{n}}\\
	&= \frac{\frac{1-p}{\sqrt{n}}\bigl(\sum_{k=1}^{[np]} \xi_k - [np]\bigr)
			- \frac{p}{\sqrt{n}}\bigl(\sum_{k=1}^{n+1-[np]} \xi_{k+[np]} - (n+1-[np])\bigr)}{\frac{\sum_{k=1}^{n+1} \xi_k}{n}}\\
	&\quad \quad + \frac{1}{\sqrt{n}}\frac{(1-p)[np] - p(n+1-[np])}{\frac{\sum_{k=1}^{n+1} \xi_k}{n}}
\end{align*}
Во-первых
\[
\frac{\lvert(1-p)[np] - p(n+1-[np])\rvert}{\sqrt{n}}
= \frac{\lvert ([np] - np) - p\rvert}{\sqrt{n}}
= o\bigl(\frac{1}{\sqrt{n}}\bigr)
\]
Во-вторых, по Закону Больших Чисел справедливо, что
\[\frac{\sum_{k=1}^{n+1} \xi_k}{n} \overset{\pr}{\to} 1\]
В-третьих, по Центральной Предельной Теореме при $m\to \infty$ выполнено
\[
\frac{1}{\sqrt{m}}\Bigl(\sum_{k=1}^m \xi_k - m\Bigr)
= \sqrt{m}\Bigl(\frac{1}{m}\sum_{k=1}^m \xi_k - 1\Bigr)
\overset{D}{\to} \Ncal(0,1)
\]
Поэтому по теореме Слуцкого
\begin{align*}
	\frac{1}{\sqrt{n}}\Bigl(\sum_{k=1}^{[np]} \xi_k - [np]\Bigr)
	&= \frac{\sqrt{[np]}}{\sqrt{n}}
		\sqrt{[np]}\Bigl(\frac{1}{[np]}\sum_{k=1}^{[np]} \xi_k - 1\Bigr)
		\overset{D}{\to} \Ncal(0,p)
\end{align*}
и
\[\frac{1}{\sqrt{n}}\Bigl(\sum_{k=[np]+1}^{n+1} \xi_k - (n+1-[np])\Bigr) \overset{D}{\to} \Ncal(0,1-p)\]
поскольку
\begin{align*}
	\frac{\sqrt{[np]}}{\sqrt{n}}
	&= \sqrt{p} \frac{\sqrt{[np]}}{\sqrt{np}}
		\to \sqrt{p}\\
	\frac{\sqrt{n+1-[np]}}{\sqrt{n}}
	&= \sqrt{1+\frac{1}{n}} \sqrt{1-\frac{[np]}{n+1}}
		\to \sqrt{1-p}
\end{align*}
Теперь, в силу того, что для любого $n$ суммы
\[\sum_{k=1}^{[np]} \xi_k\text{ и }\sum_{k=[np]+1}^{n+1} \xi_k\]
независимости, справедливо следующая асимптотическая оценка:
\begin{multline*}
(1-p)\frac{1}{\sqrt{n}}\Bigl(\sum_{k=1}^{[np]} \xi_k - [np]\Bigr)
- p\frac{1}{\sqrt{n}}\Bigl(\sum_{k=[np]+1}^{n+1} \xi_k - (n+1-[np])\Bigr) \overset{D}{\to} \\
\overset{D}{\to} \Ncal(0, p(1-p)^2 + (1-p)p^2)
= \Ncal(0, p(1-p))
\end{multline*}
Очередное применение теоремы Слуцкого приводит следующему результату:
\begin{align*}
	\sqrt{n} \bigl( U_{([np])} - p \bigr)
	&= \frac{1}{\frac{\sum_{k=1}^{n+1} \xi_k}{n}}
		\Bigl\{ \frac{1-p}{\sqrt{n}}\Bigl(\sum_{k=1}^{[np]} \xi_k - [np]\Bigr) \\
	& \quad\quad\quad- \frac{p}{\sqrt{n}}\Bigl(\sum_{k=1}^{n+1-[np]} \xi_{k+[np]} - (n+1-[np])\Bigr) \Bigr\}\\
	& \quad + \frac{1}{\frac{\sum_{k=1}^{n+1} \xi_k}{n}} \frac{(1-p)[np] - p(n+1-[np])}{\sqrt{n}} \\
	& \overset{D}{\to} 1 \cdot \Ncal(0, p(1-p)) + 0
\end{align*}
Таким образом, теперь очевидно, что
\[\sqrt{n} \frac{U_{([np])} - p}{\sqrt{p(1-p)}} \sim \Ncal(0, 1)\]

Перейдём к изучению вопроса об асимптотических свойствах $\sqrt{n}\bigl(X_{([np])} - F^{-1}(p)\bigr)$.
Предположим, что $F^{-1}$ дифференцируема в точке $p$ и вспомним, что
$X_{([np])} = F^{-1}(U_{([np])})$. Тогда из $U_{([np])} - p \overset{\pr}{\to} 0$,
вытекает
\[
\frac{F^{-1}(U_{([np])}) - F^{-1}(p)}{U_{([np])} - p}
\overset{\pr}{\to}
\frac{1}{f\bigl(F^{-1}(p)\bigr)}
\]
Следовательно, по теореме Слуцкого
\begin{multline*}
\sqrt{n} \bigl(X_{([np])} - F^{-1}(p)\bigr)
= \sqrt{n} \bigl(F^{-1}(U_{([np])}) - F^{-1}(p)\bigr)\\
= \frac{F^{-1}(U_{([np])}) - F^{-1}(p)}{U_{([np])} - p}
	\sqrt{n} \bigl(U_{([np])} - p\bigr) \overset{D}{\to}\\
\overset{D}{\to} \frac{1}{f\bigl(F^{-1}(p)\bigr)} \Ncal(0, p(1-p))
= \Ncal\Bigl(0, \frac{p(1-p)}{f\bigl(F^{-1}(p)\bigr)^2} \Bigr)
\end{multline*}

Пусть $0<p_1<p_2<1$. Для дисперсии разности эмпирических квантилей справедливо:
\[
\var\bigl(X_{([np_2])}-X_{([np_1])}\bigr)
= \var\bigl(X_{([np_2])}\bigr) +\var\bigl(X_{([np_1])}\bigr)
	+2 \text{cov}\bigl(X_{([np_2])},X_{([np_1])}\bigr)
\]
Согласно неравенства Коши-Буняковского-Шварца, ковариация случайных величин не превышает
\[
\Bigl\lvert \text{cov}\bigl(X_{([np_2])},X_{([np_1])}\bigr)\Bigr\rvert^2
\leq \var\bigl(X_{([np_2])}\bigr)\var\bigl(X_{([np_1])}\bigr)
\]
Отсюда следует, что
\[
\var\bigl(X_{([np_2])}-X_{([np_1])}\bigr)
\leq \Bigl( \sqrt{\var(X_{([np_2])})} + \sqrt{\var(X_{([np_1])})} \Bigr)^2
\]
Поскольку
\[
\sqrt{n}\bigl(X_{([np])}-F^{-1}(p)\bigr)
\overset{D}{\to}
\Ncal\bigl(0,\frac{p(1-p)}{f\bigl(F^{-1}(p)\bigr)^2}\bigr)
\]
то хотелось бы, чтобы выполнялось
\[\var(X_{([np])}) \approx \frac{1}{n}\frac{p(1-p)}{f\bigl(F^{-1}(p)\bigr)^2}\]
поскольку в таком случае
\[
\var\bigl(X_{([np_2])}-X_{([np_1])}\bigr)
\approx \frac{1}{n}\biggl( \sqrt{ \frac{p_2(1-p_2)}{f\bigl(F^{-1}(p_2)\bigr)^2}}
	+ \sqrt{ \frac{p_2(1-p_2)}{f\bigl(F^{-1}(p_2)\bigr)^2}} \biggr)^2
\]
Однако в общем случае сходимость по распределению не означает сходимость ожиданий.
Если $Y_n\sim \mathcal{U}[0,n]$, то выполняется следующее:
\begin{itemize}
	\item $\pr( n Y_n \leq x) = \frac{x}{n^2}\to 0$;
	\item $\ex(n Y_n) = \frac{1}{2} \neq 0$.
\end{itemize}
Поэтому, следует избрать другой путь.

По теореме Лагранжа из анализа если $G:[a,b]\to\Real$ непрерывна на $[a,b]$ и
дифференцируема всюду внутри $(a,b)$, то существует такое $с\in (a,b)$, что
\[ G(b) - G(a) = G'(c) (b-a) \]
Применительно к настоящей задаче существует такая последовательность $\eta_n$, что
$\lvert \eta_n - \mu_{n,p}\rvert\leq \lvert U_{([np])} - p\rvert$ почти
наверное и
\[
F^{-1}(U_{([np])}) - F^{-1}(p)
= \frac{1}{f\bigl(F^{-1}(\eta_n)\bigr)} (U_{([np])}-p)
\]
При достаточно больших $n$
\[
\frac{1}{f\bigl(F^{-1}(\eta_n)\bigr)} \approx \frac{1}{f\bigl(F^{-1}(p)\bigr)}
\]
откуда
\[
F^{-1}(U_{([np])}) - F^{-1}(p)
\approx \frac{1}{f\bigl(F^{-1}(p)\bigr)} (U_{([np])}-p)
\]
следовательно
\[
\ex\bigl(F^{-1}(U_{([np])}) - F^{-1}(p)\bigr)
\approx \frac{1}{f\bigl(F^{-1}(p)\bigr)} \frac{p(1-p)}{n+2}
\]
Отсюда, при достаточно больших $n$ следует, что
\begin{multline*}
\var\bigl(X_{([np_2])}-X_{([np_1])}\bigr)
\approx \biggl(
\sqrt{\frac{1}{f\bigl(F^{-1}(p_1)\bigr)} \frac{p_1(1-p_1)}{n+2}} + \\
+ \sqrt{\frac{1}{f\bigl(F^{-1}(p_2)\bigr)} \frac{p_2(1-p_2)}{n+2}}
\biggr)^2 = O\Bigl(\frac{1}{{n}}\Bigr)
\end{multline*}
Итак, при достаточно больших $n$ дисперсия разности квантилей $p_2$ и $p_1$
убывет как $\frac{1}{n}$.

% section problem_4_16 (end)

\section{Упражнение из лекции 2 раздела 3} % (fold)
\label{sec:problem_l2_s3}

Пусть $d\geq 2$ -- некоторое целое число и $\bigl(p_j\bigr)_{j=0}^{d-1}\in[0,1]$
набор чисел таких, что $\sum_{j=0}^{d-1} p_j = 1$. Построим набор разбиений отрезка
$[0,1]$ со следующей структурой:
\begin{itemize}
	\item $I^0_0 = (0,1]$;
	\item $I^l_j\cap I^l_k = \emptyset$ для любы $j,k=0,\ldots,d^l-1$, $j\neq k$;
	\item $I^l_k = \biguplus_{j=0}^{d-1} I^{l+1}_{dk+j}$ для $k=0,\ldots,d^l-1$;
\end{itemize}
Определим меру $\mu^l$ при$l\geq 0$ на коллекции множеств
\[
\Ical^l = \{\{0\}\} \cup \Bigl\{I^l_k\Big\rvert k=0,\ldots, d^l-1\Bigr\}
\]
в предположении, что $I^l_k = \bigl(\frac{k}{d^l},\frac{k+1}{d^l}\bigr]$, 
следующим образом: \begin{itemize}
	\item $\mu^l(\{0\}) = 0$ и $\mu^0((0,1]) = 1$;
	\item при $m<l$ для каждого $k=0,\ldots, d^m-1$ определим $\mu^l(I^m_k)$
	равной $\mu^m(I^m_k)$;
	\item $\mu^l(I^l_k) = \mu^{l-1}(I^{l-1}_{\lfloor\frac{k}{d}\rfloor}) \cdot p_j$,
	где $j=0,\ldots,d-1$.
\end{itemize}

%% Рисуем структуру интервалов
\begin{figure}[htb]\begin{center}
	\tikzstyle{part} = [fill = none, draw, minimum height = 2em, minimum width = 1cm]
	\begin{tikzpicture}[level/.style={sibling distance = 20mm, level distance = 25mm }]
	\node [part] (root) {$I^{l-1}_k$} [child anchor = north]
		child { node [part] (s1) {$I^l_{d\cdot k}$} }
		child { node [part] (s2) {$I^l_{d\cdot k+1}$}
			child { node [part] (s21) {$I^{l+1}_{d\cdot(d\cdot k+1)}$}  }
			child { node [part] (s22) {$I^{l+1}_{d\cdot(d\cdot k+1)+1}$} }
			child { node[draw=none] (hidden) {} edge from parent[draw=none] }
			child { node [part] (s2d) {$I^{l+1}_{d\cdot(d\cdot k+1)+d-1}$} } }
		child { node[draw=none] (hidden) {} edge from parent[draw=none] }
		child { node [part] (sd) {$I^l_{d\cdot (k+1)-1}$} };
	\path (s2) -- (sd) node [midway, above] {$\cdots$};
	\path (s22) -- (s2d) node [midway, above] {$\cdots$};
	\end{tikzpicture}
	\caption{Каскад разбиения произвольного множества $I^{l-1}_k$, $k=0,\ldots,d^{l-1}-1$
	на $d$ произвольных непересекающихся частей.}
\label{fig:cascade_structure}
\end{center}\end{figure}

Рассмотрим
\[
R_{\mu,\epsilon}(q)
= \inf\Bigl\{ \sum_k \mu(S_k)^q \Big\rvert (S_k)_k\,\text{-- счётное }\epsilon\text{-покрытие }\text{supp}(\mu)\Bigr\}
\]
Под $\epsilon$-покрытием множества $M$ понимается набор измеримых множеств
$(S_k)_k$ в $[0,1]$ таких, что $\lvert S_k\rvert = \epsilon$ для любого $k$.
В общем случае рассматривается сепарабельное метрическое пространство $(X,d)$,
оснащённое Борелевской $\sigma$–алгеброй, где под$\lvert F\rvert$ понимается
диаметр множества $F$:
\[\lvert F\rvert = \sup\bigl\{ d(x,y) \big\rvert x,y\in F\bigr\}\]
Известно, что на тех множествах, на которых Хаусдорфова и Боксова размерности
совпадают, справедлива асимптотика
\[ R_{\mu,\epsilon}(q)\approx \epsilon^{\tau(q)} \]
причём $\tau(q) = \inf_\alpha \{\alpha q - f(\alpha)\}$, где $f(\alpha)$
-- мультифрактальный спектр меры $\mu$.

При любом $\epsilon>0$ нетрудно найти такой $l\geq 0$, что $\frac{1}{\epsilon}\leq d^l$.
Отсюда получается, что для $l_* = \rceil -\log_d \epsilon \lceil$ выполнено
\[\lvert I^{l_*}_k \rvert = \frac{1}{d^{l_*}} \leq \epsilon\]
Тогда для любого $k=0,\ldots,d^{l_*}-1$ значение мера $\mu(I^{l_*}_k)$ равна
\[\mu(I^{l_*}_k) = \prod_{m=0}^{d-1} p_m^{n_m}\]
где $n_m$ -- количество множителей $m$ в разложении $k$ по $d$-ичной базе:
\[k = \sum_{j=0}^{l_*-1} k_j d^j\]
Таким образом, степенная сумма в $R_{\mu,\epsilon}(q)$ равна
\[
\sum_{k=0}^{d^l-1} \mu(I^{l_*}_k)^q
= \sum_{j=0}^{l_*-1} \frac{l_*!}{\prod_{m=0}^{d-1} n_m!} \prod_{m=0}^{d-1} p_m^{q n_m}
= \Bigl(\sum_{m=0}^{d-1} p_m^q\Bigr)^{l_*}
\]
Итак
\[
\tau(q)
= \lim_{\epsilon\downarrow0}\frac{\ln R_{\mu,\epsilon}(q)}{\ln \epsilon}
= \lim_{l\to\infty}\frac{\ln \Bigl(\sum_{m=0}^{d-1} p_m^q\Bigr)^l }{\ln d^{-l}}
= \log_d \sum_{m=0}^{d-1} p_m^q
\]

В случае если $n$ значений из $(p_m)_{m=0}^{d-1}$ равны нулю, а остальные равны
константе $p$ ($\frac{1}{d-n}$), выполняется следующее :
\[
\tau(q)
= \log_d (d-n) + q\log_d p
% = \log_d (d-n) - q\log_d (d-n)
= (1 - q)\log_d (d-n)
\]
Например, мера Кантора (``канторова пыль'') на $[0,1]$ соответствует каскаду
с $d=3$ и $p_1=0$.

Чтобы вычислить мультифрактальный спектр меры необходимо вычислить обратное
преобразование Лежандра $\tau(q)$ : такую $f^*:I^*\to\Real$, что
\[f(\alpha) = \inf_q\bigl(\alpha q - \tau(q)\bigr)\]
где область определения задана
\[I^* = \bigl\{ \alpha\in\Real \big\rvert \rvert\inf_q\bigl(\alpha q - \tau(q)\bigr)\rvert <\infty\bigr\}\]

Поскольку $\tau(q) = (1 - q)\log_d (d-n)$, то разность в преобразовании Лежандра
\[
\alpha q - \tau(q)
= \alpha q - \log_d (d-n) + q\log_d (d-n)
= q \bigl(\alpha + \log_d (d-n)\bigr) - \log_d (d-n)
\]
является линейной по $q$. Следовательно, $\inf_q \alpha q - \tau(q) = -\infty$ для любых
$\alpha\neq \log_d (d-n)$, откуда вытекает, что $I^* = \{-\log_d (d-n)\}$ и
\[
f(\alpha)
= - \log_d (d-n)
= \log_d p
\]
Итак мера Кантора монофрактальна, поскольку $f$ определена только в одной точке
и в этой точке $f(\alpha) = \log_d p$.

% section problem_l2_s3 (end)

% [ВШЭ ТВМММ] Зачётная работа Назаров Иван.pdf

\end{document}
