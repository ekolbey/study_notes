\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}

\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\borel}{\mathcal{B}}

\newcommand{\Ex}{\mathbb{E}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Probabilistic methods in modelling}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

Theory of Stochastic processes
Brownian motion
Martingales
Poisson point processes

Generalizationo of the central limit theorem:
-- normal distribution
-- stable processes
-- Fisher-Tipett-Gendenko thorem for maxima

Based on the law of large numbers:
-- Extreme value theory

Additional chapters of probability

Elementary probability: Kolmogorov axioms, Measure theoretic approach.
Bernoulli law or large numbers.
Stochastic processes require measure theoretic foundations for their definition on some abstract space.

%% \selectlanguage{russian}
\section{Lecutre \#1} % (fold)
\label{sec:lecutre_1}


Suppose $\brac{X_k}_{k=1}^n$ is a finite collection of random variables which are independent and identically distributed.

What is the asymptotics of some function of $\brac{X_k}_{k=1}^n$. The probability distribution 

Consider a measure space $(\Omega, \Fcal, P)$ and an RV $X(\omega)$ from $\Omega$ to $(\mathcal{X}, \Sigma)$ on it.
This induces a measure in $\mathcal{X}$ -- the image space. see image~1.
\[\mathbb{P}_X \defn X_\# \mathbb{P} = \mathbb{P}\brac{X\in A} = \int 1_{X^{-1}(A)}d\mathbb{P}\]

For limiting theorems the image space is sufficient.

Abstract spaces are needed in stochastic processes (and with filtration).

Convergence in distribution is needed in limiting theorems.

Consider $X\in \Real$.
How is the distribution of $X$ defined?
It is defined using the semi-ring of half-closed intervals $\ploc{a,b}$.
Or, the most popular method -- Cumulative Distribution function.

An RV is not a measurable map, but a measure representing its probability.
A map $F:\Real\to \clo{0,+\infty}$ is a distribution function if \begin{enumerate}
	\item $F$ is non decreasing;
	\item $F(-\infty) = 0$ and $F(+\infty) = 1$.
	\item $F$ is right-continuous and bounded;
\end{enumerate}
in short $F$ must be c\'adl\'ag. The crucial thing is $F(\Real)=1$.

Modern statistical physics was founded on Gibbs's idea.
A physical system might be in different states, thus Gibbs and Bolzmann postulated that each state has some likelihood.

What is the likelihood of picking an even number in $\mathbb{Z}$? Suppose there is some probability measure on $\mathbb{Z}$ given by $\brac{p_n}_{n\in \mathbb{Z}}$ 
\[\sum_{n\text{ div } 2} p_n\]

Let $A\subseteq \mathbb{Z}$ then the density of $A$ is \[\rho(A) \defn \lim_{n\to+\infty, m\to -\infty} \frac{\abs{\obj{\induc{k\in \mathbb{Z}}\,n\leq k\leq m }}}{n+m+1}\]
However $\rho(\cdot)$ fails to be countably sub-additive.

Classification of distribution functions on $\Real$. \begin{description}
	\item[Atomic:] \hfill \\
		there is a countable and measurable subset $A\in \borel(\Real)$ and a collection $\brac{p_k}_{k\in A}$ such that
			\[F(x) = \sum_{k\in A} p_k 1_{\obj{k}}(x)\]
	\item[Singular:] \hfill \\
		all the rest. 
	\item[Absolutely continuous:] \hfill \\
		there exists a lebesgue-measurable map $p:\Real\to\Real$ with $F(x)=\int p(x) dx$
\end{description}

Lebesgue theorem: every distribution function can be represented as a weighted sum of three basic types of distributions.


Classification of local behaviour of distributions in $\Real$.

Consider some $x_0\in \Real$ and consider the difference $F(x_0+\Delta)-F(x_0-\Delta)$ for $\Delta > 0$.
The function $F$ has the singularity order $\alpha$ if \[F(x_0+\Delta)-F(x_0-\Delta) = \Omega(\Delta^\alpha)\]

\begin{description}
	\item[Atomic:] $\alpha = 0$;
	\item[Continuous:] $\alpha = 1$;
\end{description}

If $F(x)=\min\obj{\sqrt{x}, 1} 1_{\clop{0,+\infty}}(x)$ then $0$ has singularity order $\frac{1}{2}$.

Cantor distribution has uncountably many points with $\alpha = 0$, and uncountably many points with $\alpha = \frac{\log 2}{\log 3}$.
A good idea is to represent any $x\in \clo{0,1}$ in base-3 representation. The ``interesting'' points have either $0$ or $2$ in their base-3 form.

\noindent\textbf{Problem \#1}\hfill \\
	Given $U\sim \mathcal{U}\clo{0,1}$ and $Y=f(U)$. Find $f$ such that the density $p_Y$ in given by
	\[\lambda e^{-\lambda y} 1_{\clop{0,+\infty}}\]
	If $F(x)$ is absolutely continuous and $X\sim F$, then $F(X)\sim \mathcal{U}\clo{0,1}$. (Skorokhod)

\noindent\textbf{Problem \#2}\hfill \\
	Suppose $(X,Y)$ is uniformly distributed inside the unit circle. Let $\rho \defn \sqrt{X^2+Y^2}$. What is the distribution of $(x',y')$ with the radius given by $r = \sqrt{-\log \rho}$.
	Using the Jacobian transformation, the distribution of the transformed pair is jointly gaussian.

% section lecutre_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

A shorter lecture, no definition of expectation, but full of functions:
\begin{itemize}
	\item Probability generating functions;
	\item Moment generating functions;
	\item Characteristic functions.
\end{itemize}

Suppose $X$ is a real-valued random variable with law $F_X$. If $F_X$ is absolutely continuous with respect to $dx$, then $F_X(x) = \int_{-\intfty}^x f dx$.

The expectation of $X$ is defined as \[\Ex(X) \defn \int X d{F_X}\] and in the case of absolutely continuous distribution it is true that \[\int X d{F_X} = \int X f(x) dx\]

Riemann-Stieltjes integrals are used (but those who know, can use a better Lebesgue-Stieltjes integral).
\[\int d dF = \lim_{\Delta\to 0} \sum_i g(x_i) \brac{F(\xi_{i+1}) - F(\xi_i)}\]
Lebesgue integral is pedagogically more difficult, though more abstract, flexible and overall better.
% Geometric measure theory.

Cauchy distribution does not allow $\Ex X$, however it has the mean in terms of the \textbf{main value} (integration over a symmetric interval $\clo{-M,+M}$).
% \rus{Уравнение Власова}

The expectation of a function $g$ of $X$ is $\Ex\brac{g(X)} = \int g(X) dF_X$.

The $k$-th moment of an RV is $\Ex X^k$ and the $k$-th central moment is $\Ex\brac{ X - \Ex X}^2$.
The second central moment of $X$ is the variance of $X$ : $\mathbb{D}(X) \defn \Ex\brac{ X - \Ex X}^2$.

Jensen's inequality. For any convex function $g$ of $X$ it is true that 
\[g\brac{\Ex X}\leq \Ex g(X) \]

Fatou's lemma \[\int \liminf X_n d\mu \leq \liminf \int X_n d\mu\]

Holder's inequality for nonnegative measurable $f$ and $g$ \[\int fg d\mu \leq \brac{\int f^p d\mu}^\frac{1}{p}\brac{\int g^q d\mu}^\frac{1}{q}\] whenever $\sfrac{1}{p}+\sfrac{1}{q}=1$.

Minkowski inequality \[\brac{\int {(f+g)}^p d\mu}^\frac{1}{p}\leq \brac{\int f^p d\mu}^\frac{1}{p} + \brac{\int g^p d\mu}^\frac{1}{p}\]

The probability generating function is defined for any $z\in \Cplx$ with $\abs{z}\leq 1$ as \[G_X(z) \defn \Ex z^X\]
If $X$ has discrete distribution, then $G_X(z) = \sum_{n\geq 0} z^n \Pr(X=n)$.
Since the series is analytical in a finite disc, it is differentiable.
Thus \[G_X^{(k)}(z) = \sum_{n\geq 0} \frac{n!}{(n-k)!} z^{n-k} \Pr(X=n) \]
whence $G_X^{(k)}(0) = \Ex \brac{X(X-1)\ldots (X-k+1)}$

Factorial exponent: $x^{\underline{k}} = \prod_{s=1}^k (X-s+1)$.
\[x^{\underline{k}} - \brac{x-1}^{\underline{k}} = k \cdot \brac{x-1}^{\underline{k-1}} \]

The moment generating function of $X$ is given by \[S_X(t) \defn G(e^t) = \Ex(e^{tX})\]
Differentiation of the MGF and evaluation at $t=0$ yields moments of $X$.
Indeed, if the integral converges uniformly, then $S_X^{(k)}(t) = \induc{\Ex X^k e^{tX}}_{t=0} = \Ex X^k$.

\emph{A gentle reminder}\hfill \\ For any pair of independent random variables the expectation of their product equals the product of their expectations.

Thus the mgf of a sum of independent $X,Y$ results in \[S_{X+Y}(t) = \Ex e^{t(X+Y)} = \Ex e^{tX} e^{tY} = \Ex e^{tX} \Ex e^{tY} = S_X(t) S_Y(t)\]

The cumulants of $X$ (or semiinvariants) are defined as the coefficients in the following formal series expansion of $\log S_X(t)$ as
\[\log S_X(t) \defn \sum_{n\geq 0} \frac{t^n}{n!} \kappa_n^X\]

Using the formal differentiation of the series with respect to $t$ yields
$\kappa_n^X = \induc{\frac{d^n}{dt^n} \log S_X(t)}_{t=0}$.

Since $S_X(t)$ is additive with respect to $X$ if the summands are independent,
Kurtosis, \rus{эксцесс} -- measures how far the probability is shifted to the tails.

\textbf{Martsinkevich's theorem}\hfill\\
$\log S_X(t)$ is a polynomial if and only if $X\sim\mathcal{N}(\mu,\sigma)$.


Solve exercises 1.8, 1.11, 1.12 and 1.13 on pp.~42-43.

% section lecture_2 (end)

\end{document}
