\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Rbar}{{\bar{\mathbb{R}}}}
\newcommand{\Cplx}{\mathbb{C}}

\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\borel}{\mathcal{B}}

\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\pr}{\mathbb{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}
\newcommand{\sign}{\mathop{\text{sgn}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Probabilistic methods in modelling}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

Theory of Stochastic processes
Brownian motion
Martingales
Poisson point processes

Generalizationo of the central limit theorem:
-- normal distribution
-- stable processes
-- Fisher-Tipett-Gendenko thorem for maxima

Based on the law of large numbers:
-- Extreme value theory

Additional chapters of probability

Elementary probability: Kolmogorov axioms, Measure theoretic approach.
Bernoulli law or large numbers.
Stochastic processes require measure theoretic foundations for their definition on some abstract space.

%% \selectlanguage{russian}
\section{Lecutre \#1} % (fold)
\label{sec:lecutre_1}


Suppose $\brac{X_k}_{k=1}^n$ is a finite collection of random variables which are independent and identically distributed.

What is the asymptotics of some function of $\brac{X_k}_{k=1}^n$. The probability distribution 

Consider a measure space $(\Omega, \Fcal, P)$ and an RV $X(\omega)$ from $\Omega$ to $(\mathcal{X}, \Sigma)$ on it.
This induces a measure in $\mathcal{X}$ -- the image space. see image~1.
\[\mathbb{P}_X \defn X_\# \mathbb{P} = \mathbb{P}\brac{X\in A} = \int 1_{X^{-1}(A)}d\mathbb{P}\]

For limiting theorems the image space is sufficient.

Abstract spaces are needed in stochastic processes (and with filtration).

Convergence in distribution is needed in limiting theorems.

Consider $X\in \Real$.
How is the distribution of $X$ defined?
It is defined using the semi-ring of half-closed intervals $\ploc{a,b}$.
Or, the most popular method -- Cumulative Distribution function.

An RV is not a measurable map, but a measure representing its probability.
A map $F:\Real\to \clo{0,+\infty}$ is a distribution function if \begin{enumerate}
	\item $F$ is non decreasing;
	\item $F(-\infty) = 0$ and $F(+\infty) = 1$.
	\item $F$ is right-continuous and bounded;
\end{enumerate}
in short $F$ must be c\'adl\'ag. The crucial thing is $F(\Real)=1$.

Modern statistical physics was founded on Gibbs's idea.
A physical system might be in different states, thus Gibbs and Bolzmann postulated that each state has some likelihood.

What is the likelihood of picking an even number in $\mathbb{Z}$? Suppose there is some probability measure on $\mathbb{Z}$ given by $\brac{p_n}_{n\in \mathbb{Z}}$ 
\[\sum_{n\text{ div } 2} p_n\]

Let $A\subseteq \mathbb{Z}$ then the density of $A$ is \[\rho(A) \defn \lim_{n\to+\infty, m\to -\infty} \frac{\abs{\obj{\induc{k\in \mathbb{Z}}\,n\leq k\leq m }}}{n+m+1}\]
However $\rho(\cdot)$ fails to be countably sub-additive.

Classification of distribution functions on $\Real$. \begin{description}
	\item[Atomic:] \hfill \\
		there is a countable and measurable subset $A\in \borel(\Real)$ and a collection $\brac{p_k}_{k\in A}$ such that
			\[F(x) = \sum_{k\in A} p_k 1_{\obj{k}}(x)\]
	\item[Singular:] \hfill \\
		all the rest. 
	\item[Absolutely continuous:] \hfill \\
		there exists a lebesgue-measurable map $p:\Real\to\Real$ with $F(x)=\int p(x) dx$
\end{description}

Lebesgue theorem: every distribution function can be represented as a weighted sum of three basic types of distributions.


Classification of local behaviour of distributions in $\Real$.

Consider some $x_0\in \Real$ and consider the difference $F(x_0+\Delta)-F(x_0-\Delta)$ for $\Delta > 0$.
The function $F$ has the singularity order $\alpha$ if \[F(x_0+\Delta)-F(x_0-\Delta) = \Omega(\Delta^\alpha)\]

\begin{description}
	\item[Atomic:] $\alpha = 0$;
	\item[Continuous:] $\alpha = 1$;
\end{description}

If $F(x)=\min\obj{\sqrt{x}, 1} 1_{\clop{0,+\infty}}(x)$ then $0$ has singularity order $\frac{1}{2}$.

Cantor distribution has uncountably many points with $\alpha = 0$, and uncountably many points with $\alpha = \frac{\log 2}{\log 3}$.
A good idea is to represent any $x\in \clo{0,1}$ in base-3 representation. The ``interesting'' points have either $0$ or $2$ in their base-3 form.

\noindent\textbf{Problem \#1}\hfill \\
	Given $U\sim \mathcal{U}\clo{0,1}$ and $Y=f(U)$. Find $f$ such that the density $p_Y$ in given by
	\[\lambda e^{-\lambda y} 1_{\clop{0,+\infty}}\]
	If $F(x)$ is absolutely continuous and $X\sim F$, then $F(X)\sim \mathcal{U}\clo{0,1}$. (Skorokhod)

\noindent\textbf{Problem \#2}\hfill \\
	Suppose $(X,Y)$ is uniformly distributed inside the unit circle. Let $\rho \defn \sqrt{X^2+Y^2}$. What is the distribution of $(x',y')$ with the radius given by $r = \sqrt{-\log \rho}$.
	Using the Jacobian transformation, the distribution of the transformed pair is jointly gaussian.

% section lecutre_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

A shorter lecture, no definition of expectation, but full of functions:
\begin{itemize}
	\item Probability generating functions;
	\item Moment generating functions;
	\item Characteristic functions.
\end{itemize}

Suppose $X$ is a real-valued random variable with law $F_X$. If $F_X$ is absolutely continuous with respect to $dx$, then $F_X(x) = \int_{-\infty}^x f dx$.

The expectation of $X$ is defined as \[\Ex(X) \defn \int X d{F_X}\] and in the case of absolutely continuous distribution it is true that \[\int X d{F_X} = \int X f(x) dx\]

Riemann-Stieltjes integrals are used (but those who know, can use a better Lebesgue-Stieltjes integral).
\[\int d dF = \lim_{\Delta\to 0} \sum_i g(x_i) \brac{F(\xi_{i+1}) - F(\xi_i)}\]
Lebesgue integral is pedagogically more difficult, though more abstract, flexible and overall better.
% Geometric measure theory.

Cauchy distribution does not allow $\Ex X$, however it has the mean in terms of the \textbf{main value} (integration over a symmetric interval $\clo{-M,+M}$).
% \rus{Уравнение Власова}

The expectation of a function $g$ of $X$ is $\Ex\brac{g(X)} = \int g(X) dF_X$.

The $k$-th moment of an RV is $\Ex X^k$ and the $k$-th central moment is $\Ex\brac{ X - \Ex X}^2$.
The second central moment of $X$ is the variance of $X$ : $\mathbb{D}(X) \defn \Ex\brac{ X - \Ex X}^2$.

Jensen's inequality. For any convex function $g$ of $X$ it is true that 
\[g\brac{\Ex X}\leq \Ex g(X) \]

Fatou's lemma \[\int \liminf X_n d\mu \leq \liminf \int X_n d\mu\]

Holder's inequality for nonnegative measurable $f$ and $g$ \[\int fg d\mu \leq \brac{\int f^p d\mu}^\frac{1}{p}\brac{\int g^q d\mu}^\frac{1}{q}\] whenever $\sfrac{1}{p}+\sfrac{1}{q}=1$.

Minkowski inequality \[\brac{\int {(f+g)}^p d\mu}^\frac{1}{p}\leq \brac{\int f^p d\mu}^\frac{1}{p} + \brac{\int g^p d\mu}^\frac{1}{p}\]

The probability generating function is defined for any $z\in \Cplx$ with $\abs{z}\leq 1$ as \[G_X(z) \defn \Ex z^X\]
If $X$ has discrete distribution, then $G_X(z) = \sum_{n\geq 0} z^n \Pr(X=n)$.
Since the series is analytical in a finite disc, it is differentiable.
Thus \[G_X^{(k)}(z) = \sum_{n\geq 0} \frac{n!}{(n-k)!} z^{n-k} \Pr(X=n) \]
whence $G_X^{(k)}(0) = \Ex \brac{X(X-1)\ldots (X-k+1)}$

Factorial exponent: $x^{\underline{k}} = \prod_{s=1}^k (X-s+1)$.
\[x^{\underline{k}} - \brac{x-1}^{\underline{k}} = k \cdot \brac{x-1}^{\underline{k-1}} \]

The moment generating function of $X$ is given by \[S_X(t) \defn G(e^t) = \Ex(e^{tX})\]
Differentiation of the MGF and evaluation at $t=0$ yields moments of $X$.
Indeed, if the integral converges uniformly, then $S_X^{(k)}(t) = \induc{\Ex X^k e^{tX}}_{t=0} = \Ex X^k$.

\emph{A gentle reminder}\hfill \\ For any pair of independent random variables the expectation of their product equals the product of their expectations.

Thus the mgf of a sum of independent $X,Y$ results in \[S_{X+Y}(t) = \Ex e^{t(X+Y)} = \Ex e^{tX} e^{tY} = \Ex e^{tX} \Ex e^{tY} = S_X(t) S_Y(t)\]

The cumulants of $X$ (or semiinvariants) are defined as the coefficients in the following formal series expansion of $\log S_X(t)$ as
\[\log S_X(t) \defn \sum_{n\geq 0} \frac{t^n}{n!} \kappa_n^X\]

Using the formal differentiation of the series with respect to $t$ yields
$\kappa_n^X = \induc{\frac{d^n}{dt^n} \log S_X(t)}_{t=0}$.

Since $S_X(t)$ is additive with respect to $X$ if the summands are independent,
Kurtosis, \rus{эксцесс} -- measures how far the probability is shifted to the tails.

\textbf{Martsinkevich's theorem}\hfill\\
$\log S_X(t)$ is a polynomial if and only if $X\sim\mathcal{N}(\mu,\sigma)$.


Solve exercises 1.8, 1.11, 1.12 and 1.13 on pp.~42-43.

% section lecture_2 (end)

% Task 1.8
Suppose $X$ takes $k$ distinct values at random. 



The number of partitions of a set of $n$ elements into $k$ classes is given by the Stirling Number of the second kind.
The $n$-th element can either form an isolated class in $P(n-1,k-1)$different ways,
or be a member of one of the existing $k$ classes in $P(n-1,k)$ number of ways.
Therefore \[P(n,k) = P(n-1,k-1) + k P(n-1,k)\]


% Task 1.13 -- wrong!!!!
Suppose each node spawns an $m$-tree and that each edge in any tree in impassable with probability $p$.
What is the probability $\pi_n$ of a tree with $n$ layers such that there is no paths from the root to leaves.

Since the tree is recursive and independent the following is valid:
\[\pi_{n+1} = (1 - p^m) \pi_n\]

The exact moment when the main gates closed created a root in the tree structure of chambers.


\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

%% Missed the first part of the lecture
Tracey-Widam distribution

\textbf{T}otally \textbf{A}symmatric \textbf{S}imple \textbf{E}xclusion \textbf{P}rocess: when a particles on a lattice can jump and shift to the right, but not over another particle.

% Random matrix theory -- Ivan Corvin

Asymptotic (high dimensional) representation theory:
KPZ scaling and Tracey-Widam

The (weak) Law of Large Numbers:
\[\pr\brac{\abs{ \frac{\sum_{k=1}^n X_k}{n}-\mu }>\epsilon} \to 0\]


\noindent \textbf{Definition} of weak convergence. \hfill\\
A sequence of random variables $\brac{X_n}_{n\geq1}$ converges weakly to $X$, or in distribution, if for all $a,b\in \Rbar$, such that $a,b$ are atoms, probability concentrations:
\[\pr\brac{a<X_n<b}\to \pr\brac{a<X<b}\]

\noindent \textbf{Proposition} 1\hfill \\
$X_k\overset{D}{\to}X$ if and only if $F_{X_n}\to F_X$ at any point of continuity of $F$.

Continuity is required, because the CDf is c\'adlag (only right continuous).

Let $\brac{x_k}_{k\geq0}$ be a monotonous deterministic sequence, which converges to $\mu$.
If $x_k\uparrow$, then CDFs of a deterministic sequence do converge to the CDF of their limit. And not otherwise.

\noindent \textbf{Proposition} 1\hfill \\
$X_k\overset{D}{\to}X$ if and only if $F_{X_n}\to F_X$ at any point of continuity of $F$.

\noindent \textbf{Proposition} 2 (Helly)\hfill \\
Every sequence of distribution functions $\brac{F_n}_{n\geq1}$ it is possible to select a subsequence, such that $F_{n_k}$ converges to some function $F$ at every point of continuity of $F$ and such that $F$ is monotonous.

Note that the function $F$ is non-normalised.

Sketch:
Use the rational numbers $\mathbb{Q}$ as the \textbf{knots} of convergence of $F$. Enumerate them and choose the ``diagonal''.

Proof. Suppose $\brac{q_n}_{n\geq 1}$ enumerates $\mathbb{Q}$.
The sequence $\brac{F_n(q_1)}_{n\geq1}$ is bounded, hence it has a convergent subsequence, whence exists $\brac{k_n}\uparrow\infty$ such that $F_{k_n}(q_1)$ converges to some $F(q_1)$.
Consider $\brac{F^1_n}_{n\geq 1}\defn\brac{F_{k_n}}_{n\geq1}$ as a new sequence, and apply the same argument to it recursively.

Finally we get a sequence $\brac{F^m_n}_{n\geq1}$ such that $F^m_n(q_l)$ converges to $F(q_l)$ for all $l\leq m$ as $n\to \infty$, either by being a subsequence or by the choice of the subsequence $n_k$.

Use Cantor's diagonal argument: form a new sequence $\brac{F^n_n}_{n\geq1}$. This sequence converges at any $q\in\mathbb{Q}$ to $F(q)$.

Now to prove that $F$ is continuous.

If $\lim_{q\uparrow x}F(q) = \lim_{p\downarrow x}F(p)$, then set $F(x)$ to the limit.

The function then becomes either continuous, or left limited and right continuous, since $F_n$ are monotonous and right-continuous.

Counterexample:
Suppose $X_n=n$. Then $F_n(x) = 1_{\ploc{-\infty, x}}(n)$.

In order to get a requirement in Helly's theorem, which guarantees that the limiting function is a distribution function, one has to require the following ``tightness'' condition (uniform property -- nor all $n$):
\[\forall \epsilon>0\,\exists L>0\,\text{s.t.}\,\sup_{n\geq1}\pr\brac{\abs{X_n}>L}\leq \epsilon\]

% See the photographs of the explanation of the method of closed loops.

% Алгебра -- рай, а недифференцируемость -- нечистая чила

\noindent \textbf{The main technical result}\hfill\\

The sequence of distribution function $F_n$ converges weakly to some $F$ if $\phi_n\to \phi$, their associated characteristic functions converge, and $\phi$ is continuous at $0$.

\noindent \textbf{Proposition}\hfill\\

Suppose $X_n\to X$ and $\phi_n(s)\defn \Ex\brac{e^{isX_n}}$. Then there exists a limiting function of $\phi_n$ such that $\phi_n\to \phi = \Ex\brac{e^{isX}}$.

Let's show that if $X_n\overset{D}{\to} X$ then
\[\int g(X) dF_n \to \int g(X)dF\]
for any bounded continuous function $g$.

\textbf{Proof}\hfill\\
Split the integration region into two regions:
\[\int f dF_n = \int_\obj{\abs{X}>R} g dF_n + \int_\obj{\abs{X}\leq R} g dF_n\]

The tightness condition implies that there exists $R$ such that
\[F_n\brac{\obj{\abs{X}>R}}< \frac{\epsilon}{2}\]
which means that $\int_\obj{\abs{X}>R} g dF_n$ is bounded above by $M\frac{\epsilon}{2}$ where $\abs{g}\leq M$.

Since $g$ is continuous on a compact $\obj{\abs{X}\leq R}$, for $\epsilon>0$ there exists $\delta>0$ such that on each $\delta$-range the change of $g$ is not greater that $\epsilon$.
therefore 
\[\int_\obj{\abs{X}\leq R} g dF_n = \int_\obj{\abs{X}\leq R} (g-g^\epsilon) dF_n + \int_\obj{\abs{X}\leq R} g^\epsilon dF_n \leq F_n\brac{\obj{\abs{X}\leq R}}\frac{\epsilon}{2} + \int_\obj{\abs{X}\leq R} g^\epsilon dF_n\]

where $g^\epsilon$ is an $\epsilon$ step function.

However
\[\int_\obj{\abs{X}\leq R} g^\epsilon dF_n\]
is a finite sum $\sum_i g^\epsilon$

Therefore if $X_n\overset{D}{\to} X$ then characteristic function s converge.


% Feller -- Probability: a neat proof

\noindent\textbf{Proposition} \hfill\\
If
\[\phi(s) = \int e^{isx} dF(x)\]
then for any $b>0$ the following equivalence is true for all $y$:
\[\frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds
=\frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF(x)\]
for a gaussian smothing kernel $e^{-\frac{{(x-y)}^2}{2\delta^2}}$.

Indeed, \begin{align*}
	e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} &= e^{-isy}\int e^{isx} e^{-\frac{\delta^2 s^2}{2}} dF(x)\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int e^{-isy}\int e^{isx} e^{-\frac{\delta^2 s^2}{2}} dF(x) ds\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int \int e^{is(x-y)-\frac{\delta^2 s^2}{2}} dF(x) ds\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int \int e^{-\frac{{(x-y)}^2}{2\delta^2}} e^{\frac{\delta^2}{2}\brac{-i\frac{x-y}{\delta^2}+s}^2} dF(x) ds\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int e^{-\frac{{(x-y)}^2}{2\delta^2}} \int e^{\frac{\delta^2}{2}\brac{-i\frac{x-y}{\delta^2}+s}^2} ds dF(x)\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int e^{-\frac{{(x-y)}^2}{2\delta^2}} \frac{\sqrt{2\pi}}{\delta} dF(x)\\
% Missed some details...
\end{align*}


Another lemma:
If the characteristic functions of different distributions are never identical.

Indeed, if characteristic functions coincide, but but distributions differ at least in some point, then there is a contradiction

Suppose $\phi_n(x) = \Ex\brac{e^{isX_n}}$ converges to some $\phi$ -- and $\phi$ is continuous at $x=0$, then there exists a random variable, such that $\phi(s) = \Ex\brac{e^{isX}}$ and $X_n\to X$ weakly.

SupposE $Y_k$ are iid and $X_n = \sum_{k=1}^n Y_k$ with $\Ex\brac{e^{isY}} = \psi(s) = e^{-\abs{s}}$.

Then $\phi_n(s) = \Ex\brac{e^{isX}} = \brac{\phi(s)^n}$, but it converges to $1_{s=0}$.

% the probability is smeared along the infinity/

\begin{align*}
	\frac{1}{2\pi}\int e^{-sy} \phi_n(s) e^{-\frac{\delta^2 s^2}{2}}ds
	& = \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF_n(x)
\end{align*}

Helly's theorem implies that there is a convergent subsequence $F_{n_k}$ that converges to some $F$.

Then analogously to a previously stated proposition
\[\frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF_n(x) \to \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF\]

Using Lebesgue's Dominated convergence theorem it is true that
\[\frac{1}{2\pi}\int e^{-sy} \phi_{n_k}(s) e^{-\frac{\delta^2 s^2}{2}}ds \to \frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds\]

Therefore
\[\frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds = \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF\]
Since $F$ is an arbitrary limiting function, this implies that any other convergent subsequence converges to the same $F$, whence $F_n\to F$.

To show that $F$ is a CDF we do the following: think of letting $\delta\to \infty$ and fixing $y=0$. Then
\[\frac{\delta^2}{\sqrt{2\pi}}\int \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds = \int e^{-\frac{x^2}{2\delta^2}}dF \leq \int dF\]

However as $\delta\to\infty$ it is true that
\[\int \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds\to \phi(0) = 1\]
since $\phi(0)$ is continuous at $0$.

Thus $1 \leq \int dF$. But $\int dF$ cannot strictly exceed $1$, whence $F$ is a distribution function.

\noindent\textbf{Lyapunov's Central Limit Theorem} \hfill\\
Suppose $X_k$ are i.i.d. with $\Ex X_k = \mu$ and $\Ex X_k^2 = \sigma^2$. Introduce the following sequence:
\[Y_n = \frac{\sum_{k=1}^n X_k - n \mu }{\sqrt{n}}\]
Then $Y_n\overset{D}{\to}\mathcal{N}(0,1)$

\textbf{Proff}\hfill\\

If $Z_n = \sum_{k=1}^n X_k - n\mu$. Then $\Ex Z_n = 0$ and $\Var Z_n = n \sigma^2$ due to independence.

Let $\phi_n(s) = \Ex\brac{e^{isZ_n}}$. Then
\[\phi_n(s) = 1 + 0 - \frac{n\sigma^2}{2} s^2 + o(s^2)\]

Now
\[\phi_{Y_n}(s) = \Ex\brac{e^{is\frac{Z_n}{\sigma \sqrt{n}}}} = \phi_n(\frac{s}{\sigma\sqrt{n}})\]
whence
\[\phi_{Y_n}(s) = 1 - \frac{s^2}{2} + o(s^2)\]


Another way: due to independence of $X_k$ and using the product-independence property of characteristic function
\[\phi_n(s) = \brac{\phi_{X-\mu}(s)}^n = \brac{ 1 - \frac{n\sigma^2}{2} s^2 + o(s^2) }^n\]

Therefore
\[\phi_{Y_n}(s) = \phi_n(\frac{s}{\sqrt{n}\sigma}) = \brac{ 1 - \frac{s^2}{2n} + o(s^2) }^n\]
 where $o(\cdot)$ is uniformly infinitesimal with respect to $n$.

Using a well known limit we get the following limit:
\[\phi_{Y_n}(s) \to e^{-\frac{s^2}{2}}\]

an example:
Suppose $\brac{X_n}_{n\geq1}$ are independent identically distributed such that $\phi_X(s) = 1+i\mu s + o(s)$, where $o(\cdot)$ is an infinitesimal.

Let \[Y_n = \frac{\sum_{k=1}^n X_k - n \mu }{n}\]
then
\[\phi_{Y_n}(s) = \phi_{\sum}(\frac{s}{n})
= \brac{\phi_X(\frac{s}{n})}^n = \brac{1+i\mu s + o(s)}^n
\to e^{is\mu}\]

The necessary and sufficient condition for $\phi(s) = 1+i s\mu+o(s)$ is \[\lim_{R\to\infty} R\cdot \brac{F_X(-R) + 1 - F_X(R)} = 0\]
% Khinchine's law of large numbers.

% Tutorials
% Homework У2.1, У2.6, У2.7, У2.8, У2.9, У3.9, У3.3, У3.6

% section lecture_3 (end)

\section{lecture \# 4} % (fold)
\label{sec:lecture_4}

Gamma distribution.
\[\gamma(k,\beta) \defn \frac{\beta^k}{\Gamma(k)} s^{k-1} e^{-\beta s}\]

It has all the moments, since the exponent decays very rapidly, faster than any polynomial.

Some exampls of the $\gamma$ distribution are \begin{description}
	\item[$\chi^2_k$] A $\chi^2$ random variable is actually $\gamma\brac{\frac{k}{2},\frac{1}{2}}$;
	\item[$\text{Exp}(\lambda)$] An exponential distribution is a particular case of the Gamma distribution: $\text{Exp}(\lambda) \sim \gamma(1,\lambda)$.
\end{description}

This distribution is asymmetric.
\[\frac{\sum_{k=1}^n G_k- }{}\]

Limiting behaviour of products of random variables.

Suppose $\brac{X_k}_{k=1}^n$ are non-negative random variables. Consider the asymptotics of \[P_n \defn \brac{e^{-\mu n}\prod_{k=1}^n G_k}^{\sqrt{n}}\]

The natural approach is to use $Y_k\defn \log X_k$ and consider the asymptotics of the logarithm of the product: the normalised sum
\[S_n \defn \log P_n \frac{\sum_{k=1}^n Y_k - \mu n}{\sqrt{n}}\]

Since this converges in distribution to a standard normal random variable: $S_n \overset{\mathcal{D}}{\to} \mathcal{N}(0,1)$. Thus the products converge in distribution to a log-normal random variable:
\[\frac{1}{\sqrt{2\pi\sigma^2}} e^\frac{-\log^2 x}{2\sigma^2}\frac{1}{x}\]

What happens to the central limit theorem, when its assumptions fail.
\begin{description}
	\item[Independence] Stochastic processes CLT;
	\item[Identical distribution] The most useful generalisation of the CLT;
	\item[Moments] What if $\Ex(X_k) = \mu$ and $\text{Var}(X_k) = \sigma^2$ are violated?
\end{description}

Consider the Cauchy distributed random variable. IS $\brac{X_n}_{n\geq1}\sim \text{Cauchy}$, then their normalised sum is still Cauchy (consider the characteristic function).

What precludes a distribution from having finite variance?

The main moment is the computation of the asymptotics at zero of the characteristic function (the Fourier transform of the distribution).

Consider the following a bit contrived density:
\[p(x) = 1_\obj{\abs{x}\leq 1}\frac{A}{2} + 1_\obj{\abs{x}>1}\frac{A}{2\abs{x}^{\alpha+1}}\]
where $A=\frac{\alpha}{\alpha+1}$ and $\alpha$ - the Levy exponent. The exponent characterises asymptotics convergences of the CDF to the horizontal asymptotes: $\abs{x}^{-\alpha}$. In the limit we get the Pareto-Levy family of distributions.

Let's use the characteristic functions.
\[\phi(s)\defn\int e^{-isx}p(x)dx\]
Consider the asymptotics near zero (the problems at infinity i time domain transform to the problems near zero in the frequency domain).

Since $p(x)$ is symmetrical
\begin{align*}
	\phi(s) &= \Ex\brac{e^{-isX} } = \int e^{-isx}p(x)dx \\
	&= \int_{-1}^1 \frac{A}{2} e^{-isx} dx + \brac{\int_1^\infty + \int_{-\infty}^{-1}} \frac{A}{2} \frac{e^{-isx} }{\abs{x}^{\alpha+1}} dx
\end{align*}

Now the first integral is just \begin{align*}
	\int_{-1}^1 e^{-isx} dx &= \int_{-1}^1 \cos{sx} -i\sin{sx} dx \\ 
	&= \int_{-1}^1 \cos{sx} dx - i \int_{-1}^1 \sin{sx} dx \\
	& =  \induc{\frac{\sin{sx}}{s}}_{-1}^1 - i \induc{\frac{-\cos{sx}}{s}}_{-1}^1\\
	& = \frac{\sin{s}}{s}-\frac{\sin{-s}}{s} = 2 \frac{\sin{s}}{s} = 2 \frac{\sin\abs{s}}{\abs{s}}
\end{align*}
while the second is a little bit more intricate:
\begin{align*}
	\brac{\int_1^\infty + \int_{-\infty}^{-1}} \frac{e^{-isx} }{\abs{x}^{\alpha+1}} dx
	&= \int_1^\infty \frac{\cos{sx}-i\sin{sx}}{x^{\alpha+1}} dx + \int_{-\infty}^{-1} \frac{\cos{sx}-i\sin{sx}}{\brac{-x}^{\alpha+1}} dx \\
	&= \int_1^\infty \frac{\cos{sx}-i\sin{sx}}{x^{\alpha+1}} dx + \int_1^\infty \frac{\cos(-sy)-i\sin(-sy)}{y^{\alpha+1}} dy \\
	&= \int_1^\infty \frac{2\cos{sx}}{x^{\alpha+1}} dx = 2\int_1^\infty \frac{\cos{\abs{s}x}}{x^{\alpha+1}} dx \\
	& = 2\abs{s}^\alpha \int_1^\infty \frac{\cos{\abs{s}x}}{\brac{\abs{s}x}^{\alpha+1}} \abs{s}dx = \clo{\xi = \abs{s}x} = 2\abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi
\end{align*}

Thus the characteristic function simplifies to
\[\phi(s) = A \frac{\sin\abs{s}}{\abs{s}} + A \abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi \]

The integral can be dealt with in the following way: if $\alpha>0$ then \begin{align*}
	\int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi & = 
	\int_{\abs{s}}^\infty \frac{1}{\xi^{\alpha+1}} d\xi - \int_{\abs{s}}^\infty \frac{\xi - \cos\xi}{\xi^{\alpha+1}} d\xi\\
	& = \induc{-\frac{\xi^{-\alpha}}{\alpha}}_{\abs{s}}^\infty - \int_{\abs{s}}^\infty \frac{\xi - \cos\xi}{\xi^{\alpha+1}} d\xi
\end{align*}
%%  FINISH AT HOME!!!

\begin{align*}
A \abs{s}^\alpha \int_\abs{s}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt &=  A \abs{s}^\alpha \int_\abs{s}^\infty \frac{1}{t^{\alpha+1}} dt - A \abs{s}^\alpha \int_\abs{s}^\infty \frac{1-\cos(t)}{t^{\alpha+1}}dt
\end{align*}


It is possible to show that the integral is asymptotically equal to
\[\int_\abs{s}^\infty \frac{1-\cos(t)}{t^{\alpha+1}}dt \sim I_1(\alpha) + o(\abs{s}^{2-\alpha})\]

\begin{align*}
A \abs{s}^\alpha \int_\abs{s}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt &= \abs{s}^\alpha \frac{1}{\alpha} \abs{s}^{-\alpha} + AI_1(\alpha)\abs{s}^\alpha + o(s^2)
\end{align*}

Thus \[\phi(s) = 1 - A I_1(\alpha) \abs{s}^\alpha + o(s^2)\]

Since the variables are independent, then the sum has the product characteristic function
\[\Ex\brac{e^{-is\frac{S_n}{n^\frac{1}{\alpha}}}} = \brac{1 - A I_1(\alpha) \frac{\abs{s}^\alpha}{n} + o(s^2)}^n \to e^{-AI_1(\alpha)\abs{s}^\alpha}\]

THus it turns out that if $F(x)\underset{x\to\infty}{\sim} \abs{x}^{-\alpha}$,  $F(x)\underset{x\to-\infty}{\sim} 1-\abs{x}^{-\alpha}$ and the tails are symmetric, then \[\frac{\sum_{k=1}^{n}X_k}{n^\frac{1}{\alpha}}\to X\] where the random variable $X$ has the characteristic function given by \[\phi(s) = e^{-\frac{\alpha}{\alpha+1} I_1(\alpha)\abs{s}^\alpha}\] -- the Levy-Pareto characteristic function.

When $\alpha=2$, then the infinitesimal in the asymptotic expansion balances the major component, and inside there will emerge a divergent integral $\int \frac{dt}{t}$.

Thus the normalisation must have different scaling:
\[\frac{\sum_{k=1}^n X_k}{\sqrt{n\log n}}\sim \mathcal{N}(0,1)\]

The last case is, when the density is asymmetric.
\[p(x) = 1_\obj{\abs{x}\leq 1}\frac{A}{2} + 1_\obj{x>1}\frac{A(1+\beta)}{2\abs{x}^{\alpha+1}} + 1_\obj{x<-1}\frac{A(\beta-1)}{2\abs{x}^{\alpha+1}}\]
where the parameter $\beta\in\clo{0,1}$ is the mass defect.

\begin{align*}
	\phi(s) &= \int e^{-isx}p(x)dx &= \int_\clo{-1,1} e^{-isx}\frac{A}{2}dx + \int_1^\infty e^{-isx}\frac{A(1+\beta)}{x^{\alpha+1}}dx + \int_{-\infty}^{-1} e^{-isx}\frac{A(1-\beta)}{x^{\alpha+1}}dx\\
	&= \frac{A}{2} \int_\clo{0,1} \cos(sx)dx 
	+ \frac{A(1+\beta)}{2}\int_1^\infty \frac{e^{-isx}}{x^{\alpha+1}}dx 
	+ \frac{A(1-\beta)}{2}\int_{-\infty}^{-1} \frac{e^{-isx}}{x^{\alpha+1}}dx\\
	&= \frac{A}{2} \frac{\sin(s)}{s} 
	+ \frac{A(1+\beta)}{2}
	\int_1^\infty \frac{\cos(sy) - i\sin(sy)}{y^{\alpha+1}}dy 
	+ \frac{A(1-\beta)}{2}
	\int_{-\infty}^{-1} \frac{\cos(sx) - i\sin(sx)}{x^{\alpha+1}}dx\\
	&= \frac{A}{2} \frac{\sin(s)}{s} 
	+ A \int_1^\infty \frac{\cos(sy)}{y^{\alpha+1}}dy 
	- i A\beta \int_1^\infty \frac{\sin(sy)}{y^{\alpha+1}}dy
	&=\clo{t = \abs{s}x}\\
	&= \frac{A}{2} \frac{\sin\abs{s}}{\abs{s}} 
	+ A \abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt
	- i A\beta \abs{s} \int_1^\infty \sign(s) \frac{\sin(t)}{t^{\alpha+1}}dt
\end{align*}

THe integral \[I_2(\alpha) = \int_1^\infty \sign(s) \frac{\sin(t)}{t^{\alpha+1}}dt\] converges when $\alpha\in\brac{0,1}$

Therefore 
\[\phi(s) = 1 - AI_1(\alpha)\abs{s}^\alpha \brac{1+i\beta \sign(s) \frac{I_2(\alpha)}{I_1(\alpha)}} + o(s^2)\]

From analysis it is known (?) that 
\[\frac{I_2(\alpha)}{I_1(\alpha)} = \tg\frac{\pi \alpha}{2}\]
whence the limiting distribution has the following characteristic function:
\[\phi(s) = \exp\brac{-\frac{\alpha}{\alpha+1}I_1(\alpha)\abs{s}^\alpha \brac{1+i\beta \sign(s) \tg\frac{\pi \alpha}{2} } }\]

No the case when $\alpha=1$.
\begin{align*}
	\phi(s) &= \int e^{-isx}p(x)dx &= \int_\clo{-1,1} e^{-isx}\frac{A}{2}dx + \int_1^\infty e^{-isx}\frac{A(1+\beta)}{x^{\alpha+1}}dx + \int_{-\infty}^{-1} e^{-isx}\frac{A(1-\beta)}{x^{\alpha+1}}dx
\end{align*}

% see the lecture notes 5.8, 5.9

% for small $s\to 0$
% iA\beta \abs{s}^\alpha\sign(s) \int_{\abs{s}}^\infty \frac{\sin t}{t^2} dt

% \int_{\abs{s}}^\infty \frac{\sin t}{t^2} dt = 
% \int_{\abs{s}}^1 \frac{1}{t} dt + \int_{\abs{s}}^1 \frac{\sin(t)-t}{t^2} dt + \int_1^\infty \frac{\sin t}{t^2} dt
% = iA\beta s\log \abs{s} + \text{const} i s + o(s^2)

Using the convergence of characteristic functions, extracting the divergent integral, regularizing it -- the basic idea (complex in general, simple locally). 

Stability
\[\frac{\sum_{k=1}^n X_k - \mu n}{n^\frac{1}{\alpha}} \sim X_k\]

if $0<\alpha<1$ then ...

% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Mathematical statistics.

Probability theory -- we have some understiandingho

We have an experiment, the data is random, and we attmepto to infer something about the underlying physical phenomenon.

Suppose $\brac{X_k}_{k=1}^n$ is some sample.

Using the strong law of large numbers \[\frac{\sum_{k=1}^n X_k}{n}\overset{\text{a.s}}{\to}\]
Does not tell us anything about the speed of convergence.

The model of the data plays very important role. $X_k \sim C+\xi_i$ with $\xi_i\sim \mathcal{D}(0,1)$ iid.

All probabilistic properties are governevd by the noise term.
The distribution function of the vectore of the noiset rems is gven by 
\[F\brac{y_1, \ldots, y_n;\Theta} \Pr\brac{\xi_k\leq y_k;\Theta} = \prod_{k=1}^n F_{\xi_k}(y_k)\]

THe basic properties of distribution funcutions:
\begin{itemize}
	\item Normalisation: $0\leq F\leq 1$ and $\lim_{x\to -\infty} F(x)=0$ and $\lim_{x\to \infty} F(x)=1$;
	\item Right-continuous non-decreasing map;
	\item It has a density if the derivative exists.
\end{itemize}

Multivariate distribution:
\begin{itemize}
	\item \[p(x,y) = \frac{\partial}{\partial x}\frac{\partial}{\partial y} F(x,y)\]
\end{itemize}

The law of large numbers:
The central limit theorem:

Suppose we have $\brac{\xi_k}_{k\geq1}\sim \mathcal{D}(\mu, \sigma^2)$ iid., Then
\[\sqrt{n}\frac{\frac{1}{n}\sum_{k=1}^n \xi_k - \mu n}{\sigma} \overset{\mathcal{D}}{\to} \mathcal{N}(0,1)\]

\noindent\textbf{The Chebyshev inequality}\hfill\\
Suppose $\xi$ is some random variable, then 
\[\Pr\brac{\abs{\xi-\mu}> \epsilon}\leq \frac{\Var{\xi}}{\epsilon^2}\] 

\noindent\textbf{Chernoff's inequality}\hfill\\
Suppose $\xi$ is a real-valued random variable.
\[\Pr\brac{\xi>\epsilon}\leq \inf_{\lambda>0} \Ex\exp\brac{\lambda\xi - \lambda\epsilon}\]

Since $1_\obj{\xi\>x}\leq e^{\lambda \xi - \lambda x}$ and expectations preserve monotonicity, we have for every $\lambda>0$
\[\Pr\brac{\xi>\epsilon}\leq \Ex\brac{e^{\lambda \xi - \lambda x}}\]
whence the inequality for the infumum over all $\lambda$ follows.

Similarly being a real function of a real value $\xi$ we have the following inequality
\[1_\obj{\xi\>x}\leq \frac{\abs{\xi}^p}{x^p}\]
form which Chebyshev's inequality follows as well (not only the markov's inequality).

\subsection{Random variate generation} % (fold)
\label{sub:random_variate_generation}

Suppose it is necessary to generate a random variable $\theta$ with a known distribution $F$ using the uniform random variable $U\sim\mathcal{U}\clo{0,1}$.

If the function $F$ is invertible ($F$ has no atoms), then $\eta \defn F^{-1}(U)\sim F$.

Indeed, \[\Pr\brac{\eta\leq x} = \Pr\brac{F^{-1}(U)\leq x} = \Pr\brac{U\leq F(x)} = F(x)\]

For example, for $F\sim \text{Exp}(\lambda)$ it is true that $F(x)= 1-e^{-\lambda x}$, whence \[Y \defn -\frac{1}{\lambda}\log{(1-U)}\] has is exponentially distributed. Notice, that $1-U\sim\mathcal{U}\clo{0,1}$, which implies that \[-\frac{1}{\lambda}\log U \sim F\]

The inversion method is not always convenient. Take normally distributed random variables for instance.

% subsection random_variate_generation (end)

\subsection{Box-M\"uller method} % (fold)
\label{sub:box_muller_method}

Suppose we need to generate a pair of normally distributed random variates $X,Y$.
Find the joint distribution of independent $(r,\phi)$ such that the following random vectors have a normal join distribution and are independent
\[\xi_1 = r \cos\phi\,\text{and}\,\xi_2 = r \sin\phi\]

Consider an infinitesimal patch of a plane
\[(\xi_1, \xi_2)\in\clo{r,r+\Delta r}\times \clo{\phi,\phi+\Delta \phi}\sim
\frac{1}{2\pi} e^{-\frac{r^2}{2}} \Delta_r r\sin \Delta_\phi\]
Note that $\sin\Delta_\phi\approx \Delta_\phi$ for an infinitesimal $\Delta_\phi$. At the same time
\[\clo{r,r+\Delta r}\times \clo{\phi,\phi+\Delta \phi} \sim p(r,\phi) \Delta_r\Delta_\phi\] 

Therefore
\[p(r,\phi) = \frac{1}{2\pi} r e^{-\frac{r^2}{2}}\]

Thus the marginal distribution of $\phi$ is uniform on $\ploc{0,2\pi}$, and $F_r(x) = 1 - e^{-\frac{x^2}{2}}$.

% subsection box_muller_method (end)

Suppose we have a sample $\brac{Y_k}_{k=1}^n$ iid with $F(x)$. The empirical distribution function of $Y$ given the sample is 
\[\hat{F}_n(x) \defn \frac{1}{n}\sum_{k=1}^n 1_\ploc{-\infty, x}(Y_k)\]
The expectation of $\hat{F}_n(x)$ for any $x$ is 
\[\Ex \hat{F}_n(x) = \frac{1}{n} n \Ex1_\ploc{-\infty, x}(Y) = \Pr\brac{Y\leq x} = F(x)\]

The variance of $\hat{F}_n(x)$ at any $x$ is given by
\[\Var\brac{\hat{F}_n(x)} = \frac{1}{n^2}n \Var{1_\ploc{-\infty, x}(Y)}\]
where independence assumption has been used.
The variance on an indicator is
\[\Var{1_A(Y)} = \Ex 1_A(Y) - \brac{\Ex 1_A(Y)}^2 = \Pr(Y\in A)\brac{1-\Pr(Y\in A)}\]
Thus \[\Var\brac{\hat{F}_n(x)} = \frac{1}{n} F(x)\brac{1-F(x)}\]

Using the central limit theorem we get
\[\frac{\hat{F}_n(x) - F(x)}{\sqrt{\frac{1}{n} F(x)\brac{1-F(x)}}}\overset{\mathcal{D}}{\to} \mathcal{N}(0,1)\]
The convergence rate is $\frac{1}{\sqrt{n}}$.

There is another way to define the empirical distribution function -- though the order statistics
\[\hat{F}_n(x) = \frac{1}{n}\sum_{k=1}^n 1_\ploc{-\infty,x}(Y_{(k)})\]

\noindent\textbf{Glivenko-Cantelli theorem}\hfill\\
\[\nrm{\hat{F}_n(x) - F(x)}_\infty \overset{\text{a.s}}{\to} 0\]

If $F(x)$ is invertible, $u_k\sim\mathcal{U}\clo{0,1}$ -- iid and $Y_k\sim F$ -- iid, then it is easy to see that 
\[\Pr\brac{\nrm{\hat{F}_n(x) - F(x)}_\infty>z} = \Pr\brac{\sup_{u\in\clo{0,1}}{\hat{U}_n(u) - u}>z}\]
where $\hat{U}_n(u) = \frac{1}{n}\sum_{k=1}^n 1_\ploc{0,F^{-1}(Y_k)}(u)$.

\noindent\textbf{Kolmogorov's theorem}\hfill\\
\[\Pr\brac{\sqrt{n}\nrm{\hat{F}_n(x) - F(x)}_\infty>z} = 2\sum_{k\geq1} {(-1)}^{k-1} e^{-2k^2z^2}\]

\noindent\textbf{Dworezky-Kifer-Wolfowitz theorem}\hfill\\
\[\Pr\brac{\sqrt{n}\nrm{\hat{F}_n(x) - F(x)}_\infty>z} \leq 2 e^{-2z^2}\]

\noindent\textbf{Pike's theorem}\hfill\\
Suppose $\brac{\xi_k}_{k=1}^{n+1}\sim \text{Exp}(1)$.
The distribution of the ordered statistics coincides with 
\[U_{(k)} = \frac{\sum_{i=1}^k\xi_i}{\sum_{i=1}^{n+1}\xi_i}\]

%% See Renyi 1953 On the theory of Order Statistics

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

Suppose $\brac{X_k}_{k=1}^n$ is independent and identically distributed like Cauchy, then $\frac{\sum_{k=1}^n X_k}{n}\sim \text{Cauchy}$ and this property is known as stability.

If $p(x)\sim \frac{1}{\abs{x}^{\alpha+1}}$ for $\alpha\in\brac{0,1}$, then $\frac{\sum_{k=1}^n X_k}{n}\sim n^{\frac{1}{\alpha}-1} X_i$.

This looks very much like the renormalisation transformation in self-similarity studies.

What is the reason for this strange result? The limiting distributions for extreme values.

\subsection{Extreme value distribution} % (fold)
\label{sub:extreme_value_distribution}

Other properties of characteristic functions.

Let $X,Y$ be two random variables with distributions $F$ and $G$ respectively. Then \[\Pr\brac{\max\obj{X,Y}\leq x} = F(x)G(x)\]
since $\obj{\max{X,Y}\leq x} = \obj{X\leq x}\cap \obj{Y\leq x}$ and $X\perp Y$.

This fact can be used for deriving distribution of maxima.

Let's get back to the example of Cauchy distribution. The maximum of two Cauchy distributed random variables is distributed according to
\[F^2(x) = \brac{\frac{1}{2} + \frac{1}{\pi}\arctan x}^2\]
and $F(x)\approx 1 - \frac{1}{\pi x}$.

Indeed 
\begin{align*}
	\tan \xi
	&= \frac{\cos \frac{\pi}{2} - \xi}{\sin \frac{\pi}{2} - \xi}\\
	\tan \xi &\overset{\xi\to \frac{\pi}{2}}{=} \frac{\cos \frac{\pi}{2} - \xi \to }{\sin \frac{\pi}{2} - \xi}\sim \frac{1}{\frac{\pi}{2} - \xi}\\
	x\sim \brac{\pi\brac{1-F(x)}}^{-1} \Rightarrow F(x)\sim 1-\frac{1}{\pi x}
\end{align*}

Therefore the asymptotics of the sum is
\[F_{\sum_{k=1}^n X_i} (x) = F_{nX}(x) = F_X(\frac{x}{n})\approx 1 - \frac{n}{\pi x}\]

The largest sum component dominates the sum, and the normalisation kills off the meagre sum components and retains the scaled ``giant''.

However the maximum has the right tail asymptotically equal to
\[F_{\max_{k=1}^n X_i} (x) = \brac{F_X(x)}^n \approx \brac{1 - \frac{1}{\pi x}}^n \approx 1 - \frac{n}{\pi x}\]

% subsection extreme_value_distribution (end)

\subsection{The Fisher-Tipett-Gnedenko} % (fold)
\label{sub:the_fisher_tipett}

Gnedeko proved the exclusiveness of the limiting distributions.

Reasoning in complete analogy to the limiting stable distributions, one has to decide on the recentering and rescaling.

\noindent \textbf{Definition}\hfill\\
Let $F(x)$ be some distribution function. Define the \emph{typical maximum value} in the sample of size $n$ as the solution to the following equation: \[F(\bar{x}_{(n)}) = 1-\frac{1}{n}\]

The expected number of sample elements larger than $\bar{x}_{(n)}$ is $n\frac{1}{n}$, over the random samples of size $N$ from $F(x)$.

Let's use $X_{(n)}$ to renormalise the maxima.

\noindent\textbf{Theorem}\hfill\\
Suppose $\brac{X_k}_{k=1}^n$ is an independent sample. Denote by $X_{(k)}$ the order statistics of the sample. Recall that for all $k\leq n-1$
\[x_{(k)}\leq X_{(k+1)}\]
The order statistics are obtained via a random permutation and are dependent.

The maximum of a random sample of size $n$ is \[X_{(n)} = \max_k X_k\] and its CDF is $F_{(n)}(x) = F^n(x)$.

Let's looks for the limiting distribution of the form 
\[F_{(n)}(\xi \bar{x}_{(n)}) \overset{n\to \infty}{\to} e^{-\phi(\xi)} \]

This renormalise makes sense if $F(x)<1$ for all $x\in \Real$, for otherwise the limiting distribution is a step function.

It is more practical to consider the complimentary distribution function $\bar{F}(x) = 1-F(x)$.

\begin{align*}
	F_{(n)}(\xi \bar{x}_{(n)}) &= \brac{1-\bar{F}(\xi \bar{x}_{(n)})}^n\\
	n \ln\brac{1-\bar{F}(\xi \bar{x}_{(n)})} \to -\phi(\xi)
\end{align*}

Now if $\bar{F}(\xi \bar{x}_{(n)})\to 0$, then asymptotically 
\[- n \bar{F}(\xi \bar{x}_{(n)}) \to -\phi(\xi)\]
since $\ln (1-\epsilon)\approx -\epsilon$.

On the othe hand, by definition of $\bar{x}_{(n)}$ it is true that $
n\bar{F}(\bar{x}_{(n)}) \equiv 1$. Therefore we look for
\[\lim_{n\to \infty} - \frac{n \bar{F}(\xi \bar{x}_{(n)})}{ n\bar{F}(\bar{x}_{(n)}) }\]

Note that for all $t\in \clo{ \bar{x}_{(n)}\leq \bar{x}_{(n+1)}}$ it is true that 
\[
\frac{\bar{F}(\bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n)}) } \frac{\bar{F}(\xi \bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n)}) } \leq
\frac{\bar{F}(\xi t)}{ \bar{F}(t) } \leq
\frac{\bar{F}(\bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n+1)}) }
\frac{\bar{F}(\xi \bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n+1)}) }
\]

Where $\frac{\bar{F}(\bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n)}) }\to 1$ and $\frac{\bar{F}(\bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n+1)}) }\to 1$.
%% Use limsups and liminfs

Therefore
\[\phi(\xi) = \lim_{n\to\infty} \frac{\bar{F}(\bar{x}_{(n+1)})}{\bar{F}(\bar{x}_{(n+1)}) } = \lim_{t\to\infty} \frac{\bar{F}(t \xi)}{\bar{F}(t) }\]

Next notice that for any $\xi_1, \xi_2\geq 0$ it is true that
\[\phi(\xi_1\xi_2) = \lim_{t\to\infty} \frac{\bar{F}(t \xi_1\xi_2)}{\bar{F}(t\xi_1) } \frac{\bar{F}(t \xi_1)}{\bar{F}(t) }\]
whence \[\phi(\xi_1\xi_2) = \phi(\xi_1)\phi(\xi_2)\]

A continuous positive solution to this functional equation is given by $\phi(\xi) = \xi^{-\alpha}$.
In order for $\phi$ to be a meaningful solution, it must be true that the limiting function $e^{-\phi(\xi)}$ be a distribution function.

This the limiting distribution is
\[G(\xi) = e^{-\xi^{-\alpha}}\]

Therefore the tail of the distribution
\[\bar{F}(\xi) = 1 - F(\xi) = 1-e^{-\xi^{-\alpha}}\approx \xi^{-\approx}\]

The limiting distribution if called Fr\'ech\'et distribution and it emerges when the tail is polynomial and $\bar{x}_{(n)}\sim n^\frac{1}{\alpha}$.

For light tailed distributions this derivation will yield $\alpha=\infty$. This means that it is necessary to change the normalisation for this class of distributions. The scaling is $F_{(n)}(\bar{x}_{(n)} + \eta)$. In this case the limiting exponent satisfies
\[\phi(\eta) =\lim_{t\to\infty} \frac{\bar{F(t\eta)}}{\bar{F(t)}}\]
the functional equation is thus $\phi(\eta_1+\eta_2)=\phi(\eta_1)\phi(\eta_2)$.

Thus the limiting distribution is the Gumbel's distribution
\[F(\eta) = e^{-e^{-\beta\eta}}\]

If the distribution $X\sim F$ has compact support, i.e. $X_i\leq X_*$, then one can consider another random variable $Y_i = \frac{1}{x_*-X_i}$ and reduce this problem a previously solved.

Then for heavy tail $\alpha\in\ploc{0,\infty}$: Weibull's distribution
\[F(\theta) = e^{-{(x_*-\theta)}^\alpha}\]
Since the exponential decay to zero is so fast, that it would yield a step function.

% subsection the_fisher_tipett (end)

% section lecture_6 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

Suppose we have a sample  $\brac{Y_k}_{k=1}^n$ of independent And identically distributed random variables with $F(x)$.

The empirical distribution function is
\[\hat{F}_n(x) = \frac{1}{n}\sum_{k=1}^n 1_{\ploc{-\infty, x}}(Y_k)\]

The divergence of the eCDF from the true CDF with respect to the $
sup$-norm is equivalent in distribution to
\[\Pr\brac{\sup_{x}\nrm{\hat{F}_n(x) - F(x)}_\infty > z} = \Pr\brac{\sup_{u\in\clo{0,1}}\nrm{\hat{U}_n(u) - u}_\infty > z}\]
where 
\[\hat{U}_n(u) = \frac{1}{n}\sum_{k=1}^n 1_{\clo{0, u}}\brac{F^{-1}(Y_k)}\]

Kolmogorov's theorem
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}}\nrm{\hat{U}_n(u) - u}_\infty \leq z} = 2\sum_{k\geq1} {(-1)}^{k-1} e^{-2 k^2 z^2}\]


Pike's theorem
%%%%% Suppose $\brac{\xi_k}_{k=1}^{n+1}\sim \text{Exp}(1)$.
%%%%% The distribution of the ordered statistics coincides with 
%%%%% \[U_{(k)} = \frac{\sum_{i=1}^k\xi_i}{\sum_{i=1}^{n+1}\xi_i}\]


An intermediate result:
\[\lim_{n\to \infty} \Pr\brac{\sqrt{n} \sup_{x}\nrm{\hat{F}_n(x) - F(x)}_\infty > z} = \Pr\brac{\sqrt{n} \abs{W_0(t)} > z}\]

A scalar process on a probability space $\brac{\Omega, \Fcal, \Pr}$ is an $\Fcal$ measurable function $X = X(\omega, t)$ for $\omega\in \Omega$ and $t\in \Tcal$ is a non-random parameter.

A Gaussian random process is a scalar process $X_t$ \begin{itemize}
	\item $\brac{X_{t_k}}_{k=1}^p\sim \Ncal_p\brac{\brac{m(t_i)}_{i=1}^p, \Sigma^p}$ for any $\brac{t_k}_{k=1}^m$ of time slices;
	\item The covariance matrix is given by 
	\[\Sigma^p_{ij} = \Ex\brac{X_{t_i} - m(t_i)}\brac{X_{t_j} - m(t_j)}\]
\end{itemize}

A Wiener process is a Gaussian process $W_t$, $t\geq 0$ with $m(t) = 0$ and $\Ex\brac{W_t W_s} = \min\obj{t,s}$.

The Brownian Bridge is a process defined over $t\in\clo{0,1}$ by
\[W_0(t) \defn W(t) - t W(1)\]

To reiterate:
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}} \nrm{\hat{U}_n(u) - u}_\infty \leq z} \overset{n\to\infty}{\to} \Pr\brac{\sqrt{n} \sup_{u\in \clo{0,1}}\abs{W_0(u)} > z}\]

Consider the ordered statistics $\brac{U_{(k)}}_{k=1}^n$ of a uniformly distributed random variables:
\[\hat{U}_n(U_{(k)}) = \frac{k}{n}\]

Therefore 
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}} \nrm{\hat{U}_n(u) - u}_\infty \leq z}  = \Pr\brac{\sqrt{n} \max_{k=1}^n \abs{U_{(k)} - \frac{k}{n}} \leq z} \]

According to Pike's theorem
\[U_{(k)} = \frac{\sum_{i=1}^k \xi_j}{\sum_{j=1}^{n+1} \xi_j}\]

Thus 
\[\Pr\brac{\sqrt{n} \max_{k=1}^n \abs{U_{(k)} - \frac{k}{n}} \leq z}  = \Pr\brac{\max_{k=1}^n \abs{ n \sum_{i=1}^k \xi_j - k\sum_{j=1}^{n+1} \xi_j} \leq \sqrt{n} z\sum_{j=1}^{n+1} \xi_j } \]

Next
\[ = \Pr\brac{\max_{k=1}^n \abs{ \frac{1}{\sqrt{n}} \sum_{i=1}^k (\xi_j-1) - \frac{k}{n\sqrt{n}} \sum_{j=1}^{n+1} (\xi_j-1) - \frac{1}{n}} \leq \frac{1}{n} z\sum_{j=1}^{n+1} \xi_j } \]

Consider for $\brac{\xi_i}_{i\geq1}$ independent and identically distribution $\xi_i\sim\text{exp}(1)$:
\[W_n(t) = \frac{1}{\sqrt{n}} \sum^{\floor{n t}}_{i=1} (\xi_i - 1)\]

First of all $\Ex W_n(t) = 0$ and the covariance is given by
\[\Ex W_n(t)W_n(s)  = \frac{1}{n} \sum^{\floor{n t}}_{j=1} \sum^{\floor{n s}}_{i=1} (\xi_j - 1) (\xi_i - 1) = \frac{1}{n^2}
\sum^{\floor{n t}\vee \floor{n s}}_{i=1} \Ex {(\xi_i - 1)}^2 = 
\frac{1}{n} \floor{n t}\vee \floor{n s} = \floor{t}\vee \floor{s} + o(\frac{1}{n})\]

Thus from the Central Limit Theorem on $t\in 
clo{0,1}$
\[W_n(t)\overset{\Dcal}{\to} W(t)\,\text{and}\, \frac{1}{\sqrt{n}} \sum_{i=1}^k (\xi_j-1) - \frac{t}{n\sqrt{n}} \sum_{j=1}^{n+1} (\xi_j-1) \to W(t)-tW(1)\]

Since 
\[\frac{1}{n} z\sum_{j=1}^{n+1} \xi_j \overset{\Pr}{\to} z\]

Thus the theorem is proved.


\subsection{Extreme Value Distribution} % (fold)
\label{sub:extreme_value_distribution}

A sample $\brac{Y_k}_{k=1}^n$ of independent and identically distributed random variables with $F(x)$. The distribution of 
\[\Pr\brac{\max_{i=1}^n Y_i \leq x} = \prod_{i=1}^n \Pr\brac{Y_i \leq x} = F^n(x)\]

There are three types of limitnig distributions of maxima (properly scaled and centred) \begin{description}
	\item[Fr\'ech\'et]\hfill\\
	\[ e^{-x^{-\alpha}} 1_{\clop{0,\infty}}(x) \]
	\item[Gumbel]\hfill\\
	\[ e^{-e^{-x}} 1_{\clop{0,\infty}}(x) \]
	\item[Weibull (Wallodi)]\hfill\\
	\[ e^{-{(-x)}^\alpha} 1_{\brac{-\infty, 0}}(x) \]
\end{description}


The normal distribution is 
\[F(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-\frac{u^2}{2}}du\]

recall the inversion method $Y_{(n)} = F^{-1}(U_{(n)})$.

Now using Pike's theorem
\[U_{(n)} = 1-\frac{ \xi_{n+1}}{\sum_{j=1}^{n+1} \xi_j}\]

Next
\[U_{(n)} = 1-\frac{ \xi_{n+1}}{n+1}\brac{1 + \frac{1}{n+1}\sum_{j=1}^{n+1} {(\xi_j-1)}}^{-1}\]

Using the CLT we get the following
\[U_{(n)} \overset{\Dcal}{\approx} 1-\frac{\xi_{n+1}}{n+1}\brac{1+o(\frac{1}{\sqrt{n+1}})}\]

So $1-U_{(n)}$ decays as $n\to \infty$, thus we apply the gaussian inversion to it.

Integrating the Gaussian distribution by parts yields
\[\int_x^\infty \frac{1}{u} de^{-\frac{u^2}{2}} =  \induc{ \frac{1}{u} e^{-\frac{u^2}{2}} }_x^\infty - \int_x^\infty \frac{1}{u^2} e^{-\frac{u^2}{2}}du\]

Then 
\[= - \frac{1}{x} e^{-\frac{x^2}{2}}
+ \int_x^\infty \frac{1}{u^3} de^{-\frac{u^2}{2}} 
= - e^{-\frac{x^2}{2}} \brac{\frac{1}{x} + o(\frac{1}{x^3})}\]

Thus
\[F(x) = 1 - \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \brac{\frac{1}{x} + o(\frac{1}{x^3})}\]

let's invert this for values of $y$ close to zero.

\[ x = F^{-1}(y) = \sqrt{ 2\log{\frac{1}{x}+o(\frac{1}{x^3})} + \log \frac{1}{1-y}\frac{1}{\sqrt{2\pi}}} = L_y(x) \]

Then 
\[L_y'(x) \leq - \frac{L_y(x)}{2x}\]

whence an approximate solution of the inversion is
\[ x\approx \sqrt{-\log{(1-y)2\sqrt{\pi}}}\]

Therefore
\[Y_{(n)} = F^{-1}(U_{(n)}) \approx F^{-1}\brac{1-\frac{\xi_{n+1}}{n+1}\brac{1+o(\frac{1}{\sqrt{n+1}})}}\]

whence 
\[Y_{(n)} \approx \sqrt{ \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } } \]

Using the first approximation of $x$ into $L_y(x)$ we can obtain a more precise approximation to $x$ and thus the final distribution:

\[ Y_{(n)} \approx \sqrt{ \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } - 2\log \log \frac{{(n+1)}^2}{\xi_{n+1}^2 \sqrt{2\pi} }  + o(1) }  \]

Setting 
\[m_n^2 \defn \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } \]

we get 
\[Y_{(n)} \approx \sqrt{ m_n^2 - \frac{2\log \xi_{n+1}}{m_n^2} }\]

Using Taylor's expansion (? STRANGE)
\[Y_{(n)} \approx m_n\brac{ 1 - \frac{2\log \xi_{n+1}}{m_n} }\]

Thus 
\[\Pr\brac{ Y_{(n)} < z } \approx \Pr\brac{ m_n\brac{ 1 - \frac{2\log \xi_{n+1}}{m_n} } < z } \]


\textbf{CHECK THE CONSTANTS!!!!!}

Redoing this: the first approximation id $x$ is
\[x_0 \approx \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} }\]

Then 
\[x = \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} + \log\brac{ \frac{1}{x} + o(\frac{1}{x^3}) }}\]

Substituting $x_0$ instead of $x$ and neglecting the infinitesimal yields:
\[ x \approx \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} - \frac{1}{2} \log\log\frac{1}{(1-y)\sqrt{2\pi}} }\]

Then 
\[ Y_{(n)}^2 \approx \brac{
\log\frac{n+1}{\xi_{n+1}\sqrt{2\pi}}
- \frac{1}{2} \log\log\frac{n+1}{\xi_{n+1}\sqrt{2\pi} + o(1)}
}\]

So
\[ Y_{(n)} \approx \sqrt{ - \log \xi_{n+1} + \log\frac{n+1}{2\log{(n+1)} \sqrt{8\pi} + o(1) } }\]

Whence
\[ Y_{(n)} \approx \brac{-\log \xi_{n+1} + m_n + o(1) }^\frac{1}{2}\]

Thus using the Taylor once again
\[ Y_{(n)} \approx m_n^\frac{1}{2} \brac{-\frac{\log \xi_{n+1}}{m_n} + 1 }\]

Next 
\[ \sqrt{m_n} Y_{(n)} - m_n\approx - \log \xi_{n+1}\]

Thus
\[ \Pr\brac{\sqrt{m_n} \brac{Y_{(n)} - \sqrt{m_n}}} \approx \Pr\brac{- \log \xi_{n+1} < z }\]

So
\[\Pr\brac{- \log \xi_{n+1} < z } = \Pr\brac{\xi_{n+1} > e^{-z} } = e^{-e^{-z}}\]

And $m_n\approx \sqrt{ 2 \log n}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Laplace distribution
\[F(x) = 1-e^{-\frac{x}{2}} 1_{x\geq 0} + e^{\frac{x}{2}} 1_{x\leq 0}\]

The inverse is given for $y\in clo{\frac{1}{2},1}$ by
\[ F^{-1}(y) = - \log 2(1-y)\]

Since $\max_{i=1}^n Y_i \sim \log\frac{n+1}{2\xi_{n+1}}$ 
we have
\[\Pr\brac{ \log\frac{n+1}{2} - \log \xi_{n+1} < z } \approx e^{-e^{-z}}\]

% subsection extreme_value_distribution (end)

% section lecture_7 (end)

\end{document}
