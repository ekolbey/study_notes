\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Rbar}{{\bar{\mathbb{R}}}}
\newcommand{\Cplx}{\mathbb{C}}

\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\borel}{\mathcal{B}}

\newcommand{\Ex}{\mathbb{E}}
\newcommand{\pr}{\mathbb{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}
\newcommand{\sign}{\mathop{\text{sgn}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Probabilistic methods in modelling}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

Theory of Stochastic processes
Brownian motion
Martingales
Poisson point processes

Generalizationo of the central limit theorem:
-- normal distribution
-- stable processes
-- Fisher-Tipett-Gendenko thorem for maxima

Based on the law of large numbers:
-- Extreme value theory

Additional chapters of probability

Elementary probability: Kolmogorov axioms, Measure theoretic approach.
Bernoulli law or large numbers.
Stochastic processes require measure theoretic foundations for their definition on some abstract space.

%% \selectlanguage{russian}
\section{Lecutre \#1} % (fold)
\label{sec:lecutre_1}


Suppose $\brac{X_k}_{k=1}^n$ is a finite collection of random variables which are independent and identically distributed.

What is the asymptotics of some function of $\brac{X_k}_{k=1}^n$. The probability distribution 

Consider a measure space $(\Omega, \Fcal, P)$ and an RV $X(\omega)$ from $\Omega$ to $(\mathcal{X}, \Sigma)$ on it.
This induces a measure in $\mathcal{X}$ -- the image space. see image~1.
\[\mathbb{P}_X \defn X_\# \mathbb{P} = \mathbb{P}\brac{X\in A} = \int 1_{X^{-1}(A)}d\mathbb{P}\]

For limiting theorems the image space is sufficient.

Abstract spaces are needed in stochastic processes (and with filtration).

Convergence in distribution is needed in limiting theorems.

Consider $X\in \Real$.
How is the distribution of $X$ defined?
It is defined using the semi-ring of half-closed intervals $\ploc{a,b}$.
Or, the most popular method -- Cumulative Distribution function.

An RV is not a measurable map, but a measure representing its probability.
A map $F:\Real\to \clo{0,+\infty}$ is a distribution function if \begin{enumerate}
	\item $F$ is non decreasing;
	\item $F(-\infty) = 0$ and $F(+\infty) = 1$.
	\item $F$ is right-continuous and bounded;
\end{enumerate}
in short $F$ must be c\'adl\'ag. The crucial thing is $F(\Real)=1$.

Modern statistical physics was founded on Gibbs's idea.
A physical system might be in different states, thus Gibbs and Bolzmann postulated that each state has some likelihood.

What is the likelihood of picking an even number in $\mathbb{Z}$? Suppose there is some probability measure on $\mathbb{Z}$ given by $\brac{p_n}_{n\in \mathbb{Z}}$ 
\[\sum_{n\text{ div } 2} p_n\]

Let $A\subseteq \mathbb{Z}$ then the density of $A$ is \[\rho(A) \defn \lim_{n\to+\infty, m\to -\infty} \frac{\abs{\obj{\induc{k\in \mathbb{Z}}\,n\leq k\leq m }}}{n+m+1}\]
However $\rho(\cdot)$ fails to be countably sub-additive.

Classification of distribution functions on $\Real$. \begin{description}
	\item[Atomic:] \hfill \\
		there is a countable and measurable subset $A\in \borel(\Real)$ and a collection $\brac{p_k}_{k\in A}$ such that
			\[F(x) = \sum_{k\in A} p_k 1_{\obj{k}}(x)\]
	\item[Singular:] \hfill \\
		all the rest. 
	\item[Absolutely continuous:] \hfill \\
		there exists a lebesgue-measurable map $p:\Real\to\Real$ with $F(x)=\int p(x) dx$
\end{description}

Lebesgue theorem: every distribution function can be represented as a weighted sum of three basic types of distributions.


Classification of local behaviour of distributions in $\Real$.

Consider some $x_0\in \Real$ and consider the difference $F(x_0+\Delta)-F(x_0-\Delta)$ for $\Delta > 0$.
The function $F$ has the singularity order $\alpha$ if \[F(x_0+\Delta)-F(x_0-\Delta) = \Omega(\Delta^\alpha)\]

\begin{description}
	\item[Atomic:] $\alpha = 0$;
	\item[Continuous:] $\alpha = 1$;
\end{description}

If $F(x)=\min\obj{\sqrt{x}, 1} 1_{\clop{0,+\infty}}(x)$ then $0$ has singularity order $\frac{1}{2}$.

Cantor distribution has uncountably many points with $\alpha = 0$, and uncountably many points with $\alpha = \frac{\log 2}{\log 3}$.
A good idea is to represent any $x\in \clo{0,1}$ in base-3 representation. The ``interesting'' points have either $0$ or $2$ in their base-3 form.

\noindent\textbf{Problem \#1}\hfill \\
	Given $U\sim \mathcal{U}\clo{0,1}$ and $Y=f(U)$. Find $f$ such that the density $p_Y$ in given by
	\[\lambda e^{-\lambda y} 1_{\clop{0,+\infty}}\]
	If $F(x)$ is absolutely continuous and $X\sim F$, then $F(X)\sim \mathcal{U}\clo{0,1}$. (Skorokhod)

\noindent\textbf{Problem \#2}\hfill \\
	Suppose $(X,Y)$ is uniformly distributed inside the unit circle. Let $\rho \defn \sqrt{X^2+Y^2}$. What is the distribution of $(x',y')$ with the radius given by $r = \sqrt{-\log \rho}$.
	Using the Jacobian transformation, the distribution of the transformed pair is jointly gaussian.

% section lecutre_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

A shorter lecture, no definition of expectation, but full of functions:
\begin{itemize}
	\item Probability generating functions;
	\item Moment generating functions;
	\item Characteristic functions.
\end{itemize}

Suppose $X$ is a real-valued random variable with law $F_X$. If $F_X$ is absolutely continuous with respect to $dx$, then $F_X(x) = \int_{-\infty}^x f dx$.

The expectation of $X$ is defined as \[\Ex(X) \defn \int X d{F_X}\] and in the case of absolutely continuous distribution it is true that \[\int X d{F_X} = \int X f(x) dx\]

Riemann-Stieltjes integrals are used (but those who know, can use a better Lebesgue-Stieltjes integral).
\[\int d dF = \lim_{\Delta\to 0} \sum_i g(x_i) \brac{F(\xi_{i+1}) - F(\xi_i)}\]
Lebesgue integral is pedagogically more difficult, though more abstract, flexible and overall better.
% Geometric measure theory.

Cauchy distribution does not allow $\Ex X$, however it has the mean in terms of the \textbf{main value} (integration over a symmetric interval $\clo{-M,+M}$).
% \rus{Уравнение Власова}

The expectation of a function $g$ of $X$ is $\Ex\brac{g(X)} = \int g(X) dF_X$.

The $k$-th moment of an RV is $\Ex X^k$ and the $k$-th central moment is $\Ex\brac{ X - \Ex X}^2$.
The second central moment of $X$ is the variance of $X$ : $\mathbb{D}(X) \defn \Ex\brac{ X - \Ex X}^2$.

Jensen's inequality. For any convex function $g$ of $X$ it is true that 
\[g\brac{\Ex X}\leq \Ex g(X) \]

Fatou's lemma \[\int \liminf X_n d\mu \leq \liminf \int X_n d\mu\]

Holder's inequality for nonnegative measurable $f$ and $g$ \[\int fg d\mu \leq \brac{\int f^p d\mu}^\frac{1}{p}\brac{\int g^q d\mu}^\frac{1}{q}\] whenever $\sfrac{1}{p}+\sfrac{1}{q}=1$.

Minkowski inequality \[\brac{\int {(f+g)}^p d\mu}^\frac{1}{p}\leq \brac{\int f^p d\mu}^\frac{1}{p} + \brac{\int g^p d\mu}^\frac{1}{p}\]

The probability generating function is defined for any $z\in \Cplx$ with $\abs{z}\leq 1$ as \[G_X(z) \defn \Ex z^X\]
If $X$ has discrete distribution, then $G_X(z) = \sum_{n\geq 0} z^n \Pr(X=n)$.
Since the series is analytical in a finite disc, it is differentiable.
Thus \[G_X^{(k)}(z) = \sum_{n\geq 0} \frac{n!}{(n-k)!} z^{n-k} \Pr(X=n) \]
whence $G_X^{(k)}(0) = \Ex \brac{X(X-1)\ldots (X-k+1)}$

Factorial exponent: $x^{\underline{k}} = \prod_{s=1}^k (X-s+1)$.
\[x^{\underline{k}} - \brac{x-1}^{\underline{k}} = k \cdot \brac{x-1}^{\underline{k-1}} \]

The moment generating function of $X$ is given by \[S_X(t) \defn G(e^t) = \Ex(e^{tX})\]
Differentiation of the MGF and evaluation at $t=0$ yields moments of $X$.
Indeed, if the integral converges uniformly, then $S_X^{(k)}(t) = \induc{\Ex X^k e^{tX}}_{t=0} = \Ex X^k$.

\emph{A gentle reminder}\hfill \\ For any pair of independent random variables the expectation of their product equals the product of their expectations.

Thus the mgf of a sum of independent $X,Y$ results in \[S_{X+Y}(t) = \Ex e^{t(X+Y)} = \Ex e^{tX} e^{tY} = \Ex e^{tX} \Ex e^{tY} = S_X(t) S_Y(t)\]

The cumulants of $X$ (or semiinvariants) are defined as the coefficients in the following formal series expansion of $\log S_X(t)$ as
\[\log S_X(t) \defn \sum_{n\geq 0} \frac{t^n}{n!} \kappa_n^X\]

Using the formal differentiation of the series with respect to $t$ yields
$\kappa_n^X = \induc{\frac{d^n}{dt^n} \log S_X(t)}_{t=0}$.

Since $S_X(t)$ is additive with respect to $X$ if the summands are independent,
Kurtosis, \rus{эксцесс} -- measures how far the probability is shifted to the tails.

\textbf{Martsinkevich's theorem}\hfill\\
$\log S_X(t)$ is a polynomial if and only if $X\sim\mathcal{N}(\mu,\sigma)$.


Solve exercises 1.8, 1.11, 1.12 and 1.13 on pp.~42-43.

% section lecture_2 (end)

% Task 1.8
Suppose $X$ takes $k$ distinct values at random. 



The number of partitions of a set of $n$ elements into $k$ classes is given by the Stirling Number of the second kind.
The $n$-th element can either form an isolated class in $P(n-1,k-1)$different ways,
or be a member of one of the existing $k$ classes in $P(n-1,k)$ number of ways.
Therefore \[P(n,k) = P(n-1,k-1) + k P(n-1,k)\]


% Task 1.13 -- wrong!!!!
Suppose each node spawns an $m$-tree and that each edge in any tree in impassable with probability $p$.
What is the probability $\pi_n$ of a tree with $n$ layers such that there is no paths from the root to leaves.

Since the tree is recursive and independent the following is valid:
\[\pi_{n+1} = (1 - p^m) \pi_n\]

The exact moment when the main gates closed created a root in the tree structure of chambers.


\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

%% Missed the first part of the lecture
Tracey-Widam distribution

\textbf{T}otally \textbf{A}symmatric \textbf{S}imple \textbf{E}xclusion \textbf{P}rocess: when a particles on a lattice can jump and shift to the right, but not over another particle.

% Random matrix theory -- Ivan Corvin

Asymptotic (high dimensional) representation theory:
KPZ scaling and Tracey-Widam

The (weak) Law of Large Numbers:
\[\pr\brac{\abs{ \frac{\sum_{k=1}^n X_k}{n}-\mu }>\epsilon} \to 0\]


\noindent \textbf{Definition} of weak convergence. \hfill\\
A sequence of random variables $\brac{X_n}_{n\geq1}$ converges weakly to $X$, or in distribution, if for all $a,b\in \Rbar$, such that $a,b$ are atoms, probability concentrations:
\[\pr\brac{a<X_n<b}\to \pr\brac{a<X<b}\]

\noindent \textbf{Proposition} 1\hfill \\
$X_k\overset{D}{\to}X$ if and only if $F_{X_n}\to F_X$ at any point of continuity of $F$.

Continuity is required, because the CDf is c\'adlag (only right continuous).

Let $\brac{x_k}_{k\geq0}$ be a monotonous deterministic sequence, which converges to $\mu$.
If $x_k\uparrow$, then CDFs of a deterministic sequence do converge to the CDF of their limit. And not otherwise.

\noindent \textbf{Proposition} 1\hfill \\
$X_k\overset{D}{\to}X$ if and only if $F_{X_n}\to F_X$ at any point of continuity of $F$.

\noindent \textbf{Proposition} 2 (Helly)\hfill \\
Every sequence of distribution functions $\brac{F_n}_{n\geq1}$ it is possible to select a subsequence, such that $F_{n_k}$ converges to some function $F$ at every point of continuity of $F$ and such that $F$ is monotonous.

Note that the function $F$ is non-normalised.

Sketch:
Use the rational numbers $\mathbb{Q}$ as the \textbf{knots} of convergence of $F$. Enumerate them and choose the ``diagonal''.

Proof. Suppose $\brac{q_n}_{n\geq 1}$ enumerates $\mathbb{Q}$.
The sequence $\brac{F_n(q_1)}_{n\geq1}$ is bounded, hence it has a convergent subsequence, whence exists $\brac{k_n}\uparrow\infty$ such that $F_{k_n}(q_1)$ converges to some $F(q_1)$.
Consider $\brac{F^1_n}_{n\geq 1}\defn\brac{F_{k_n}}_{n\geq1}$ as a new sequence, and apply the same argument to it recursively.

Finally we get a sequence $\brac{F^m_n}_{n\geq1}$ such that $F^m_n(q_l)$ converges to $F(q_l)$ for all $l\leq m$ as $n\to \infty$, either by being a subsequence or by the choice of the subsequence $n_k$.

Use Cantor's diagonal argument: form a new sequence $\brac{F^n_n}_{n\geq1}$. This sequence converges at any $q\in\mathbb{Q}$ to $F(q)$.

Now to prove that $F$ is continuous.

If $\lim_{q\uparrow x}F(q) = \lim_{p\downarrow x}F(p)$, then set $F(x)$ to the limit.

The function then becomes either continuous, or left limited and right continuous, since $F_n$ are monotonous and right-continuous.

Counterexample:
Suppose $X_n=n$. Then $F_n(x) = 1_{\ploc{-\infty, x}}(n)$.

In order to get a requirement in Helly's theorem, which guarantees that the limiting function is a distribution function, one has to require the following ``tightness'' condition (uniform property -- nor all $n$):
\[\forall \epsilon>0\,\exists L>0\,\text{s.t.}\,\sup_{n\geq1}\pr\brac{\abs{X_n}>L}\leq \epsilon\]

% See the photographs of the explanation of the method of closed loops.

% Алгебра -- рай, а недифференцируемость -- нечистая чила

\noindent \textbf{The main technical result}\hfill\\

The sequence of distribution function $F_n$ converges weakly to some $F$ if $\phi_n\to \phi$, their associated characteristic functions converge, and $\phi$ is continuous at $0$.

\noindent \textbf{Proposition}\hfill\\

Suppose $X_n\to X$ and $\phi_n(s)\defn \Ex\brac{e^{isX_n}}$. Then there exists a limiting function of $\phi_n$ such that $\phi_n\to \phi = \Ex\brac{e^{isX}}$.

Let's show that if $X_n\overset{D}{\to} X$ then
\[\int g(X) dF_n \to \int g(X)dF\]
for any bounded continuous function $g$.

\textbf{Proof}\hfill\\
Split the integration region into two regions:
\[\int f dF_n = \int_\obj{\abs{X}>R} g dF_n + \int_\obj{\abs{X}\leq R} g dF_n\]

The tightness condition implies that there exists $R$ such that
\[F_n\brac{\obj{\abs{X}>R}}< \frac{\epsilon}{2}\]
which means that $\int_\obj{\abs{X}>R} g dF_n$ is bounded above by $M\frac{\epsilon}{2}$ where $\abs{g}\leq M$.

Since $g$ is continuous on a compact $\obj{\abs{X}\leq R}$, for $\epsilon>0$ there exists $\delta>0$ such that on each $\delta$-range the change of $g$ is not greater that $\epsilon$.
therefore 
\[\int_\obj{\abs{X}\leq R} g dF_n = \int_\obj{\abs{X}\leq R} (g-g^\epsilon) dF_n + \int_\obj{\abs{X}\leq R} g^\epsilon dF_n \leq F_n\brac{\obj{\abs{X}\leq R}}\frac{\epsilon}{2} + \int_\obj{\abs{X}\leq R} g^\epsilon dF_n\]

where $g^\epsilon$ is an $\epsilon$ step function.

However
\[\int_\obj{\abs{X}\leq R} g^\epsilon dF_n\]
is a finite sum $\sum_i g^\epsilon$

Therefore if $X_n\overset{D}{\to} X$ then characteristic function s converge.


% Feller -- Probability: a neat proof

\noindent\textbf{Proposition} \hfill\\
If
\[\phi(s) = \int e^{isx} dF(x)\]
then for any $b>0$ the following equivalence is true for all $y$:
\[\frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds
=\frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF(x)\]
for a gaussian smothing kernel $e^{-\frac{{(x-y)}^2}{2\delta^2}}$.

Indeed, \begin{align*}
	e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} &= e^{-isy}\int e^{isx} e^{-\frac{\delta^2 s^2}{2}} dF(x)\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int e^{-isy}\int e^{isx} e^{-\frac{\delta^2 s^2}{2}} dF(x) ds\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int \int e^{is(x-y)-\frac{\delta^2 s^2}{2}} dF(x) ds\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int \int e^{-\frac{{(x-y)}^2}{2\delta^2}} e^{\frac{\delta^2}{2}\brac{-i\frac{x-y}{\delta^2}+s}^2} dF(x) ds\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int e^{-\frac{{(x-y)}^2}{2\delta^2}} \int e^{\frac{\delta^2}{2}\brac{-i\frac{x-y}{\delta^2}+s}^2} ds dF(x)\\
	\frac{1}{2\pi}\int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds &= \frac{1}{2\pi}\int e^{-\frac{{(x-y)}^2}{2\delta^2}} \frac{\sqrt{2\pi}}{\delta} dF(x)\\
% Missed some details...
\end{align*}


Another lemma:
If the characteristic functions of different distributions are never identical.

Indeed, if characteristic functions coincide, but but distributions differ at least in some point, then there is a contradiction

Suppose $\phi_n(x) = \Ex\brac{e^{isX_n}}$ converges to some $\phi$ -- and $\phi$ is continuous at $x=0$, then there exists a random variable, such that $\phi(s) = \Ex\brac{e^{isX}}$ and $X_n\to X$ weakly.

SupposE $Y_k$ are iid and $X_n = \sum_{k=1}^n Y_k$ with $\Ex\brac{e^{isY}} = \psi(s) = e^{-\abs{s}}$.

Then $\phi_n(s) = \Ex\brac{e^{isX}} = \brac{\phi(s)^n}$, but it converges to $1_{s=0}$.

% the probability is smeared along the infinity/

\begin{align*}
	\frac{1}{2\pi}\int e^{-sy} \phi_n(s) e^{-\frac{\delta^2 s^2}{2}}ds
	& = \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF_n(x)
\end{align*}

Helly's theorem implies that there is a convergent subsequence $F_{n_k}$ that converges to some $F$.

Then analogously to a previously stated proposition
\[\frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF_n(x) \to \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF\]

Using Lebesgue's Dominated convergence theorem it is true that
\[\frac{1}{2\pi}\int e^{-sy} \phi_{n_k}(s) e^{-\frac{\delta^2 s^2}{2}}ds \to \frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds\]

Therefore
\[\frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds = \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF\]
Since $F$ is an arbitrary limiting function, this implies that any other convergent subsequence converges to the same $F$, whence $F_n\to F$.

To show that $F$ is a CDF we do the following: think of letting $\delta\to \infty$ and fixing $y=0$. Then
\[\frac{\delta^2}{\sqrt{2\pi}}\int \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds = \int e^{-\frac{x^2}{2\delta^2}}dF \leq \int dF\]

However as $\delta\to\infty$ it is true that
\[\int \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds\to \phi(0) = 1\]
since $\phi(0)$ is continuous at $0$.

Thus $1 \leq \int dF$. But $\int dF$ cannot strictly exceed $1$, whence $F$ is a distribution function.

\noindent\textbf{Lyapunov's Central Limit Theorem} \hfill\\
Suppose $X_k$ are i.i.d. with $\Ex X_k = \mu$ and $\Ex X_k^2 = \sigma^2$. Introduce the following sequence:
\[Y_n = \frac{\sum_{k=1}^n X_k - n \mu }{\sqrt{n}}\]
Then $Y_n\overset{D}{\to}\mathcal{N}(0,1)$

\textbf{Proff}\hfill\\

If $Z_n = \sum_{k=1}^n X_k - n\mu$. Then $\Ex Z_n = 0$ and $\text{var} Z_n = n \sigma^2$ due to independence.

Let $\phi_n(s) = \Ex\brac{e^{isZ_n}}$. Then
\[\phi_n(s) = 1 + 0 - \frac{n\sigma^2}{2} s^2 + o(s^2)\]

Now
\[\phi_{Y_n}(s) = \Ex\brac{e^{is\frac{Z_n}{\sigma \sqrt{n}}}} = \phi_n(\frac{s}{\sigma\sqrt{n}})\]
whence
\[\phi_{Y_n}(s) = 1 - \frac{s^2}{2} + o(s^2)\]


Another way: due to independence of $X_k$ and using the product-independence property of characteristic function
\[\phi_n(s) = \brac{\phi_{X-\mu}(s)}^n = \brac{ 1 - \frac{n\sigma^2}{2} s^2 + o(s^2) }^n\]

Therefore
\[\phi_{Y_n}(s) = \phi_n(\frac{s}{\sqrt{n}\sigma}) = \brac{ 1 - \frac{s^2}{2n} + o(s^2) }^n\]
 where $o(\cdot)$ is uniformly infinitesimal with respect to $n$.

Using a well known limit we get the following limit:
\[\phi_{Y_n}(s) \to e^{-\frac{s^2}{2}}\]

an example:
Suppose $\brac{X_n}_{n\geq1}$ are independent identically distributed such that $\phi_X(s) = 1+i\mu s + o(s)$, where $o(\cdot)$ is an infinitesimal.

Let \[Y_n = \frac{\sum_{k=1}^n X_k - n \mu }{n}\]
then
\[\phi_{Y_n}(s) = \phi_{\sum}(\frac{s}{n})
= \brac{\phi_X(\frac{s}{n})}^n = \brac{1+i\mu s + o(s)}^n
\to e^{is\mu}\]

The necessary and sufficient condition for $\phi(s) = 1+i s\mu+o(s)$ is \[\lim_{R\to\infty} R\cdot \brac{F_X(-R) + 1 - F_X(R)} = 0\]
% Khinchine's law of large numbers.

% Tutorials
% Homework У2.1, У2.6, У2.7, У2.8, У2.9, У3.9, У3.3, У3.6

% section lecture_3 (end)

\section{lecture \# 4} % (fold)
\label{sec:lecture_4}

Gamma distribution.
\[\gamma(k,\beta) \defn \frac{\beta^k}{\Gamma(k)} s^{k-1} e^{-\beta s}\]

It has all the moments, since the exponent decays very rapidly, faster than any polynomial.

Some exampls of the $\gamma$ distribution are \begin{description}
	\item[$\chi^2_k$] A $\chi^2$ random variable is actually $\gamma\brac{\frac{k}{2},\frac{1}{2}}$;
	\item[$\text{Exp}(\lambda)$] An exponential distribution is a particular case of the Gamma distribution: $\text{Exp}(\lambda) \sim \gamma(1,\lambda)$.
\end{description}

This distribution is asymmetric.
\[\frac{\sum_{k=1}^n G_k- }{}\]

Limiting behaviour of products of random variables.

Suppose $\brac{X_k}_{k=1}^n$ are non-negative random variables. Consider the asymptotics of \[P_n \defn \brac{e^{-\mu n}\prod_{k=1}^n G_k}^{\sqrt{n}}\]

The natural approach is to use $Y_k\defn \log X_k$ and consider the asymptotics of the logarithm of the product: the normalised sum
\[S_n \defn \log P_n \frac{\sum_{k=1}^n Y_k - \mu n}{\sqrt{n}}\]

Since this converges in distribution to a standard normal random variable: $S_n \overset{\mathcal{D}}{\to} \mathcal{N}(0,1)$. Thus the products converge in distribution to a log-normal random variable:
\[\frac{1}{\sqrt{2\pi\sigma^2}} e^\frac{-\log^2 x}{2\sigma^2}\frac{1}{x}\]

What happens to the central limit theorem, when its assumptions fail.
\begin{description}
	\item[Independence] Stochastic processes CLT;
	\item[Identical distribution] The most useful generalisation of the CLT;
	\item[Moments] What if $\Ex(X_k) = \mu$ and $\text{Var}(X_k) = \sigma^2$ are violated?
\end{description}

Consider the Cauchy distributed random variable. IS $\brac{X_n}_{n\geq1}\sim \text{Cauchy}$, then their normalised sum is still Cauchy (consider the characteristic function).

What precludes a distribution from having finite variance?

The main moment is the computation of the asymptotics at zero of the characteristic function (the Fourier transform of the distribution).

Consider the following a bit contrived density:
\[p(x) = 1_\obj{\abs{x}\leq 1}\frac{A}{2} + 1_\obj{\abs{x}>1}\frac{A}{2\abs{x}^{\alpha+1}}\]
where $A=\frac{\alpha}{\alpha+1}$ and $\alpha$ - the Levy exponent. The exponent characterises asymptotics convergences of the CDF to the horizontal asymptotes: $\abs{x}^{-\alpha}$. In the limit we get the Pareto-Levy family of distributions.

Let's use the characteristic functions.
\[\phi(s)\defn\int e^{-isx}p(x)dx\]
Consider the asymptotics near zero (the problems at infinity i time domain transform to the problems near zero in the frequency domain).

Since $p(x)$ is symmetrical
\begin{align*}
	\phi(s) &= \Ex\brac{e^{-isX} } = \int e^{-isx}p(x)dx \\
	&= \int_{-1}^1 \frac{A}{2} e^{-isx} dx + \brac{\int_1^\infty + \int_{-\infty}^{-1}} \frac{A}{2} \frac{e^{-isx} }{\abs{x}^{\alpha+1}} dx
\end{align*}

Now the first integral is just \begin{align*}
	\int_{-1}^1 e^{-isx} dx &= \int_{-1}^1 \cos{sx} -i\sin{sx} dx \\ 
	&= \int_{-1}^1 \cos{sx} dx - i \int_{-1}^1 \sin{sx} dx \\
	& =  \induc{\frac{\sin{sx}}{s}}_{-1}^1 - i \induc{\frac{-\cos{sx}}{s}}_{-1}^1\\
	& = \frac{\sin{s}}{s}-\frac{\sin{-s}}{s} = 2 \frac{\sin{s}}{s} = 2 \frac{\sin\abs{s}}{\abs{s}}
\end{align*}
while the second is a little bit more intricate:
\begin{align*}
	\brac{\int_1^\infty + \int_{-\infty}^{-1}} \frac{e^{-isx} }{\abs{x}^{\alpha+1}} dx
	&= \int_1^\infty \frac{\cos{sx}-i\sin{sx}}{x^{\alpha+1}} dx + \int_{-\infty}^{-1} \frac{\cos{sx}-i\sin{sx}}{\brac{-x}^{\alpha+1}} dx \\
	&= \int_1^\infty \frac{\cos{sx}-i\sin{sx}}{x^{\alpha+1}} dx + \int_1^\infty \frac{\cos(-sy)-i\sin(-sy)}{y^{\alpha+1}} dy \\
	&= \int_1^\infty \frac{2\cos{sx}}{x^{\alpha+1}} dx = 2\int_1^\infty \frac{\cos{\abs{s}x}}{x^{\alpha+1}} dx \\
	& = 2\abs{s}^\alpha \int_1^\infty \frac{\cos{\abs{s}x}}{\brac{\abs{s}x}^{\alpha+1}} \abs{s}dx = \clo{\xi = \abs{s}x} = 2\abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi
\end{align*}

Thus the characteristic function simplifies to
\[\phi(s) = A \frac{\sin\abs{s}}{\abs{s}} + A \abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi \]

The integral can be dealt with in the following way: if $\alpha>0$ then \begin{align*}
	\int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi & = 
	\int_{\abs{s}}^\infty \frac{1}{\xi^{\alpha+1}} d\xi - \int_{\abs{s}}^\infty \frac{\xi - \cos\xi}{\xi^{\alpha+1}} d\xi\\
	& = \induc{-\frac{\xi^{-\alpha}}{\alpha}}_{\abs{s}}^\infty - \int_{\abs{s}}^\infty \frac{\xi - \cos\xi}{\xi^{\alpha+1}} d\xi
\end{align*}
%%  FINISH AT HOME!!!

\begin{align*}
A \abs{s}^\alpha \int_\abs{s}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt &=  A \abs{s}^\alpha \int_\abs{s}^\infty \frac{1}{t^{\alpha+1}} dt - A \abs{s}^\alpha \int_\abs{s}^\infty \frac{1-\cos(t)}{t^{\alpha+1}}dt
\end{align*}


It is possible to show that the integral is asymptotically equal to
\[\int_\abs{s}^\infty \frac{1-\cos(t)}{t^{\alpha+1}}dt \sim I_1(\alpha) + o(\abs{s}^{2-\alpha})\]

\begin{align*}
A \abs{s}^\alpha \int_\abs{s}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt &= \abs{s}^\alpha \frac{1}{\alpha} \abs{s}^{-\alpha} + AI_1(\alpha)\abs{s}^\alpha + o(s^2)
\end{align*}

Thus \[\phi(s) = 1 - A I_1(\alpha) \abs{s}^\alpha + o(s^2)\]

Since the variables are independent, then the sum has the product characteristic function
\[\Ex\brac{e^{-is\frac{S_n}{n^\frac{1}{\alpha}}}} = \brac{1 - A I_1(\alpha) \frac{\abs{s}^\alpha}{n} + o(s^2)}^n \to e^{-AI_1(\alpha)\abs{s}^\alpha}\]

THus it turns out that if $F(x)\underset{x\to\infty}{\sim} \abs{x}^{-\alpha}$,  $F(x)\underset{x\to-\infty}{\sim} 1-\abs{x}^{-\alpha}$ and the tails are symmetric, then \[\frac{\sum_{k=1}^{n}X_k}{n^\frac{1}{\alpha}}\to X\] where the random variable $X$ has the characteristic function given by \[\phi(s) = e^{-\frac{\alpha}{\alpha+1} I_1(\alpha)\abs{s}^\alpha}\] -- the Levy-Pareto characteristic function.

When $\alpha=2$, then the infinitesimal in the asymptotic expansion balances the major component, and inside there will emerge a divergent integral $\int \frac{dt}{t}$.

Thus the normalisation must have different scaling:
\[\frac{\sum_{k=1}^n X_k}{\sqrt{n\log n}}\sim \mathcal{N}(0,1)\]

The last case is, when the density is asymmetric.
\[p(x) = 1_\obj{\abs{x}\leq 1}\frac{A}{2} + 1_\obj{x>1}\frac{A(1+\beta)}{2\abs{x}^{\alpha+1}} + 1_\obj{x<-1}\frac{A(\beta-1)}{2\abs{x}^{\alpha+1}}\]
where the parameter $\beta\in\clo{0,1}$ is the mass defect.

\begin{align*}
	\phi(s) &= \int e^{-isx}p(x)dx &= \int_\clo{-1,1} e^{-isx}\frac{A}{2}dx + \int_1^\infty e^{-isx}\frac{A(1+\beta)}{x^{\alpha+1}}dx + \int_{-\infty}^{-1} e^{-isx}\frac{A(1-\beta)}{x^{\alpha+1}}dx\\
	&= \frac{A}{2} \int_\clo{0,1} \cos(sx)dx 
	+ \frac{A(1+\beta)}{2}\int_1^\infty \frac{e^{-isx}}{x^{\alpha+1}}dx 
	+ \frac{A(1-\beta)}{2}\int_{-\infty}^{-1} \frac{e^{-isx}}{x^{\alpha+1}}dx\\
	&= \frac{A}{2} \frac{\sin(s)}{s} 
	+ \frac{A(1+\beta)}{2}
	\int_1^\infty \frac{\cos(sy) - i\sin(sy)}{y^{\alpha+1}}dy 
	+ \frac{A(1-\beta)}{2}
	\int_{-\infty}^{-1} \frac{\cos(sx) - i\sin(sx)}{x^{\alpha+1}}dx\\
	&= \frac{A}{2} \frac{\sin(s)}{s} 
	+ A \int_1^\infty \frac{\cos(sy)}{y^{\alpha+1}}dy 
	- i A\beta \int_1^\infty \frac{\sin(sy)}{y^{\alpha+1}}dy
	&=\clo{t = \abs{s}x}\\
	&= \frac{A}{2} \frac{\sin\abs{s}}{\abs{s}} 
	+ A \abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt
	- i A\beta \abs{s} \int_1^\infty \sign(s) \frac{\sin(t)}{t^{\alpha+1}}dt
\end{align*}

THe integral \[I_2(\alpha) = \int_1^\infty \sign(s) \frac{\sin(t)}{t^{\alpha+1}}dt\] converges when $\alpha\in\brac{0,1}$

Therefore 
\[\phi(s) = 1 - AI_1(\alpha)\abs{s}^\alpha \brac{1+i\beta \sign(s) \frac{I_2(\alpha)}{I_1(\alpha)}} + o(s^2)\]

From analysis it is known (?) that 
\[\frac{I_2(\alpha)}{I_1(\alpha)} = \tg\frac{\pi \alpha}{2}\]
whence the limiting distribution has the following characteristic function:
\[\phi(s) = \exp\brac{-\frac{\alpha}{\alpha+1}I_1(\alpha)\abs{s}^\alpha \brac{1+i\beta \sign(s) \tg\frac{\pi \alpha}{2} } }\]

No the case when $\alpha=1$.
\begin{align*}
	\phi(s) &= \int e^{-isx}p(x)dx &= \int_\clo{-1,1} e^{-isx}\frac{A}{2}dx + \int_1^\infty e^{-isx}\frac{A(1+\beta)}{x^{\alpha+1}}dx + \int_{-\infty}^{-1} e^{-isx}\frac{A(1-\beta)}{x^{\alpha+1}}dx
\end{align*}

% see the lecture notes 5.8, 5.9

% for small $s\to 0$
% iA\beta \abs{s}^\alpha\sign(s) \int_{\abs{s}}^\infty \frac{\sin t}{t^2} dt

% \int_{\abs{s}}^\infty \frac{\sin t}{t^2} dt = 
% \int_{\abs{s}}^1 \frac{1}{t} dt + \int_{\abs{s}}^1 \frac{\sin(t)-t}{t^2} dt + \int_1^\infty \frac{\sin t}{t^2} dt
% = iA\beta s\log \abs{s} + \text{const} i s + o(s^2)

Using the convergence of characteristic functions, extracting the divergent integral, regularizing it -- the basic idea (complex in general, simple locally). 

Stability
\[\frac{\sum_{k=1}^n X_k - \mu n}{n^\frac{1}{\alpha}} \sim X_k\]

if $0<\alpha<1$ then ...


% section lecture_4 (end)

\end{document}
