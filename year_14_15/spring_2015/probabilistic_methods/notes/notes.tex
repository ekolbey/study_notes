\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Rbar}{{\bar{\mathbb{R}}}}
\newcommand{\Cplx}{\mathbb{C}}

\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ical}{\mathfrak{I}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\borel}{\mathcal{B}}

\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\pr}{\mathbb{P}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}
\newcommand{\sign}{\mathop{\text{sgn}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Probabilistic methods in modelling}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}
\selectlanguage{english}
\maketitle

Theory of Stochastic processes
Brownian motion
Martingales
Poisson point processes

Generalizationo of the central limit theorem:
-- normal distribution
-- stable processes
-- Fisher-Tipett-Gendenko thorem for maxima

Based on the law of large numbers:
-- Extreme value theory

Additional chapters of probability

Elementary probability: Kolmogorov axioms, Measure theoretic approach.
Bernoulli law of large numbers.
Stochastic processes require measure theoretic foundations for their definition on some abstract space.

%% \selectlanguage{russian}
\section{Lecutre \#1} % (fold)
\label{sec:lecutre_1}


Suppose $\brac{X_k}_{k=1}^n$ is a finite collection of random variables which are independent and identically distributed.

What are the asymptotics of functions of $\brac{X_k}_{k=1}^n$ and their probability distributions.

Consider a measure space $(\Omega, \Fcal, P)$ and an RV $X(\omega)$ from $\Omega$ to $(\Xcal, \Sigma)$ on it.
This induces a measure in $\Xcal$ -- the image space. see \textbf{image~1}

\[\mathbb{P}_X \defn X_\# \mathbb{P} = \mathbb{P}\brac{X\in A} = \int 1_{X^{-1}(A)}d\mathbb{P}\]

For limiting theorems the image space is sufficient, but we need convergence in distributions. However, abstract spaces would be needed in studies of stochastic processes (and endowed with filtration).

Consider $X\in \Real$.
How is the distribution of $X$ defined?

It can be defined using the semi-ring of half-closed intervals $\ploc{a,b}$, but the most popular, and equivalent, method is via a function with specific properties -- \textbf{C}umulative \textbf{D}istribution \textbf{F}unction.

Since every random variable induces a distribution on its image space, and it is always possible to construct a probability space and a measurable function such that they induce the required law in the image space (Skorohod's representation, see [Williams, 1991, p.~34]).
Thus random variables can be partitioned into classes identified by their distribution function.
In the following random variables and their corresponding distributions can be used interchangeably.

A map $F:\Real\to \clo{0,+\infty}$ is a distribution function if \begin{enumerate}
	\item $F$ is non decreasing;
	\item $F(-\infty) = 0$ and $F(+\infty) = 1$.
	\item $F$ is right-continuous and bounded;
\end{enumerate}
In short $F$ must be c\'adl\'ag with a crucial requirement that $F(\Real)=1$.

Modern statistical physics was founded on Gibbs's idea.
A physical system might be in different states, thus Gibbs and Bolzmann postulated that each state has some likelihood.

What is the likelihood of picking an even number in $\mathbb{Z}$? Suppose there is some probability measure on $\mathbb{Z}$ given by $\brac{p_n}_{n\in \mathbb{Z}}$ 
\[\sum_{n\text{ div } 2} p_n\]

Let $A\subseteq \mathbb{Z}$ then the density of $A$ is \[\rho(A) \defn \lim_{n\to+\infty, m\to -\infty} \frac{\abs{\obj{\induc{k\in \mathbb{Z}}\,n\leq k\leq m }}}{n+m+1}\]
However $\rho(\cdot)$ fails to be countably sub-additive.

Classification of distribution functions on $\Real$. \begin{description}
	\item[Atomic:] \hfill \\
		there is a countable and measurable subset $A\in \borel(\Real)$ and a collection $\brac{p_k}_{k\in A}$ such that
			\[F(x) = \sum_{k\in A} p_k 1_{\obj{k}}(x)\]
	\item[Singular:] \hfill \\
		all the rest. 
	\item[Absolutely continuous:] \hfill \\
		there exists a lebesgue-measurable map $p:\Real\to\Real$ with $F(x)=\int p(x) dx$
\end{description}

Lebesgue theorem: every distribution function can be represented as a weighted sum of three basic types of distributions.


Classification of local behaviour of distributions in $\Real$.

Consider some $x_0\in \Real$ and consider the difference $F(x_0+\Delta)-F(x_0-\Delta)$ for $\Delta > 0$.
The function $F$ has the singularity order $\alpha$ if \[F(x_0+\Delta)-F(x_0-\Delta) = \Omega(\Delta^\alpha)\]

\begin{description}
	\item[Atomic:] $\alpha = 0$;
	\item[Continuous:] $\alpha = 1$;
\end{description}

If $F(x)=\min\obj{\sqrt{x}, 1} 1_{\clop{0,+\infty}}(x)$ then $0$ has singularity order $\frac{1}{2}$.

Cantor distribution has uncountably many points with $\alpha = 0$, and uncountably many points with $\alpha = \frac{\log 2}{\log 3}$.
A good idea is to represent any $x\in \clo{0,1}$ in base-3 representation. The ``interesting'' points have either $0$ or $2$ in their base-3 form.

\noindent\textbf{Problem \#1}\hfill \\
	Given $U\sim \mathcal{U}\clo{0,1}$ and $Y=f(U)$. Find $f$ such that the density $p_Y$ in given by
	\[\lambda e^{-\lambda y} 1_{\clop{0,+\infty}}\]
	If $F(x)$ is absolutely continuous and $X\sim F$, then $F(X)\sim \mathcal{U}\clo{0,1}$. (Skorokhod)

\noindent\textbf{Problem \#2}\hfill \\
	Suppose $(X,Y)$ is uniformly distributed inside the unit circle. Let $\rho \defn \sqrt{X^2+Y^2}$. What is the distribution of $(x',y')$ with the radius given by $r = \sqrt{-\log \rho}$.
	Using the Jacobian transformation, the distribution of the transformed pair is jointly gaussian.

% section lecutre_1 (end)

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

A shorter lecture, no definition of expectation, but full of functions:
\begin{itemize}
	\item Probability generating functions;
	\item Moment generating functions;
	\item Characteristic functions.
\end{itemize}

Suppose $X$ is a real-valued random variable with law $F_X$. If $F_X$ is absolutely continuous with respect to $dx$, then $F_X(x) = \int_{-\infty}^x f dx$.

The expectation of $X$ is defined as \[\Ex(X) \defn \int X d{F_X}\] and in the case of absolutely continuous distribution it is true that \[\int X d{F_X} = \int X f(x) dx\]

Riemann-Stieltjes integrals are used (but those who know, can use a better Lebesgue-Stieltjes integral).
\[\int d dF = \lim_{\Delta\to 0} \sum_i g(x_i) \brac{F(\xi_{i+1}) - F(\xi_i)}\]
Lebesgue integral is pedagogically more difficult, though more abstract, flexible and overall better.
% Geometric measure theory.

Cauchy distribution does not allow $\Ex X$, however it has the mean in terms of the \textbf{main value} (integration over a symmetric interval $\clo{-M,+M}$).
% \rus{Уравнение Власова}

The expectation of a function $g$ of $X$ is $\Ex\brac{g(X)} = \int g(X) dF_X$.

The $k$-th moment of an RV is $\Ex X^k$ and the $k$-th central moment is $\Ex\brac{ X - \Ex X}^2$.
The second central moment of $X$ is the variance of $X$ : $\mathbb{D}(X) \defn \Ex\brac{ X - \Ex X}^2$.

Jensen's inequality. For any convex function $g$ of $X$ it is true that 
\[g\brac{\Ex X}\leq \Ex g(X) \]

Fatou's lemma \[\int \liminf X_n d\mu \leq \liminf \int X_n d\mu\]

Holder's inequality for nonnegative measurable $f$ and $g$ \[\int fg d\mu \leq \brac{\int f^p d\mu}^\frac{1}{p}\brac{\int g^q d\mu}^\frac{1}{q}\] whenever $\sfrac{1}{p}+\sfrac{1}{q}=1$.

Minkowski inequality \[\brac{\int {(f+g)}^p d\mu}^\frac{1}{p}\leq \brac{\int f^p d\mu}^\frac{1}{p} + \brac{\int g^p d\mu}^\frac{1}{p}\]

The probability generating function is defined for any $z\in \Cplx$ with $\abs{z}\leq 1$ as \[G_X(z) \defn \Ex z^X\]
If $X$ has discrete distribution, then $G_X(z) = \sum_{n\geq 0} z^n \Pr(X=n)$.
Since the series is analytical in a finite disc, it is differentiable.
Thus \[G_X^{(k)}(z) = \sum_{n\geq 0} \frac{n!}{(n-k)!} z^{n-k} \Pr(X=n) \]
whence $G_X^{(k)}(0) = \Ex \brac{X(X-1)\ldots (X-k+1)}$

Factorial exponent: $x^{\underline{k}} = \prod_{s=1}^k (X-s+1)$.
\[x^{\underline{k}} - \brac{x-1}^{\underline{k}} = k \cdot \brac{x-1}^{\underline{k-1}} \]

The moment generating function of $X$ is given by \[S_X(t) \defn G(e^t) = \Ex(e^{tX})\]
Differentiation of the MGF and evaluation at $t=0$ yields moments of $X$.
Indeed, if the integral converges uniformly, then $S_X^{(k)}(t) = \induc{\Ex X^k e^{tX}}_{t=0} = \Ex X^k$.

\emph{A gentle reminder}\hfill \\
For any pair of independent random variables the expectation of their product equals the product of their expectations.

Thus the mgf of a sum of independent $X,Y$ results in \[S_{X+Y}(t) = \Ex e^{t(X+Y)} = \Ex e^{tX} e^{tY} = \Ex e^{tX} \Ex e^{tY} = S_X(t) S_Y(t)\]

The cumulants of $X$ (or semiinvariants) are defined as the coefficients in the following formal series expansion of $\log S_X(t)$ as
\[\log S_X(t) \defn \sum_{n\geq 0} \frac{t^n}{n!} \kappa_n^X\]

Using the formal differentiation of the series with respect to $t$ yields
$\kappa_n^X = \induc{\frac{d^n}{dt^n} \log S_X(t)}_{t=0}$.

Since $S_X(t)$ is additive with respect to $X$ if the summands are independent,
Kurtosis, \rus{эксцесс} -- measures how far the probability is shifted to the tails.

\textbf{Martsinkevich's theorem}\hfill\\
$\log S_X(t)$ is a polynomial if and only if $X\sim\mathcal{N}(\mu,\sigma)$.


Solve exercises 1.8, 1.11, 1.12 and 1.13 on pp.~42-43.

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

%% Missed the first part of the lecture
Tracey-Widam distribution

\textbf{T}otally \textbf{A}symmatric \textbf{S}imple \textbf{E}xclusion \textbf{P}rocess: when a particles on a lattice can jump and shift to the right, but not over another particle.

% Random matrix theory -- Ivan Corvin

Asymptotic (high dimensional) representation theory:
KPZ scaling and Tracey-Widam

The (weak) Law of Large Numbers:
\[\pr\brac{\abs{ \frac{\sum_{k=1}^n X_k}{n}-\mu }>\epsilon} \to 0\]


\noindent \textbf{Definition} of weak convergence. \hfill\\
A sequence of random variables $\brac{X_n}_{n\geq1}$ converges weakly to $X$, or in distribution, if for all $a,b\in \Rbar$, such that $a,b$ are atoms, probability concentrations:
\[\pr\brac{a<X_n<b}\to \pr\brac{a<X<b}\]

\noindent \textbf{Proposition} 1\hfill \\
$X_k\overset{D}{\to}X$ if and only if $F_{X_n}\to F_X$ at any point of continuity of $F$.

Continuity is required, because the CDf is c\'adlag (only right continuous).

Let $\brac{x_k}_{k\geq0}$ be a monotonous deterministic sequence, which converges to $\mu$.
If $x_k\uparrow$, then CDFs of a deterministic sequence do converge to the CDF of their limit. And not otherwise.

\noindent \textbf{Proposition} 1\hfill \\
$X_k\overset{D}{\to}X$ if and only if $F_{X_n}\to F_X$ at any point of continuity of $F$.

\noindent \textbf{Proposition} 2 (Helly)\hfill \\
Every sequence of distribution functions $\brac{F_n}_{n\geq1}$ it is possible to select a subsequence, such that $F_{n_k}$ converges to some function $F$ at every point of continuity of $F$ and such that $F$ is monotonous.

Note that the function $F$ is non-normalised.

Sketch:
Use the rational numbers $\mathbb{Q}$ as the \textbf{knots} of convergence of $F$. Enumerate them and choose the ``diagonal''.

Proof. Suppose $\brac{q_n}_{n\geq 1}$ enumerates $\mathbb{Q}$.
The sequence $\brac{F_n(q_1)}_{n\geq1}$ is bounded, hence it has a convergent subsequence, whence exists $\brac{k_n}\uparrow\infty$ such that $F_{k_n}(q_1)$ converges to some $F(q_1)$.
Consider $\brac{F^1_n}_{n\geq 1}\defn\brac{F_{k_n}}_{n\geq1}$ as a new sequence, and apply the same argument to it recursively.

Finally we get a sequence $\brac{F^m_n}_{n\geq1}$ such that $F^m_n(q_l)$ converges to $F(q_l)$ for all $l\leq m$ as $n\to \infty$, either by being a subsequence or by the choice of the subsequence $n_k$.

Use Cantor's diagonal argument: form a new sequence $\brac{F^n_n}_{n\geq1}$. This sequence converges at any $q\in\mathbb{Q}$ to $F(q)$.

Now to prove that $F$ is continuous.

If $\lim_{q\uparrow x}F(q) = \lim_{p\downarrow x}F(p)$, then set $F(x)$ to the limit.

The function then becomes either continuous, or left limited and right continuous, since $F_n$ are monotonous and right-continuous.

Counterexample:
Suppose $X_n=n$. Then $F_n(x) = 1_{\ploc{-\infty, x}}(n)$.

In order to get a requirement in Helly's theorem, which guarantees that the limiting function is a distribution function, one has to require the following ``tightness'' condition (uniform property -- nor all $n$):
\[\forall \epsilon>0\,\exists L>0\,\text{s.t.}\,\sup_{n\geq1}\pr\brac{\abs{X_n}>L}\leq \epsilon\]

% See the photographs of the explanation of the method of closed loops.

% Алгебра -- рай, а недифференцируемость -- нечистая чила

\noindent \textbf{The main technical result}\hfill\\

The sequence of distribution function $F_n$ converges weakly to some $F$ if $\phi_n\to \phi$, their associated characteristic functions converge, and $\phi$ is continuous at $0$.

\noindent \textbf{Proposition}\hfill\\

Suppose $X_n\to X$ and $\phi_n(s)\defn \Ex\brac{e^{isX_n}}$. Then there exists a limiting function of $\phi_n$ such that $\phi_n\to \phi = \Ex\brac{e^{isX}}$.

Let's show that if $X_n\overset{D}{\to} X$ then
\[\int g(X) dF_n \to \int g(X)dF\]
for any bounded continuous function $g$.

\textbf{Proof}\hfill\\
Split the integration region into two regions:
\[\int f dF_n = \int_\obj{\abs{X}>R} g dF_n + \int_\obj{\abs{X}\leq R} g dF_n\]

The tightness condition implies that there exists $R$ such that
\[F_n\brac{\obj{\abs{X}>R}}< \frac{\epsilon}{2}\]
which means that $\int_\obj{\abs{X}>R} g dF_n$ is bounded above by $M\frac{\epsilon}{2}$ where $\abs{g}\leq M$.

Since $g$ is continuous on a compact $\obj{\abs{X}\leq R}$, for $\epsilon>0$ there exists $\delta>0$ such that on each $\delta$-range the change of $g$ is not greater that $\epsilon$.
therefore 
\[\int_\obj{\abs{X}\leq R} g dF_n = \int_\obj{\abs{X}\leq R} (g-g^\epsilon) dF_n + \int_\obj{\abs{X}\leq R} g^\epsilon dF_n \leq F_n\brac{\obj{\abs{X}\leq R}}\frac{\epsilon}{2} + \int_\obj{\abs{X}\leq R} g^\epsilon dF_n\]

where $g^\epsilon$ is an $\epsilon$ step function.

However
\[\int_\obj{\abs{X}\leq R} g^\epsilon dF_n\]
is a finite sum $\sum_i g^\epsilon$

Therefore if $X_n\overset{D}{\to} X$ then characteristic function s converge.


% Feller -- Probability: a neat proof

\noindent\textbf{Proposition} \hfill\\
If
\[\phi(s) = \int e^{isx} dF(x)\]
then the following equivalence is true for any $\delta>0$ and $y$:
\[\frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds
=\frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF(x)\]
for a gaussian smoothing kernel $e^{-\frac{{(x-y)}^2}{2\delta^2}}$.

Indeed,
\[e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}}
= e^{-isy} e^{-\frac{\delta^2 s^2}{2}}\int e^{isx} dF(x)
 = e^{-isy}\int e^{isx} e^{-\frac{\delta^2 s^2}{2}} dF(x)\]
whence
\begin{align*}
	\frac{1}{2\pi} \int e^{-isy}\phi(s)e^{-\frac{\delta^2 s^2}{2}} ds
	&= \frac{1}{2\pi} \int e^{-isy} \int e^{isx} e^{-\frac{\delta^2 s^2}{2}} dF(x) ds\\
	&= \frac{1}{2\pi} \int \int e^{is(x-y)-\frac{\delta^2 s^2}{2}} dF(x) ds
\end{align*}
Since
\[is(x-y)-\frac{\delta^2 s^2}{2}
= \frac{1}{2}2s\delta \frac{i(x-y)}{\delta}-\frac{\delta^2 s^2}{2} \pm \frac{1}{2}\frac{(x-y)^2}{\delta^2}
= -\frac{1}{2}\Big( \big( \frac{i(x-y)}{\delta} - s\delta \big)^2 + \frac{(x-y)^2}{\delta^2} \Big)
\]
we get
\begin{align*}
	\ldots
	&= \frac{1}{2\pi} \int \int e^{-\frac{{(x-y)}^2}{2\delta^2}} e^{\frac{\delta^2}{2}\brac{-i\frac{x-y}{\delta^2}+s}^2} dF(x) ds\\
	&= \frac{1}{2\pi} \int e^{-\frac{{(x-y)}^2}{2\delta^2}} \int e^{\frac{\delta^2}{2}\brac{-i\frac{x-y}{\delta^2}+s}^2} ds dF(x)\\
	&= \frac{1}{2\pi} \int e^{-\frac{{(x-y)}^2}{2\delta^2}} \frac{\sqrt{2\pi}}{\delta} dF(x)
% Missed some details...
\end{align*}

\noindent\textbf{Another lemma}\hfill\\
\noindent The characteristic functions of different distributions are never identical.

Indeed, if characteristic functions coincide, but but distributions differ at least in some point, then there is a contradiction

Suppose $\phi_n(x) = \Ex\brac{e^{isX_n}}$ converges to some $\phi$ -- and $\phi$ is continuous at $x=0$, then there exists a random variable, such that $\phi(s) = \Ex\brac{e^{isX}}$ and $X_n\to X$ weakly.

Suppose $Y_k$ are iid and $X_n = \sum_{k=1}^n Y_k$ with $\Ex\brac{e^{isY}} = \psi(s) = e^{-\abs{s}}$.

Then $\phi_n(s) = \Ex\brac{e^{isX}} = \brac{\phi(s)^n}$, but it converges to $1_{s=0}$.

% the probability is smeared along the infinity/

\begin{align*}
	\frac{1}{2\pi}\int e^{-sy} \phi_n(s) e^{-\frac{\delta^2 s^2}{2}}ds
	& = \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF_n(x)
\end{align*}

Helly's theorem implies that there is a convergent subsequence $F_{n_k}$ that converges to some $F$.

Then analogously to a previously stated proposition
\[\frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF_n(x)
\to \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF\]

Using Lebesgue's Dominated convergence theorem it is true that
\[\frac{1}{2\pi}\int e^{-sy} \phi_{n_k}(s) e^{-\frac{\delta^2 s^2}{2}}ds
\to \frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds\]

Therefore
\[\frac{1}{2\pi}\int e^{-sy} \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds
= \frac{1}{\sqrt{2\pi\delta^2}}\int e^{-\frac{{(x-y)}^2}{2\delta^2}}dF\]
Since $F$ is an arbitrary limiting function, this implies that any other convergent subsequence converges to the same $F$, whence $F_n\to F$.

To show that $F$ is a CDF we do the following: think of letting $\delta\to \infty$ and fixing $y=0$. Then
\[\frac{\delta^2}{\sqrt{2\pi}}\int \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds
= \int e^{-\frac{x^2}{2\delta^2}}dF \leq \int dF\]

However as $\delta\to\infty$ it is true that
\[\int \phi(s) e^{-\frac{\delta^2 s^2}{2}}ds\to \phi(0) = 1\]
since $\phi(0)$ is continuous at $0$.

Thus $1 \leq \int dF$. But $\int dF$ cannot strictly exceed $1$, whence $F$ is a distribution function.

\noindent\textbf{Lyapunov's Central Limit Theorem} \hfill\\
Suppose $X_k$ are i.i.d. with $\Ex X_k = \mu$ and $\Ex X_k^2 = \sigma^2$. Introduce the following sequence:
\[Y_n = \frac{\sum_{k=1}^n X_k - n \mu }{\sqrt{n}}\]
Then $Y_n\overset{D}{\to}\mathcal{N}(0,1)$

\textbf{Proof}\hfill\\

If $Z_n = \sum_{k=1}^n X_k - n\mu$. Then $\Ex Z_n = 0$ and $\Var Z_n = n \sigma^2$ due to independence.

Let $\phi_n(s) = \Ex\brac{e^{isZ_n}}$. Then
\[\phi_n(s) = 1 + 0 - \frac{n\sigma^2}{2} s^2 + o(s^2)\]

Now
\[\phi_{Y_n}(s) = \Ex\brac{e^{is\frac{Z_n}{\sigma \sqrt{n}}}} = \phi_n(\frac{s}{\sigma\sqrt{n}})\]
whence
\[\phi_{Y_n}(s) = 1 - \frac{s^2}{2} + o(s^2)\]


Another way: due to independence of $X_k$ and using the product-independence property of characteristic function
\[\phi_n(s) = \brac{\phi_{X-\mu}(s)}^n = \brac{ 1 - \frac{n\sigma^2}{2} s^2 + o(s^2) }^n\]

Therefore
\[\phi_{Y_n}(s) = \phi_n(\frac{s}{\sqrt{n}\sigma}) = \brac{ 1 - \frac{s^2}{2n} + o(s^2) }^n\]
 where $o(\cdot)$ is uniformly infinitesimal with respect to $n$.

Using a well known limit we get the following limit:
\[\phi_{Y_n}(s) \to e^{-\frac{s^2}{2}}\]

an example:
Suppose $\brac{X_n}_{n\geq1}$ are independent identically distributed such that $\phi_X(s) = 1+i\mu s + o(s)$, where $o(\cdot)$ is an infinitesimal.

Let \[Y_n = \frac{\sum_{k=1}^n X_k - n \mu }{n}\]
then
\[\phi_{Y_n}(s) = \phi_{\sum}(\frac{s}{n})
= \brac{\phi_X(\frac{s}{n})}^n = \brac{1+i\mu s + o(s)}^n
\to e^{is\mu}\]

The necessary and sufficient condition for $\phi(s) = 1+i s\mu+o(s)$ is
\[\lim_{R\to\infty} R\cdot \brac{F_X(-R) + 1 - F_X(R)} = 0\]
% Khinchine's law of large numbers.

% Tutorials
% Homework У2.1, У2.6, У2.7, У2.8, У2.9, У3.9, У3.3, У3.6

% section lecture_3 (end)

\section{lecture \# 4} % (fold)
\label{sec:lecture_4}

Gamma distribution.
\[\gamma(k,\beta) \defn \frac{\beta^k}{\Gamma(k)} s^{k-1} e^{-\beta s}\]

It has all the moments, since the exponent decays very rapidly, faster than any polynomial.

Some exampls of the $\gamma$ distribution are \begin{description}
	\item[$\chi^2_k$] A $\chi^2$ random variable is actually $\gamma\brac{\frac{k}{2},\frac{1}{2}}$;
	\item[$\text{Exp}(\lambda)$] An exponential distribution is a particular case of the Gamma distribution: $\text{Exp}(\lambda) \sim \gamma(1,\lambda)$.
\end{description}

This distribution is asymmetric.
\[\frac{\sum_{k=1}^n G_k- }{}\]

Limiting behaviour of products of random variables.

Suppose $\brac{X_k}_{k=1}^n$ are non-negative random variables. Consider the asymptotics of \[P_n \defn \brac{e^{-\mu n}\prod_{k=1}^n G_k}^{\sqrt{n}}\]

The natural approach is to use $Y_k\defn \log X_k$ and consider the asymptotics of the logarithm of the product: the normalised sum
\[S_n \defn \log P_n \frac{\sum_{k=1}^n Y_k - \mu n}{\sqrt{n}}\]

Since this converges in distribution to a standard normal random variable: $S_n \overset{\mathcal{D}}{\to} \mathcal{N}(0,1)$. Thus the products converge in distribution to a log-normal random variable:
\[\frac{1}{\sqrt{2\pi\sigma^2}} e^\frac{-\log^2 x}{2\sigma^2}\frac{1}{x}\]

What happens to the central limit theorem, when its assumptions fail.
\begin{description}
	\item[Independence] Stochastic processes CLT;
	\item[Identical distribution] The most useful generalisation of the CLT;
	\item[Moments] What if $\Ex(X_k) = \mu$ and $\text{Var}(X_k) = \sigma^2$ are violated?
\end{description}

Consider the Cauchy distributed random variable. IS $\brac{X_n}_{n\geq1}\sim \text{Cauchy}$, then their normalised sum is still Cauchy (consider the characteristic function).

What precludes a distribution from having finite variance?

The main moment is the computation of the asymptotics at zero of the characteristic function (the Fourier transform of the distribution).

Consider the following a bit contrived density:
\[p(x) = 1_\obj{\abs{x}\leq 1}\frac{A}{2} + 1_\obj{\abs{x}>1}\frac{A}{2\abs{x}^{\alpha+1}}\]
where $A=\frac{\alpha}{\alpha+1}$ and $\alpha$ - the Levy exponent. The exponent characterises asymptotics convergences of the CDF to the horizontal asymptotes: $\abs{x}^{-\alpha}$. In the limit we get the Pareto-Levy family of distributions.

Let's use the characteristic functions.
\[\phi(s)\defn\int e^{-isx}p(x)dx\]
Consider the asymptotics near zero (the problems at infinity i time domain transform to the problems near zero in the frequency domain).

Since $p(x)$ is symmetrical
\begin{align*}
	\phi(s) &= \Ex\brac{e^{-isX} } = \int e^{-isx}p(x)dx \\
	&= \int_{-1}^1 \frac{A}{2} e^{-isx} dx + \brac{\int_1^\infty + \int_{-\infty}^{-1}} \frac{A}{2} \frac{e^{-isx} }{\abs{x}^{\alpha+1}} dx
\end{align*}

Now the first integral is just \begin{align*}
	\int_{-1}^1 e^{-isx} dx &= \int_{-1}^1 \cos{sx} -i\sin{sx} dx \\ 
	&= \int_{-1}^1 \cos{sx} dx - i \int_{-1}^1 \sin{sx} dx \\
	& =  \induc{\frac{\sin{sx}}{s}}_{-1}^1 - i \induc{\frac{-\cos{sx}}{s}}_{-1}^1\\
	& = \frac{\sin{s}}{s}-\frac{\sin{-s}}{s} = 2 \frac{\sin{s}}{s} = 2 \frac{\sin\abs{s}}{\abs{s}}
\end{align*}
while the second is a little bit more intricate:
\begin{align*}
	\brac{\int_1^\infty + \int_{-\infty}^{-1}} \frac{e^{-isx} }{\abs{x}^{\alpha+1}} dx
	&= \int_1^\infty \frac{\cos{sx}-i\sin{sx}}{x^{\alpha+1}} dx + \int_{-\infty}^{-1} \frac{\cos{sx}-i\sin{sx}}{\brac{-x}^{\alpha+1}} dx \\
	&= \int_1^\infty \frac{\cos{sx}-i\sin{sx}}{x^{\alpha+1}} dx + \int_1^\infty \frac{\cos(-sy)-i\sin(-sy)}{y^{\alpha+1}} dy \\
	&= \int_1^\infty \frac{2\cos{sx}}{x^{\alpha+1}} dx = 2\int_1^\infty \frac{\cos{\abs{s}x}}{x^{\alpha+1}} dx \\
	& = 2\abs{s}^\alpha \int_1^\infty \frac{\cos{\abs{s}x}}{\brac{\abs{s}x}^{\alpha+1}} \abs{s}dx = \clo{\xi = \abs{s}x} = 2\abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi
\end{align*}

Thus the characteristic function simplifies to
\[\phi(s) = A \frac{\sin\abs{s}}{\abs{s}} + A \abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi \]

The integral can be dealt with in the following way: if $\alpha>0$ then \begin{align*}
	\int_{\abs{s}}^\infty \frac{\cos \xi}{\xi^{\alpha+1}} d\xi & = 
	\int_{\abs{s}}^\infty \frac{1}{\xi^{\alpha+1}} d\xi - \int_{\abs{s}}^\infty \frac{\xi - \cos\xi}{\xi^{\alpha+1}} d\xi\\
	& = \induc{-\frac{\xi^{-\alpha}}{\alpha}}_{\abs{s}}^\infty - \int_{\abs{s}}^\infty \frac{\xi - \cos\xi}{\xi^{\alpha+1}} d\xi
\end{align*}
%%  FINISH AT HOME!!!

\begin{align*}
A \abs{s}^\alpha \int_\abs{s}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt
&= A \abs{s}^\alpha \int_\abs{s}^\infty \frac{1}{t^{\alpha+1}} dt - A \abs{s}^\alpha \int_\abs{s}^\infty \frac{1-\cos(t)}{t^{\alpha+1}}dt
\end{align*}


It is possible to show (?) that the very last integral in the right hand side is asymptotically equal to
\[\int_\abs{s}^\infty \frac{1-\cos(t)}{t^{\alpha+1}}dt \sim \Ical_1(\alpha) + o(\abs{s}^{2-\alpha})\]
which means that
\begin{align*}
A \abs{s}^\alpha \int_\abs{s}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt
&= \abs{s}^\alpha \frac{1}{\alpha} \abs{s}^{-\alpha} + A \Ical_1(\alpha)\abs{s}^\alpha + o(s^2)
\end{align*}
Thus
\[\phi(s) = 1 - A \Ical_1(\alpha) \abs{s}^\alpha + o(s^2)\]

Since the variables are independent, then the sum has the product characteristic function
\[\Ex\brac{ e^{-is\frac{S_n}{n^\frac{1}{\alpha}}} }
= \brac{1 - A \Ical_1(\alpha) \frac{\abs{s}^\alpha}{n} + o(s^2)}^n \to e^{-A \Ical_1(\alpha)\abs{s}^\alpha}\]

Thus it turns out that if $F(x)\underset{x\to\infty}{\sim} \abs{x}^{-\alpha}$,
$F(x)\underset{x\to-\infty}{\sim} 1-\abs{x}^{-\alpha}$ and the tails are symmetric,
then
\[\frac{\sum_{k=1}^{n}X_k}{n^\frac{1}{\alpha}}\to X\]
where the random variable $X$ has the characteristic function given by
\[\phi(s) = e^{-\frac{\alpha}{\alpha+1} \Ical_1(\alpha)\abs{s}^\alpha}\]
-- the Levy-Pareto characteristic function.

When $\alpha=2$, then the infinitesimal in the asymptotic expansion balances the major component, and inside there will emerge a divergent integral $\int \frac{dt}{t}$.
Thus the normalisation must be different:
\[\frac{\sum_{k=1}^n X_k}{\sqrt{n\log n}}\sim \mathcal{N}(0,1)\]

The last case is, when the density is asymmetric.
\[p(x) = 1_\obj{\abs{x}\leq 1}\frac{A}{2}
	+ 1_\obj{x>1}\frac{A(1+\beta)}{2\abs{x}^{\alpha+1}}
	+ 1_\obj{x<-1}\frac{A(\beta-1)}{2\abs{x}^{\alpha+1}}\]
where the parameter $\beta \in \clo{0,1}$ is the mass defect.
\begin{align*}
	\phi(s) &= \int e^{-isx}p(x)dx\\
	&= \int_\clo{-1,1} e^{-isx}\frac{A}{2}dx
		+ \int_1^\infty e^{-isx}\frac{A(1+\beta)}{|x|^{\alpha+1}}dx
		+ \int_{-\infty}^{-1} e^{-isx}\frac{A(1-\beta)}{|x|^{\alpha+1}}dx\\
	&= \frac{A}{2} \int_\clo{0,1} \cos(sx)dx 
		+ \frac{A(1+\beta)}{2}\int_1^\infty \frac{e^{-isx}}{x^{\alpha+1}}dx 
		+ \frac{A(1-\beta)}{2}\int_{-\infty}^{-1} \frac{e^{-isx}}{(-x)^{\alpha+1}}dx\\
	&= \frac{A}{2} \frac{\sin(s)}{s} 
		+ \frac{A(1+\beta)}{2} \int_1^\infty \frac{\cos(sy) - i\sin(sy)}{y^{\alpha+1}}dy \\
		&\quad + \frac{A(1-\beta)}{2} \int_{-\infty}^{-1} \frac{\cos(sy) - i\sin(sy)}{(-y)^{\alpha+1}}dy\\
	&= \frac{A}{2} \frac{\sin(s)}{s} 
		+ A \int_1^\infty \frac{\cos(sy)}{y^{\alpha+1}}dy 
		- i A\beta \int_1^\infty \frac{\sin(sy)}{y^{\alpha+1}}dy\\
	&= \Big[ \text{substitution: } t = \abs{s} x; \cos\text{--even function}\Big ]\\
	&= \frac{A}{2} \frac{\sin\abs{s}}{\abs{s}} 
		+ A \abs{s}^\alpha \int_{\abs{s}}^\infty \frac{\cos(t)}{t^{\alpha+1}}dt
		- i A\beta \abs{s} \int_1^\infty \sign(s) \frac{\sin(t)}{t^{\alpha+1}}dt
\end{align*}
The integral
\[\Ical_2(\alpha) = \int_1^\infty \sign(s) \frac{\sin(t)}{t^{\alpha+1}}dt\]
converges when $\alpha\in\brac{0,1}$. Therefore 
\[\phi(s) = 1 - A \Ical_1(\alpha)\abs{s}^\alpha \brac{1+i\beta \sign(s) \frac{\Ical_2(\alpha)}{ \Ical_1(\alpha)}} + o(s^2)\]
From analysis it is known (?) that
\[\frac{\Ical_2(\alpha)}{ \Ical_1(\alpha)} = \tg\frac{\pi \alpha}{2}\]
whence the limiting distribution must have the following characteristic function:
\[\phi(s) = \exp\brac{-\frac{\alpha}{\alpha+1} \Ical_1(\alpha)\abs{s}^\alpha \brac{1+i\beta \sign(s) \tg\frac{\pi \alpha}{2} } }\]

Now the case when $\alpha=1$.
\begin{align*}
	\phi(s) &= \int e^{-isx}p(x)dx
	= \int_\clo{-1,1} e^{-isx}\frac{A}{2}dx
		+ \int_1^\infty e^{-isx}\frac{A(1+\beta)}{x^{\alpha+1}}dx
		+ \int_{-\infty}^{-1} e^{-isx}\frac{A(1-\beta)}{x^{\alpha+1}}dx
\end{align*}

% see the lecture notes 5.8, 5.9

% for small $s\to 0$
% iA\beta \abs{s}^\alpha\sign(s) \int_{\abs{s}}^\infty \frac{\sin t}{t^2} dt

% \int_{\abs{s}}^\infty \frac{\sin t}{t^2} dt = 
% \int_{\abs{s}}^1 \frac{1}{t} dt + \int_{\abs{s}}^1 \frac{\sin(t)-t}{t^2} dt + \int_1^\infty \frac{\sin t}{t^2} dt
% = iA\beta s\log \abs{s} + \text{const} i s + o(s^2)

Using the convergence of characteristic functions, extracting the divergent integral, regularizing it -- the basic idea (complex in general, simple locally). 

Stability
\[\frac{\sum_{k=1}^n X_k - \mu n}{n^\frac{1}{\alpha}} \sim X_k\]

if $0<\alpha<1$ then ...

% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Mathematical statistics.
% Probability theory -- we have some understanding
We have an experiment, the data is random, and we attempt to to infer something about the underlying physical phenomenon.

Suppose $\brac{X_k}_{k=1}^n$ is some sample.
Using the strong law of large numbers \[\frac{\sum_{k=1}^n X_k}{n}\overset{\text{a.s}}{\to}\]
Does not tell us anything about the speed of convergence.

The model of the data plays very important role. We consider 
\[X_k \sim C + \xi_i;\quad \xi_i\sim \mathcal{D}(0,1)\,\text{iid}\]
Here All probabilistic properties are governed by the noise term.
The distribution function of the vectors of the noise terms is given by 
\[F\brac{y_1, \ldots, y_n;\Theta} \Pr\brac{\xi_k\leq y_k;\Theta} = \prod_{k=1}^n F_{\xi_k}(y_k)\]

The basic properties of distribution functions:
\begin{itemize}
	\item Normalisation: $0\leq F\leq 1$ and $\lim_{x\to -\infty} F(x)=0$ and $\lim_{x\to \infty} F(x)=1$;
	\item Right-continuous and left-limited non-decreasing map;
	\item It has a density if the Radon-Nikodym derivative exists.
\end{itemize}

Multivariate distribution:
\begin{itemize}
	\item \[p(x,y) = \frac{\partial}{\partial x}\frac{\partial}{\partial y} F(x,y)\]
\end{itemize}

% The law of large numbers
The central limit theorem

Suppose we have $\brac{\xi_k}_{k\geq1}\sim \mathcal{D}(\mu, \sigma^2)$ iid., Then
\[\frac{\frac{1}{n}\sum_{k=1}^n \xi_k - \mu}{\sfrac{\sigma}{\sqrt{n}}} \overset{\mathcal{D}}{\to} \mathcal{N}(0,1)\]

\noindent\textbf{The Chebyshev inequality}\hfill\\
Suppose $\xi$ is some random variable, then 
\[\Pr\brac{\abs{\xi-\mu}> \epsilon}\leq \frac{\Var{\xi}}{\epsilon^2}\] 

\noindent\textbf{Chernoff's inequality}\hfill\\
Suppose $\xi$ is a real-valued random variable.
\[\Pr\brac{\xi>\epsilon}\leq \inf_{\lambda>0} \Ex\exp\brac{\lambda\xi - \lambda\epsilon}\]

Since $1_\obj{\xi\>x}\leq e^{\lambda \xi - \lambda x}$ and expectations preserve monotonicity, we have for every $\lambda>0$
\[\Pr\brac{\xi>\epsilon}\leq \Ex\brac{e^{\lambda \xi - \lambda x}}\]
whence the inequality for the infumum over all $\lambda$ follows.

Similarly being a real function of a real value $\xi$ we have the following inequality
\[1_\obj{\xi\>x}\leq \frac{\abs{\xi}^p}{x^p}\]
form which Chebyshev's inequality follows as well (not only the markov's inequality).

\subsection{Random variate generation} % (fold)
\label{sub:random_variate_generation}

Suppose it is necessary to generate a random variable $\theta$ with a known distribution $F$ using the uniform random variable $U\sim\mathcal{U}\clo{0,1}$.

If the function $F$ is invertible ($F$ has no atoms), then $\eta \defn F^{-1}(U)\sim F$.

Indeed, \[\Pr\brac{\eta\leq x} = \Pr\brac{F^{-1}(U)\leq x} = \Pr\brac{U\leq F(x)} = F(x)\]

For example, for $F\sim \text{Exp}(\lambda)$ it is true that $F(x)= 1-e^{-\lambda x}$, whence \[Y \defn -\frac{1}{\lambda}\log{(1-U)}\] has is exponentially distributed. Notice, that $1-U\sim\mathcal{U}\clo{0,1}$, which implies that \[-\frac{1}{\lambda}\log U \sim F\]

The inversion method is not always convenient. Take normally distributed random variables for instance.

% subsection random_variate_generation (end)

\subsection{Box-M\"uller method} % (fold)
\label{sub:box_muller_method}

Suppose we need to generate a pair of normally distributed random variates $X,Y$.
Find the joint distribution of independent $(r,\phi)$ such that the following random
vectors have a normal join distribution and are independent
\[\xi_1 = r \cos\phi\,\text{and}\,\xi_2 = r \sin\phi\]

Consider an infinitesimal patch of a plane
\[(\xi_1, \xi_2)\in\clo{r,r+\Delta r}\times \clo{\phi,\phi+\Delta \phi}\sim
\frac{1}{2\pi} e^{-\frac{r^2}{2}} \Delta_r r\sin \Delta_\phi\]
Note that $\sin\Delta_\phi\approx \Delta_\phi$ for an infinitesimal $\Delta_\phi$. At the same time
\[\clo{r,r+\Delta r}\times \clo{\phi,\phi+\Delta \phi} \sim p(r,\phi) \Delta_r\Delta_\phi\] 

Therefore
\[p(r,\phi) = \frac{1}{2\pi} r e^{-\frac{r^2}{2}}\]

Thus the marginal distribution of $\phi$ is uniform on $\ploc{0,2\pi}$, and $F_r(x) = 1 - e^{-\frac{x^2}{2}}$.

% subsection box_muller_method (end)

Suppose we have a sample $\brac{Y_k}_{k=1}^n$ iid with $F(x)$. The empirical distribution function of $Y$ given the sample is 
\[\hat{F}_n(x) \defn \frac{1}{n}\sum_{k=1}^n 1_\ploc{-\infty, x}(Y_k)\]
The expectation of $\hat{F}_n(x)$ for any $x$ is 
\[\Ex \hat{F}_n(x) = \frac{1}{n} n \Ex1_\ploc{-\infty, x}(Y) = \Pr\brac{Y\leq x} = F(x)\]

The variance of $\hat{F}_n(x)$ at any $x$ is given by
\[\Var\brac{\hat{F}_n(x)} = \frac{1}{n^2}n \Var{1_\ploc{-\infty, x}(Y)}\]
where independence assumption has been used.
The variance on an indicator is
\[\Var{1_A(Y)} = \Ex 1_A(Y) - \brac{\Ex 1_A(Y)}^2 = \Pr(Y\in A)\brac{1-\Pr(Y\in A)}\]
Thus \[\Var\brac{\hat{F}_n(x)} = \frac{1}{n} F(x)\brac{1-F(x)}\]

Using the central limit theorem we get
\[\frac{\hat{F}_n(x) - F(x)}{\sqrt{\frac{1}{n} F(x)\brac{1-F(x)}}}\overset{\mathcal{D}}{\to} \mathcal{N}(0,1)\]
The convergence rate is $\frac{1}{\sqrt{n}}$.

There is another way to define the empirical distribution function -- though the order statistics
\[\hat{F}_n(x) = \frac{1}{n}\sum_{k=1}^n 1_\ploc{-\infty,x}(Y_{(k)})\]

\noindent\textbf{Glivenko-Cantelli theorem}\hfill\\
\[\nrm{\hat{F}_n(x) - F(x)}_\infty \overset{\text{a.s}}{\to} 0\]

If $F(x)$ is invertible, $u_k\sim\mathcal{U}\clo{0,1}$ -- iid and $Y_k\sim F$ -- iid, then it is easy to see that 
\[\Pr\brac{\nrm{\hat{F}_n(x) - F(x)}_\infty>z} = \Pr\brac{\sup_{u\in\clo{0,1}}{\hat{U}_n(u) - u}>z}\]
where $\hat{U}_n(u) = \frac{1}{n}\sum_{k=1}^n 1_\ploc{0,F^{-1}(Y_k)}(u)$.

\noindent\textbf{Kolmogorov's theorem}\hfill\\
\[\Pr\brac{\sqrt{n}\nrm{\hat{F}_n(x) - F(x)}_\infty>z} = 2\sum_{k\geq1} {(-1)}^{k-1} e^{-2k^2z^2}\]

\noindent\textbf{Dworezky-Kifer-Wolfowitz theorem}\hfill\\
\[\Pr\brac{\sqrt{n}\nrm{\hat{F}_n(x) - F(x)}_\infty>z} \leq 2 e^{-2z^2}\]

\noindent\textbf{Pike's theorem}\hfill\\
Suppose $\brac{\xi_k}_{k=1}^{n+1}\sim \text{Exp}(1)$.
The distribution of the ordered statistics coincides with 
\[U_{(k)} = \frac{\sum_{i=1}^k\xi_i}{\sum_{i=1}^{n+1}\xi_i}\]

%% See Renyi 1953 On the theory of Order Statistics

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

Suppose $\brac{X_k}_{k=1}^n$ is independent and identically distributed like Cauchy, then $\frac{\sum_{k=1}^n X_k}{n}\sim \text{Cauchy}$ and this property is known as stability.

If $p(x)\sim \frac{1}{\abs{x}^{\alpha+1}}$ for $\alpha\in\brac{0,1}$, then $\frac{\sum_{k=1}^n X_k}{n}\sim n^{\frac{1}{\alpha}-1} X_i$.

This looks very much like the renormalisation transformation in self-similarity studies.

What is the reason for this strange result? The limiting distributions for extreme values.

\subsection{Extreme value distribution} % (fold)
\label{sub:extreme_value_distribution}

Other properties of characteristic functions.

Let $X,Y$ be two random variables with distributions $F$ and $G$ respectively. Then \[\Pr\brac{\max\obj{X,Y}\leq x} = F(x)G(x)\]
since $\obj{\max{X,Y}\leq x} = \obj{X\leq x}\cap \obj{Y\leq x}$ and $X\perp Y$.

This fact can be used for deriving distribution of maxima.

Let's get back to the example of Cauchy distribution. The maximum of two Cauchy distributed random variables is distributed according to
\[F^2(x) = \brac{\frac{1}{2} + \frac{1}{\pi}\arctan x}^2\]
and $F(x)\approx 1 - \frac{1}{\pi x}$.

Indeed 
\begin{align*}
	\tan \xi
	&= \frac{\cos \frac{\pi}{2} - \xi}{\sin \frac{\pi}{2} - \xi}\\
	\tan \xi &\overset{\xi\to \frac{\pi}{2}}{=} \frac{\cos \frac{\pi}{2} - \xi \to }{\sin \frac{\pi}{2} - \xi}\sim \frac{1}{\frac{\pi}{2} - \xi}\\
	x\sim \brac{\pi\brac{1-F(x)}}^{-1} \Rightarrow F(x)\sim 1-\frac{1}{\pi x}
\end{align*}

Therefore the asymptotics of the sum is
\[F_{\sum_{k=1}^n X_i} (x) = F_{nX}(x) = F_X(\frac{x}{n})\approx 1 - \frac{n}{\pi x}\]

The largest sum component dominates the sum, and the normalisation kills off the meagre sum components and retains the scaled ``giant''.

However the maximum has the right tail asymptotically equal to
\[F_{\max_{k=1}^n X_i} (x) = \brac{F_X(x)}^n \approx \brac{1 - \frac{1}{\pi x}}^n \approx 1 - \frac{n}{\pi x}\]

% subsection extreme_value_distribution (end)

\subsection{The Fisher-Tipett-Gnedenko} % (fold)
\label{sub:the_fisher_tipett}

Gnedeko proved the exclusiveness of the limiting distributions.

Reasoning in complete analogy to the limiting stable distributions, one has to decide on the recentering and rescaling.

\noindent \textbf{Definition}\hfill\\
Let $F(x)$ be some distribution function. Define the \emph{typical maximum value} in the sample of size $n$ as the solution to the following equation: \[F(\bar{x}_{(n)}) = 1-\frac{1}{n}\]

The expected number of sample elements larger than $\bar{x}_{(n)}$ is $n\frac{1}{n}$, over the random samples of size $N$ from $F(x)$.

Let's use $X_{(n)}$ to renormalise the maxima.

\noindent\textbf{Theorem}\hfill\\
Suppose $\brac{X_k}_{k=1}^n$ is an independent sample. Denote by $X_{(k)}$ the order statistics of the sample. Recall that for all $k\leq n-1$
\[x_{(k)}\leq X_{(k+1)}\]
The order statistics are obtained via a random permutation and are dependent.

The maximum of a random sample of size $n$ is \[X_{(n)} = \max_k X_k\] and its CDF is $F_{(n)}(x) = F^n(x)$.

Let's looks for the limiting distribution of the form 
\[F_{(n)}(\xi \bar{x}_{(n)}) \overset{n\to \infty}{\to} e^{-\phi(\xi)} \]

This renormalise makes sense if $F(x)<1$ for all $x\in \Real$, for otherwise the limiting distribution is a step function.

It is more practical to consider the complimentary distribution function $\bar{F}(x) = 1-F(x)$.

\begin{align*}
	F_{(n)}(\xi \bar{x}_{(n)}) &= \brac{1-\bar{F}(\xi \bar{x}_{(n)})}^n\\
	n \ln\brac{1-\bar{F}(\xi \bar{x}_{(n)})} \to -\phi(\xi)
\end{align*}

Now if $\bar{F}(\xi \bar{x}_{(n)})\to 0$, then asymptotically 
\[- n \bar{F}(\xi \bar{x}_{(n)}) \to -\phi(\xi)\]
since $\ln (1-\epsilon)\approx -\epsilon$.

On the othe hand, by definition of $\bar{x}_{(n)}$ it is true that $
n\bar{F}(\bar{x}_{(n)}) \equiv 1$. Therefore we look for
\[\lim_{n\to \infty} - \frac{n \bar{F}(\xi \bar{x}_{(n)})}{ n\bar{F}(\bar{x}_{(n)}) }\]

Note that for all $t\in \clo{ \bar{x}_{(n)}\leq \bar{x}_{(n+1)}}$ it is true that 
\[
\frac{\bar{F}(\bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n)}) } \frac{\bar{F}(\xi \bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n)}) } \leq
\frac{\bar{F}(\xi t)}{ \bar{F}(t) } \leq
\frac{\bar{F}(\bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n+1)}) }
\frac{\bar{F}(\xi \bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n+1)}) }
\]

Where $\frac{\bar{F}(\bar{x}_{(n+1)})}{ \bar{F}(\bar{x}_{(n)}) }\to 1$ and $\frac{\bar{F}(\bar{x}_{(n)})}{ \bar{F}(\bar{x}_{(n+1)}) }\to 1$.
%% Use limsups and liminfs

Therefore
\[\phi(\xi) = \lim_{n\to\infty} \frac{\bar{F}(\bar{x}_{(n+1)})}{\bar{F}(\bar{x}_{(n+1)}) } = \lim_{t\to\infty} \frac{\bar{F}(t \xi)}{\bar{F}(t) }\]

Next notice that for any $\xi_1, \xi_2\geq 0$ it is true that
\[\phi(\xi_1\xi_2) = \lim_{t\to\infty} \frac{\bar{F}(t \xi_1\xi_2)}{\bar{F}(t\xi_1) } \frac{\bar{F}(t \xi_1)}{\bar{F}(t) }\]
whence \[\phi(\xi_1\xi_2) = \phi(\xi_1)\phi(\xi_2)\]

A continuous positive solution to this functional equation is given by $\phi(\xi) = \xi^{-\alpha}$.
In order for $\phi$ to be a meaningful solution, it must be true that the limiting function $e^{-\phi(\xi)}$ be a distribution function.

This the limiting distribution is
\[G(\xi) = e^{-\xi^{-\alpha}}\]

Therefore the tail of the distribution
\[\bar{F}(\xi) = 1 - F(\xi) = 1-e^{-\xi^{-\alpha}}\approx \xi^{-\approx}\]

The limiting distribution if called Fr\'ech\'et distribution and it emerges when the tail is polynomial and $\bar{x}_{(n)}\sim n^\frac{1}{\alpha}$.

For light tailed distributions this derivation will yield $\alpha=\infty$. This means that it is necessary to change the normalisation for this class of distributions. The scaling is $F_{(n)}(\bar{x}_{(n)} + \eta)$. In this case the limiting exponent satisfies
\[\phi(\eta) =\lim_{t\to\infty} \frac{\bar{F(t\eta)}}{\bar{F(t)}}\]
the functional equation is thus $\phi(\eta_1+\eta_2)=\phi(\eta_1)\phi(\eta_2)$.

Thus the limiting distribution is the Gumbel's distribution
\[F(\eta) = e^{-e^{-\beta\eta}}\]

If the distribution $X\sim F$ has compact support, i.e. $X_i\leq X_*$, then one can consider another random variable $Y_i = \frac{1}{x_*-X_i}$ and reduce this problem a previously solved.

Then for heavy tail $\alpha\in\ploc{0,\infty}$: Weibull's distribution
\[F(\theta) = e^{-{(x_*-\theta)}^\alpha}\]
Since the exponential decay to zero is so fast, that it would yield a step function.

% subsection the_fisher_tipett (end)

% section lecture_6 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

Suppose we have a sample  $\brac{Y_k}_{k=1}^n$ of independent And identically distributed random variables with $F(x)$.

The empirical distribution function is
\[\hat{F}_n(x) = \frac{1}{n}\sum_{k=1}^n 1_{\ploc{-\infty, x}}(Y_k)\]

The divergence of the eCDF from the true CDF with respect to the $
sup$-norm is equivalent in distribution to
\[\Pr\brac{\sup_{x}\nrm{\hat{F}_n(x) - F(x)}_\infty > z} = \Pr\brac{\sup_{u\in\clo{0,1}}\nrm{\hat{U}_n(u) - u}_\infty > z}\]
where 
\[\hat{U}_n(u) = \frac{1}{n}\sum_{k=1}^n 1_{\clo{0, u}}\brac{F^{-1}(Y_k)}\]

Kolmogorov's theorem
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}}\nrm{\hat{U}_n(u) - u}_\infty \leq z} = 2\sum_{k\geq1} {(-1)}^{k-1} e^{-2 k^2 z^2}\]


Pike's theorem
%%%%% Suppose $\brac{\xi_k}_{k=1}^{n+1}\sim \text{Exp}(1)$.
%%%%% The distribution of the ordered statistics coincides with 
%%%%% \[U_{(k)} = \frac{\sum_{i=1}^k\xi_i}{\sum_{i=1}^{n+1}\xi_i}\]


An intermediate result:
\[\lim_{n\to \infty} \Pr\brac{\sqrt{n} \sup_{x}\nrm{\hat{F}_n(x) - F(x)}_\infty > z} = \Pr\brac{\sqrt{n} \abs{W_0(t)} > z}\]

A scalar process on a probability space $\brac{\Omega, \Fcal, \Pr}$ is an $\Fcal$ measurable function $X = X(\omega, t)$ for $\omega\in \Omega$ and $t\in \Tcal$ is a non-random parameter.

A Gaussian random process is a scalar process $X_t$ \begin{itemize}
	\item $\brac{X_{t_k}}_{k=1}^p\sim \Ncal_p\brac{\brac{m(t_i)}_{i=1}^p, \Sigma^p}$ for any $\brac{t_k}_{k=1}^m$ of time slices;
	\item The covariance matrix is given by 
	\[\Sigma^p_{ij} = \Ex\brac{X_{t_i} - m(t_i)}\brac{X_{t_j} - m(t_j)}\]
\end{itemize}

A Wiener process is a Gaussian process $W_t$, $t\geq 0$ with $m(t) = 0$ and $\Ex\brac{W_t W_s} = \min\obj{t,s}$.

The Brownian Bridge is a process defined over $t\in\clo{0,1}$ by
\[W_0(t) \defn W(t) - t W(1)\]

To reiterate:
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}} \nrm{\hat{U}_n(u) - u}_\infty \leq z} \overset{n\to\infty}{\to} \Pr\brac{\sqrt{n} \sup_{u\in \clo{0,1}}\abs{W_0(u)} > z}\]

Consider the ordered statistics $\brac{U_{(k)}}_{k=1}^n$ of a uniformly distributed random variables:
\[\hat{U}_n(U_{(k)}) = \frac{k}{n}\]

Therefore 
\[\Pr\brac{\sqrt{n} \sup_{u\in\clo{0,1}} \nrm{\hat{U}_n(u) - u}_\infty \leq z}  = \Pr\brac{\sqrt{n} \max_{k=1}^n \abs{U_{(k)} - \frac{k}{n}} \leq z} \]

According to Pike's theorem
\[U_{(k)} = \frac{\sum_{i=1}^k \xi_j}{\sum_{j=1}^{n+1} \xi_j}\]

Thus 
\[\Pr\brac{\sqrt{n} \max_{k=1}^n \abs{U_{(k)} - \frac{k}{n}} \leq z}  = \Pr\brac{\max_{k=1}^n \abs{ n \sum_{i=1}^k \xi_j - k\sum_{j=1}^{n+1} \xi_j} \leq \sqrt{n} z\sum_{j=1}^{n+1} \xi_j } \]

Next
\[ = \Pr\brac{\max_{k=1}^n \abs{ \frac{1}{\sqrt{n}} \sum_{i=1}^k (\xi_j-1) - \frac{k}{n\sqrt{n}} \sum_{j=1}^{n+1} (\xi_j-1) - \frac{1}{n}} \leq \frac{1}{n} z\sum_{j=1}^{n+1} \xi_j } \]

Consider for $\brac{\xi_i}_{i\geq1}$ independent and identically distribution $\xi_i\sim\text{exp}(1)$:
\[W_n(t) = \frac{1}{\sqrt{n}} \sum^{\floor{n t}}_{i=1} (\xi_i - 1)\]

First of all $\Ex W_n(t) = 0$ and the covariance is given by
\[\Ex W_n(t)W_n(s)  = \frac{1}{n} \sum^{\floor{n t}}_{j=1} \sum^{\floor{n s}}_{i=1} (\xi_j - 1) (\xi_i - 1) = \frac{1}{n^2}
\sum^{\floor{n t}\vee \floor{n s}}_{i=1} \Ex {(\xi_i - 1)}^2 = 
\frac{1}{n} \floor{n t}\vee \floor{n s} = \floor{t}\vee \floor{s} + o(\frac{1}{n})\]

Thus from the Central Limit Theorem on $t\in \clo{0,1}$
\[W_n(t)\overset{\Dcal}{\to} W(t)\,\text{and}\, \frac{1}{\sqrt{n}} \sum_{i=1}^k (\xi_j-1) - \frac{t}{n\sqrt{n}} \sum_{j=1}^{n+1} (\xi_j-1) \to W(t)-tW(1)\]

Since 
\[\frac{1}{n} z\sum_{j=1}^{n+1} \xi_j \overset{\Pr}{\to} z\]

Thus the theorem is proved.


\subsection{Extreme Value Distribution} % (fold)
\label{sub:extreme_value_distribution}

A sample $\brac{Y_k}_{k=1}^n$ of independent and identically distributed random variables with $F(x)$. The distribution of 
\[\Pr\brac{\max_{i=1}^n Y_i \leq x} = \prod_{i=1}^n \Pr\brac{Y_i \leq x} = F^n(x)\]

There are three types of limitnig distributions of maxima (properly scaled and centred) \begin{description}
	\item[Fr\'ech\'et]\hfill\\
	\[ e^{-x^{-\alpha}} 1_{\clop{0,\infty}}(x) \]
	\item[Gumbel]\hfill\\
	\[ e^{-e^{-x}} 1_{\clop{0,\infty}}(x) \]
	\item[Weibull (Wallodi)]\hfill\\
	\[ e^{-{(-x)}^\alpha} 1_{\brac{-\infty, 0}}(x) \]
\end{description}


The normal distribution is 
\[F(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-\frac{u^2}{2}}du\]

recall the inversion method $Y_{(n)} = F^{-1}(U_{(n)})$.

Now using Pike's theorem
\[U_{(n)} = 1-\frac{ \xi_{n+1}}{\sum_{j=1}^{n+1} \xi_j}\]

Next
\[U_{(n)} = 1-\frac{ \xi_{n+1}}{n+1}\brac{1 + \frac{1}{n+1}\sum_{j=1}^{n+1} {(\xi_j-1)}}^{-1}\]

Using the CLT we get the following
\[U_{(n)} \overset{\Dcal}{\approx} 1-\frac{\xi_{n+1}}{n+1}\brac{1+o(\frac{1}{\sqrt{n+1}})}\]

So $1-U_{(n)}$ decays as $n\to \infty$, thus we apply the gaussian inversion to it.

Integrating the Gaussian distribution by parts yields
\[\int_x^\infty \frac{1}{u} de^{-\frac{u^2}{2}} =  \induc{ \frac{1}{u} e^{-\frac{u^2}{2}} }_x^\infty - \int_x^\infty \frac{1}{u^2} e^{-\frac{u^2}{2}}du\]

Then 
\[= - \frac{1}{x} e^{-\frac{x^2}{2}}
+ \int_x^\infty \frac{1}{u^3} de^{-\frac{u^2}{2}} 
= - e^{-\frac{x^2}{2}} \brac{\frac{1}{x} + o(\frac{1}{x^3})}\]

Thus
\[F(x) = 1 - \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \brac{\frac{1}{x} + o(\frac{1}{x^3})}\]

let's invert this for values of $y$ close to zero.

\[ x = F^{-1}(y) = \sqrt{ 2\log{\frac{1}{x}+o(\frac{1}{x^3})} + \log \frac{1}{1-y}\frac{1}{\sqrt{2\pi}}} = L_y(x) \]

Then 
\[L_y'(x) \leq - \frac{L_y(x)}{2x}\]

whence an approximate solution of the inversion is
\[ x\approx \sqrt{-\log{(1-y)2\sqrt{\pi}}}\]

Therefore
\[Y_{(n)} = F^{-1}(U_{(n)}) \approx F^{-1}\brac{1-\frac{\xi_{n+1}}{n+1}\brac{1+o(\frac{1}{\sqrt{n+1}})}}\]

whence 
\[Y_{(n)} \approx \sqrt{ \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } } \]

Using the first approximation of $x$ into $L_y(x)$ we can obtain a more precise approximation to $x$ and thus the final distribution:

\[ Y_{(n)} \approx \sqrt{ \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } - 2\log \log \frac{{(n+1)}^2}{\xi_{n+1}^2 \sqrt{2\pi} }  + o(1) }  \]

Setting 
\[m_n^2 \defn \log \frac{n+1}{\xi_{n+1} \sqrt{2\pi} } \]

we get 
\[Y_{(n)} \approx \sqrt{ m_n^2 - \frac{2\log \xi_{n+1}}{m_n^2} }\]

Using Taylor's expansion (? STRANGE)
\[Y_{(n)} \approx m_n\brac{ 1 - \frac{2\log \xi_{n+1}}{m_n} }\]

Thus 
\[\Pr\brac{ Y_{(n)} < z } \approx \Pr\brac{ m_n\brac{ 1 - \frac{2\log \xi_{n+1}}{m_n} } < z } \]


\textbf{CHECK THE CONSTANTS!!!!!}

Redoing this: the first approximation id $x$ is
\[x_0 \approx \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} }\]

Then 
\[x = \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} + \log\brac{ \frac{1}{x} + o(\frac{1}{x^3}) }}\]

Substituting $x_0$ instead of $x$ and neglecting the infinitesimal yields:
\[ x \approx \sqrt{ \log\frac{1}{(1-y)\sqrt{2\pi}} - \frac{1}{2} \log\log\frac{1}{(1-y)\sqrt{2\pi}} }\]

Then 
\[ Y_{(n)}^2 \approx \brac{
\log\frac{n+1}{\xi_{n+1}\sqrt{2\pi}}
- \frac{1}{2} \log\log\frac{n+1}{\xi_{n+1}\sqrt{2\pi} + o(1)}
}\]

So
\[ Y_{(n)} \approx \sqrt{ - \log \xi_{n+1} + \log\frac{n+1}{2\log{(n+1)} \sqrt{8\pi} + o(1) } }\]

Whence
\[ Y_{(n)} \approx \brac{-\log \xi_{n+1} + m_n + o(1) }^\frac{1}{2}\]

Thus using the Taylor once again
\[ Y_{(n)} \approx m_n^\frac{1}{2} \brac{-\frac{\log \xi_{n+1}}{m_n} + 1 }\]

Next 
\[ \sqrt{m_n} Y_{(n)} - m_n\approx - \log \xi_{n+1}\]

Thus
\[ \Pr\brac{\sqrt{m_n} \brac{Y_{(n)} - \sqrt{m_n}}} \approx \Pr\brac{- \log \xi_{n+1} < z }\]

So
\[\Pr\brac{- \log \xi_{n+1} < z } = \Pr\brac{\xi_{n+1} > e^{-z} } = e^{-e^{-z}}\]

And $m_n\approx \sqrt{ 2 \log n}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Laplace distribution
\[F(x) = 1-e^{-\frac{x}{2}} 1_{x\geq 0} + e^{\frac{x}{2}} 1_{x\leq 0}\]

The inverse is given for $y\in \clo{\frac{1}{2},1}$ by
\[ F^{-1}(y) = - \log 2(1-y)\]

Since $\max_{i=1}^n Y_i \sim \log\frac{n+1}{2\xi_{n+1}}$ 
we have
\[\Pr\brac{ \log\frac{n+1}{2} - \log \xi_{n+1} < z } \approx e^{-e^{-z}}\]

% subsection extreme_value_distribution (end)

% section lecture_7 (end)

\section{LEcture \# 8} % (fold)
\label{sec:lecture_8}

\subsection{Discrete extreme value theory} % (fold)
\label{sub:discrete_extreme_value_theory}
Suppose we have a collection of $\brac{X_k}_{k=1^n}$ independent and identically distributed with values $\brac{1, -1}$ with probabilities $\brac{\frac{1}{2},\frac{1}{2}}$.

Define the cumulative sum $S_n = \sum_{k=1}^n X_k$ -- this is a discrete 1-D random walk.

According to the Law of large numbers the mean displacement 
\[\frac{S_n}{n} \overset{\Pr}{\to} 0\]

An extreme value is the defined by the event $E_\alpha \defn \obj{ \sfrac{S_n}{n} = \alpha } = \obj{ S_n = n\alpha }$ for $\alpha\in \clo{-1,1}$.

Fluctuations around $0$ have of the order $\sqrt{n}$, that is why a large deviation is defined as $n\alpha$.

The number of upward and downward steps is equal to $N_+ = \frac{1+\alpha}{2}n$ and $N_- = \frac{1-\alpha}{n}$ respectively. With $S-n = N_+-N_-$

\subsubsection{Lazy random walk} % (fold)
\label{ssub:lazy_random_walk}

Suppose $X_k = 0$ with positive probability $p_0$, while $X_k = 1$ with $p_+$ and $X_k=-1$ with $p_-$. 

Then $\Ex \frac{S_n}{n} = p_+ - p_-$, where $S_n = \frac{N_+-N_-}{n}$ with $N_++N_0+N_- = n$ and $N_i$ is the number of steps in the direction $i$.

Even the most improbable event occurs with some highly probable chances.

% subsubsection lazy_random_walk (end)

\subsubsection{General theory} % (fold)
\label{ssub:general_theory}

Suppose $\Acal$ is some finite alphabet with $\abs{\Acal}$.

Consider a probability distribution $P = \brac{P_a}_{a\in \Acal}$ on $\Acal$.

Consider a string $s\in \Acal^n$ and define $n_a(s)$  as the $\abs{\obj{\induc{i=1,\ldots,n} s_i = a }}$. Then $\sum_{a\in\Acal} n_a(s) = n$.

Suppose the symbols in $s$ are independent and identically distributed random variables on $\Acal$ with probability distribution $P$.

Consider the probability space $\Acal^n$ with the product probability measure $\Pr(s) = \prod_{a\in \Acal} p_a^{n_a(s)}$.
Use the discrete $\sigma$-algebra.

One-point distribution of one symbol on $k=1,\ldots,n$ (as opposed to distributions of finite vectors). 

Is the random process is stationary, then the probability of encountering a symbol $a\in \Acal$ is independent of is place in $s$.

How does the typical and atypical string look like. Typicality is defined as the ratio of occurrences of a particular symbol.

The number of strings grows exponentially with $n$.

A characteristic vector of string $s\in\Acal^n$ is the vector $n = \brac{n_a(s)}_{a\in \Acal}$.

The number of string with a given characteristic vector $m = \brac{m_a}_{a\in \Acal}$ is given by 
\[\Pr(\vec{m}) \defn \frac{n!}{\prod_{a\in \Acal} m_a!}\] 

The number of different characteristic vectors is given by 
\[\frac{(n+\abs{\Acal}-1)!}{(\abs{\Acal}-1)!n!} = \frac{(n+1)\ldots (n+\abs{\Acal}-1)}{(\abs{\Acal}-1)!}\]
the number of combinations of size $n$ of $k$ symbols with repetition.
Thus the number of possible characteristic types is polynomial in the size of the string. One can draw a conclusion that the complexity of strings grows slower than the size of the string.

Consider a characteristic vector $n$, then the probability of a string of this type is \[\Pr\Big(\Big. s \big.\big\rvert \vec{m}\Big.\Big) = \prod_{a\in \Acal} p_a^{n_a}\]
and the number of strings of the type is $\Pr(\vec{m})$
Thus the chance of a particular string is
\[\Pr(s) = \Pr\Big(\Big. s \big.\big\rvert \vec{m}\Big.\Big) \Pr(\vec{m}) = \prod_{a\in \Acal} p_a^{n_a}\]

Since the Stirling's formula is $n! = n^n e^{-n} \sqrt{2 \pi n} \big(1+o(\frac{1}{n})\big)$, we have
\begin{align*}
	\frac{n!}{\prod_{a\in \Acal} m_a!} \prod_{a\in \Acal} p_a^{n_a}
	& \approx \frac{n^n e^{-n} \sqrt{2 \pi n} }{\prod_{a\in \Acal} m_a^{m_a} e^{-m_a} \sqrt{2 \pi m_a} } \prod_{a\in \Acal} p_a^{n_a} \\
	& \approx \prod_{a\in \Acal} \brac{p_a \frac{n}{m_a}}^m_a \\
	& = \text{exp}\brac{ n \big( \sum_{a\in \Acal} \eta_a \ln\frac{p_a}{\eta_a} \big) } = e^{-n\text{KL}\big( \eta \lvert \rvert P\big)}
\end{align*}

where $\eta_a = \frac{m_a}{n}$. 

The relative entropy, or Kullback-Leibler entropy, describes the differences in distributions $\eta$ with respect to $P$:
\[\text{KL}\big( \eta \lvert \rvert P\big) = \sum_{a\in \Acal} \eta_a \ln \frac{\eta_a}{p_a} = \sum_{a\in \Acal} p_a \frac{\eta_a}{p_a} \ln \frac{\eta_a}{p_a} \]
Where $\frac{\eta_a}{p_a}$ is the Radon-Nikodym derivative of $\eta$ relative to $p$.

For two measures $Q,P$ on the same space, if $Q<<P$ then
\[\text{KL}\big( Q \lvert \rvert P\big) = \int \frac{dQ}{dP} \ln \frac{dQ}{dP} dP\]
where $\frac{dQ}{dP}$ is the Radon-Nikodym derivative of $Q$ with respect to $P$ :
\[Q(E) = \int_E \frac{dQ}{dP} dP\]

The Radon-Nikodym derivative of $Q$ with respect to $P$ when $Q,P<<dx$ and $Q<<P$ is
\[\int_E \frac{q}{p} dP = \int_E \frac{q}{p} p dx = \int_E q dx = \int_E dQ = \int_E \frac{dQ}{dP} dP\]
whence $\frac{dQ}{dP} = \frac{q}{p}$ $P$-almost surely. (and thus $dx$-almost surely).

So far we have got the following result
\begin{align*}
	\Pr\brac{\brac{n \eta_a}_{a\in \Acal}} &\approx e^{- n \text{KL}\brac{\eta \lvert\rvert p}}
\end{align*}

If $\Phi(x)$ is a convex function, then for all $x,x_0\in \Real^d$ the following is true
\[ \Phi(x) \geq \Phi(x_0) + \nabla \Phi(x_0) \big(x-x_0\big) \]

If $\Phi(x) = \sum_{k=1}^d x_k \ln x_k$ for $x\in \Real_+^d$, then $\Phi(x)$ is convex.

Around $\brac{p_a}_{a\in \Acal}$:
\[\sum_{a\in \Acal} p_a \ln p_a + \sum_{a\in \Acal} \big(1 + \ln p_a \big) \big(x_a - p_a\big)\]
whence $\sum_{a\in \Acal} x_a \ln\frac{x_a}{p_a} \geq 0$.

% The distribution of possible probability vectors $\brac{p_a}_{a\in \Acal}$ is given by 
% \[\text{Dir}\brac{\brac{p_a}_{a\in \Acal}} = \frac{\prod_{a\in \Acal} \Gamma(\alpha_a)}{\Gamma\brac{\sum_{a\in \Acal} \alpha_a }}\prod_{a\in \Acal} p_a^{\alpha_a-1}\]

Let's refine the approximation above.

Consider the 
\[-\frac{1}{n}\ln\Pr\brac{\brac{n \eta_a}_{a\in \Acal}} \overset{n\to\infty}{\to} \text{KL}\brac{\eta \lvert\rvert p} \]

\noindent\textbf{Sanov's theorem}\hfill\\
Suppose $A$ is a closed subset of a probability simplex $S_\Acal$. Then 
\[-\frac{1}{n}\ln \Pr\brac{A}\to \min_{\eta\in A} \text{KL}\brac{\eta \lvert\rvert p} \]
and inside $A$ the conditional probability concentrates around the points where the minimum is achieved.

% subsubsection general_theory (end)

% subsection discrete_extreme_value_theory (end)

\subsection{Continuous extreme values theory} % (fold)
\label{sub:continuous_extreme_values_theory}

\subsubsection{Extreme values: Cram\'er's theorem} % (fold)
\label{ssub:extreme_values_cramer_theorem}

One of the earliest theorems on extreme values (1938).

Is investinages the distribution of the normalised sum of independent and identically distributed random variables $\brac{X_k}_{k=1}^n$.

Again the law of large numbers tells us that
\[\frac{\sum_{k=1}^n X_k}{n} \to \Ex X = \mu\]

A large deviation is the following event :
\[\obj{ \sum_{k=1}^n X_k - n\mu \neq 0 }\]

The intuition suggests that the following asymptotics holds
\[\Pr\brac{a < \bar{X}_n < b} \sim e^{-n \rho(a,b)}\] as $n \to \infty$ where $\bar{X}_n = \frac{\sum_{k=1}^n X_k}{n}$.

Let's derive heuristically a general form of $\rho$.

If one has a sub-additive sequence, them there is one general trick in approaching the limit.

We want to show that 
\[-\frac{1}{n}\ln \Pr(a<\bar{X}_n<b) \overset{n\to\infty}{\to} \rho(a,b)\]

Observe that for $n,m\geq 1$
\[-\ln \Pr(a<\bar{X}_{n+m}<b) \leq - \ln \Pr(a<\bar{X}_n<b) - \ln \Pr(a<\bar{X}_m<b)\]

Indeed, the sequence of these logarithms is subadditive.
For this event it is true that
\[a<\bar{X}_{n+m}<b \Leftrightarrow a<\frac{n \bar{X}_n + m \bar{X}_m}{n+m}<b \]
and this is true when both $a<\bar{X}_n<b$ and $a<\bar{X}_m<b$ due to convex linear combination.

We have 
\[\Pr\brac{a<\bar{X}_m<b} = \Pr\brac{a<\frac{\sum_{k=n+1}^{n+m}X_k}{m}<b}\]
since $X_k$ are identically distributed.

Thus 
\[\Pr\brac{a<\bar{X}_{n+m}<b} \geq \Pr\brac{a<\bar{X}_n<b \cup a<\bar{X}_m<b} = \Pr\brac{a<\bar{X}_n<b}\Pr\brac{a<\bar{X}_m<b} \]

whence 
\[\ln \Pr\brac{a<\bar{X}_{n+m}<b} \geq \ln \Pr\brac{a<\bar{X}_n<b} + \ln\Pr\brac{a<\bar{X}_m<b} \]

and
\[-\ln \Pr\brac{a<\bar{X}_{n+m}<b} \leq -\ln \Pr\brac{a<\bar{X}_n<b}  - \ln\Pr\brac{a<\bar{X}_m<b} \]

Suppose $\brac{S_k}_{k\geq1}$ is a subadditive sequence, i.e $S_{n+m}\leq S_n + S_m$, then there exist $\lim_{n\to \infty} \frac{S_n}{n}$.

Indeed, fix some arbitrary $n_0\geq1$. Then for any $n > n_0$ it is true that $n = q n_0 + r$.  Hence 
\[S_n \leq S_{n-n_0} + S_{n_0}\leq \ldots \leq S_{n-q n_0} + q S_{n_0} = S_r + q S_{n_0}\]

Therefore 
\[S_n \leq \frac{S_r}{n} + \frac{q}{q n_0 + r}S_{n_0}\leq \frac{S_r}{n} + \frac{S_{n_0}}{n_0}\]

thus
\[\bar{S} = \limsup_{n\to \infty} \frac{S_n}{n} \leq \limsup_{n\to \infty} \frac{S_r}{n} + \frac{S_{n_0}}{n_0} = \frac{S_{n_0}}{n_0}\]

Therefore $\bar{S}\leq \frac{S_{n_0}}{n_0}$. Since $n_0$ is arbitrary, 
\[\liminf_{n_0\to\infty} \frac{S_{n_0}}{n_0} \geq \bar{S}\]

But $\frac{S_{n_0}}{n_0}$ is the same sequence as $\frac{S_n}{n}$! Now since $\liminf a_n \geq \limsup a_n$ for any sequence $a_n$, it must be true that
\[\bar{S}\leq \liminf_{n\to\infty} \frac{S_n}{n} \leq \limsup_{n\to\infty} \frac{S_n}{n} \leq \bar{S}\]

Given the above observation we have the existence
\[-\frac{1}{n}\ln \Pr\brac{a<\bar{X}_n<b} \to \rho(a,b)\]

We want not only a result concerning the density of the limiting distribution of the mean, which by the LLN is \[p_{\bar{X}_n}(x)\to \delta(x-\mu)\]
but also some speed of convergence:
\[p_{\bar{X}_n}(x)\sim e^{-n \rho^*(x)}\]

\[ -\frac{1}{n} \ln \int dx \]

Then 
\[-\frac{1}{n}\ln \Pr\brac{a<\bar{X}_n<b} \to \min_{x\in (a,b)} \rho^*(x)\]

The characteristic function is
\[\phi_{\bar{X}_n}(s) = \brac{\phi_X\big(\frac{s}{n}\big)}^n = e^{n\eta(\frac{s}{n})}\]

Now the inverse Fourier transform is 
\[p_{\bar{X}_n}(x) = \frac{1}{2\pi}\int e^{-isx } \phi_{\bar{X}_n}(s) ds = \frac{1}{2\pi}\int e^{-isx + n \eta(\frac{s}{n})} ds\]

whence
\[p_{\bar{X}_n}(x) = \frac{n}{2\pi}\int e^{n\brac{-iux + \eta(u)}} du\]

\noindent \textbf{Saddle point method} \hfill\\
Suppose we want to integrate 
\[\int_\Real e^{n\Phi(u)} du\]
where $\Phi$ is some analytic function.

Now, obviously 
\[\Phi(u) = \Re \Phi(u) + i \Im\Phi(u)\]
with the imaginary part governing its phase shift, while the real part influencing the amplitude.

If $\Im\Phi(u)$ varies a lot along a contour, then due to high oscillations they will contribute a small value to the integral.

Thus we want to deform the contour, so that the oscillations are insignificant.

Thus we must deform the contour so that it passed through the point $u^*$ of $\frac{d}{d z}\Phi(z) = 0$. The real and imaginary parts are harmonic functions due to Cauchy-Riemann conditions.

The imaginary part of $\Phi(z)$ must be locally constant.

Using thus deformation
\[\int_\Real e^{n\Phi(u)} du \approx \int_{\epsilon(u^*)} e^{n\brac{\Phi(u^*) + \frac{1}{2}\Phi''(u^*) \brac{u-u^*}^2}} du\]

whence 
\[\approx e^{n\Phi(u^*)} \int_{\epsilon(u^*)} e^{\frac{n}{2}\Phi''(u^*) {(u-u^*)}^2} du \approx e^{n\Phi(u^*)} \sqrt{\frac{2\pi}{n\abs{\Phi''(u^*)}}}\brac{1 + O(\frac{1}{n})}\]

Deformation $v\in \Cplx$ : $u = u^* + t v$.

\noindent \textbf{Getting back to Cram\'er} \hfill\\

Suppose the Laplace transform of the distribution of iid $X_k$ is given by
\[e^{\kappa(s)} = \int_{-\infty}^{+\infty} e^{sx} p(x) dx\]
exists for $s$ in some interval $\brac{\alpha,\beta}$.

And $p(x) \sim e^{-\beta x}$ for $x\to \beta-$, $p(x) \sim e^{-\alpha x}$ for $x\to \alpha+$. This condition is stronger than what is required for the LLN.

We need the asymptotics of
\[p_{\bar{X}_n}(x) = \frac{n}{2\pi}\int e^{n\brac{-iux + \eta(u)}} du\]
Note that $\kappa(is) = \eta(s)$ for all $\Re s \in \brac{\alpha, \beta}$.

Thus $\eta(s)$ is defined when $\Im s\in \brac{-\beta, -\alpha}$ and the integral is analytic.

The largest value of $\Re \eta(s)$ on $\Im s = \xi \in \brac{-\beta, -\alpha}$ is attained when $\Re s = 0$.

Then $\phi(u) = e^{\eta(u)}$ implies that $\abs{\phi(u)} = e^{\Re \eta(u)}$. However 
\[\abs{\phi(u)} = \abs{\int_\Real e^{iux} p(x) dx} = e^{-\Im u} \abs{ \int_\Real e^{i\Re u x} p(x) dx } \leq e^{-\Im u} 1 \]

Second observation: $\kappa(s)$ is convex and $\lim_{s\to \alpha+} \kappa(s) = \lim_{s\to \beta-} \kappa(s) = +\infty$.

Indeed, 
\begin{align*}
	1 &= \int e^{sx - \kappa(s)} p(x) dx \\
	e^{\kappa(s)} \kappa'(s) &= \int xe^{sx} p(x) dx \\
	\kappa'(s) &= \int xe^{sx - \kappa(s)} p(x) dx = \Ex_s X
\end{align*}

Where $\Ex_s$ is the expectation with respect to a distribution with density $e^{sx - \kappa(s)} p(x)$.

Then
\begin{align*}
	e^{\kappa(s)} \kappa'(s) &= \int xe^{sx} p(x) dx \\
	e^{\kappa(s)} \kappa''(s) + e^{\kappa(s)} \brac{\kappa'(s)}^2 &= \int x^2 e^{sx} p(x) dx \\
	\kappa''(s) + \brac{\kappa'(s)}^2 &= \int x^2 e^{sx-\kappa(s)} p(x) dx \\
	\kappa''(s) + \brac{\kappa'(s)}^2 &= \Ex_s X^2 \\
	\kappa''(s) &= \Ex_s X^2 - \brac{\Ex_s X}^2\geq 0
\end{align*}

Therefore $\kappa(s)$ can be described as an envelope of tangents:
\[ \kappa(s) = \max_{y} \brac{sy - \kappa^*(y)} \]
where $\kappa^*(y)$ is the Legendre's transform of $\kappa(s)$:
\[ \kappa^*(x) = \max_{s} \brac{sx - \kappa(y)} \]

Recall 
\[\Phi(u) = - iux +\eta(u)\]

From the first observation it follows that $u^* = -is^*$, where
\[s^* = \min_{s}\Phi(-is) = \min_{s} -sx + \kappa(s) = - \kappa^*(x)\]

Therefore we get the following asymptotics
\[p_{\bar{X}_n}(x) = e^{-n\kappa^*(x)} \sqrt{\frac{n}{2\pi \abs{\kappa''(s^*)}}}\brac{1+O(\frac{1}{n})}\]

where $\kappa^*(x)$ is the Legendre transform of the logarithm of the Laplace transform of the original density.

% subsubsection extreme_values_cramer_theorem (end)

% subsection continuous_extreme_values_theory (end)

% section lecture_8 (end)

\section{Lecture \# 9} % (fold)
\label{sec:lecture_9}

Fractals and the multifractal formalism reside at the intersection of the measure theory and geometry. In 60s it it was discovered that there are fractal (self-similar) models in physics.

There are different definitions of fractal dimensions.
Dimensions in the topological sense is different from some intuitively similar dimensionality measure.

Consider a fyord. What is its coastline length? Given different resolution, the result will converge to the true length.

\subsection{Koch's snowflake} % (fold)
\label{sub:koch_s_snowflake}

Is an iterative process which on the $n+1$-th step adds an equilateral triangle at the middle third of each edge of the flake at the $n$-th integration.

\begin{tabular}{c|c|c}
$n$ & \# sides & perimeter \\
$0$ & $3$ & $3$ \\
$1$ & $3\cdot 4$ & $3 \cdot 4 \frac{1}{3}$ \\
$2$ & $3\cdot 4^2$ & $3 \cdot 4^2 \frac{1}{3^2}$ \\
$\vdots$ & $\cdots$ & $\cdots$\\
$k$ & $3 \cdot 4^k$ & $3 \cdot 4^k \frac{1}{3^k} = \frac{4^k}{3^{k-1}}$
\end{tabular}

% subsection koch_s_snowflake (end)

How could one define a value for dimensionality of a set geometrically?

Splitting a region into a finite number and counting how to reconstruct it won't do.


Take a bounded line segment and cover it with a covering of some $\epsilon$ templates, $\epsilon$-balls, for example. This would require a certain finte amount of templates $N_\epsilon$. Then $N_\epsilon\propto \epsilon^{-1}$.

For a bounded planar region, we have $S_\epsilon\propto \epsilon^{-2}$.

Consider a metric space $(X,d)$ and a bounded set $M\subseteq X$: the diameter of a set is finite. The box dimensionality of a set is
\[d_B(M)\defn -\lim_{\epsilon\to 0} \frac{\ln N_\epsilon(M)}{\ln \epsilon}\]
where $N_\epsilon$ is the exact lower bound of the number of templates across all coverings of $M$ with templates of size $\epsilon$. Templates in a metric space are closed $\epsilon$-balls.

The Box dimensionality of the Koch's snowflake is
\begin{align*}
	N_\epsilon &= 3 4^n = 3 4^\frac{-\ln \epsilon}{\ln 3}\\
	d_B &= -\lim_{\epsilon\to 0} \frac{\ln 3 - \frac{\ln \epsilon}{\ln 3} \ln 4}{\ln \epsilon} = \frac{\ln 4}{\ln 3} > 1
\end{align*}
where the standard $\epsilon$-ball template was used.

But the box dimensionality is imperfect. Is there a better one?

\subsection{Hausd\"orf dimensionality} % (fold)
\label{sub:hausdorf_dimensionality}

Consider a unit interval $\clo{0,1}$ and a set 
\[M \defn \obj{ \induc{ n^{-\alpha} } n\geq 1}\]
for some $\alpha>0$.

Let's pick a standard template and some optimal $\epsilon$ cover.

Cover the set thusly : \begin{itemize}
	\item if the points spaced farther than $\epsilon>0$ apart should be covered with a separate $\epsilon$-ball;
	\item $\bar{n}_\epsilon$ points clustering near zero are covered by a single common $\epsilon$-ball.
\end{itemize}
The estimate is $\bar{n}_\epsilon ^ {-\alpha} < \epsilon$, and then $\bar{n}_\epsilon = O\big(\epsilon^\frac{1}{\alpha}\big)$. Thus $N_\epsilon = C\epsilon^\frac{1}{\alpha}$.

A better estimate is 
\[\frac{1}{n_*^\alpha} - \frac{1}{(n_*+1)^\alpha}\sim \frac{\alpha}{n_*^{\alpha+1}} < \epsilon\]
whence $n_*\sim \epsilon^\frac{-1}{\alpha+1}$ and $N_\epsilon > O(\epsilon^\frac{-1}{\alpha+1})$.

This implies that $d_B(M) \geq \frac{1}{\alpha+1}>0$.
However a countable set of points clustering at zero seems to have $0$-dimensionality.

\noindent\textbf{Definition}\hfill\\

Consider a bounded set $M\subseteq \Real^n$. Then for $\alpha>0$ the Hausd\"orf dimensionality is 
\[H_\alpha(M) = \lim_{\epsilon\downarrow 0} \inf_{\text{cover}} \sum_{k} \text{diam}(S_k)^\alpha\]
The limit inferior is taken over all finite or countable covers $S_k$ such that $S_k$ is an arbitrary subset with diameter less that $\epsilon$, where \[\text{diam}(A) \defn \sup_{x,y\in A} d(x,y)\]

Any procedure of measuring a two-dimensional set by a one dimensional measure should yield $\infty$.

Bijection approach.
A bijection must honour some geometric structure (a diffeomorphism -- a deformation when not only things do not get torn apart but also do not become broken by wild bending).

It is necessary to preserve some kind Lipschitz condition on the space (?).

The Hausd\"orf measure is additive. It is necessary to check that
the infimums are additive when the sets are disjoint.

If $\alpha<\beta$ then $H_\alpha(M) \geq H_\beta(M)$.
Indeed, consider some covering $\brac{S_k}$. Then 
\[\sum_k \text{diam}(S_k)^\alpha \geq \sum_k \text{diam}(S_k)^\beta\]

If $H_\alpha(M) > 0$ for some $\alpha$ then $H_\beta(M) = +\infty$ for all $\beta<\alpha$.
If $H_\alpha(M) < \infty$ for some $\alpha$ then $H_\beta(M) = 0$ for all $\alpha<\beta$.

Suppose $H_\alpha(M) < +\infty$, then there is an $\epsilon>0$ and a cover $S_k$ such that 
\[\sum_k \text{diam}(S_k)^\alpha < \infty\]

\[\text{diam}(S_k)^\beta \leq \epsilon^{\beta-\alpha} \text{diam}(S_k)^\alpha \]
since $\text{diam}(S_k)^\beta = \text{diam}(S_k)^{\beta-\alpha}\text{diam}(S_k)^\alpha$.
Thus
\begin{align*}
	\sum_k \text{diam}(S_k)^\beta &\leq \epsilon^{\beta - \alpha} \sum_k \text{diam}(S_k)^\alpha\\
	\inf \sum_k \text{diam}(S_k)^\beta &\leq \epsilon^{\beta - \alpha} \sum_k \text{diam}(S_k)^\alpha\\
	\inf \sum_k \text{diam}(S_k)^\beta &\leq \epsilon^{\beta - \alpha} \inf \sum_k \text{diam}(S_k)^\alpha\\
	H_\beta(M) &\leq \epsilon^{\beta - \alpha} H_\alpha(M)
\end{align*}

Thus there exists a unique value of $\alpha$ such that $H_\alpha(M) \in (0,\infty)$. Put 
\[\alpha_0(M) = \sup_{\beta>0} \obj{ H_\beta(M) = +\infty }\]
or alternatively
\[\alpha_0(M) = \inf_{\beta>0} \obj{ H_\beta(M) = 0 }\]

\noindent\textbf{Definition}\hfill\\
Thus the Hausd\"orf dimensionality is defined as 
\[d_H(M) \defn \alpha_0(M)\]

In fact $H_\alpha$ defines a measure on Borel sets of $(X, d)$. It coincides with the Lebesgue measure $dx^n$ if $\alpha = n$.

% subsection hausdorf_dimensionality (end)

% section lecture_9 (end)

\section{Lecture \# 10} % (fold)
\label{sec:lecture_10}

Consider a sample $\brac{X_k}_{k=1}^n$ of iid random variables distributed according to $F(x)$. Let $X_{(n)} \defn \max_{k=1,\ldots,n} X_k$. Using the inversion method the uniform variables are defined as $X_k = F^{-1}(U_k)$. Thus $\brac{U_k}_{k=1}^n\sim \mathcal{U}\clo{0,1}$.

Using Pike's theorem
\[U_{(n)} = 1 - \frac{e_{n+1}}{n+1}\brac{1+O(\frac{1}{\sqrt{n+1}})}\] with $e_{n+1}\sim\text{exp}(1)$.

Then we found $F^{-1}(1-z)$ when $z\to 0$.

Cauchy distribution:
\begin{align*}
	p_X(x) &= \frac{1}{\pi(1+x^2)} \\
	F(x) &= \frac{1}{2} + \frac{1}{\pi} \text{arctg}(x) \\
	F^{-1}(y) &= \text{tg}\big(\pi (y-\frac{1}{2}) \big) \\
\end{align*}

Now \begin{align*}
	F^{-1}(1-z)
	& = \text{tg}\big(\pi (\frac{1}{2}-z) \big)\\
	& = \frac{\sin(\frac{\pi}{2} - \pi z)}{\cos(\frac{\pi}{2} - \pi z)}\\
	& = \frac{\cos(\pi z)}{\sin(\pi z)} \sim \frac{1}{\pi z}
\end{align*}
thus 
\[X_{(n)} = F^{-1}\big( 1 - (1-U_{(n)}) \big) = \approx \frac{n+1}{\pi e_{n+1}}\]

Thus 
\[\lim_{n\to \infty}\Pr\big( \frac{\pi}{n}\max_{k=1,\ldots,n}X_k < x \big) = \Pr\big(\frac{1}{e_{n+1}} < x\big) = e^{-\frac{1}{x}} \approx 1-\frac{1}{x}\]

Further consider
\[S_n = \frac{1}{n}\sum_{k=1}X_k\]
then using the characteristic function we get $F_{S_n}(x) \sim \text{Cauchy}$, whence the sample mean value of Cauchy distributed random variates is dominated by the maximum value.

\subsection{Subgaussain random variables} % (fold)
\label{sub:subgaussain_random_variables}


Consider $\brac{Y_i}_{i=1}^n$  independent with $\Ex Y_i = 0$ and $\Ex Y_i^2 = 1$, then
\[\Pr\brac{ \frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i > x } \to \frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-\frac{u^2}{2}}du \]

However
\begin{align*}
	\frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-\frac{u^2}{2}}du = \frac{1}{\sqrt{2\pi}} \int_0^\infty e^{-\frac{(u+x)^2}{2}}du \leq \frac{e^{-\frac{x^2}{2}}}{2}
\end{align*}
for some $n\geq n_0(x)$.

Consider independent $\brac{Y_i}_{i=1}^n$ such that they are subgaussian: $\exists \sigma^2 > 0$ with \[\Ex\big( e^{\lambda Y_i} \big) \leq \text{exp}\big( \frac{\sigma^2 \lambda^2}{2} \big)\]
then 
\[\Pr\brac{ \frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i > x }\leq e^{-\frac{x^2}{2}}\]

Indeed using Chernoff's inequality
\[\Pr\brac{\xi>\epsilon}\leq \inf_{\lambda>0} \Ex\exp\brac{\lambda\xi - \lambda\epsilon}\]

we get
\[\Pr\brac{\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i>\epsilon}\leq \inf_{\lambda>0} \exp\brac{ - \lambda\epsilon} \Ex\exp\brac{\lambda\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i}\]

since $Y_i$ is subgaussian, we have by the independence
\[\Ex\exp\brac{\lambda\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i} = 
\prod_{i=1}^n \Ex\exp\brac{\frac{\lambda}{\sqrt{n}} Y_i} \leq 
\text{exp}\big( n \frac{\sigma^2 \lambda^2}{2n} \big)
\]

whence, minimizing
\[\inf_{\lambda>0} \Ex\exp\brac{\lambda\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i} \leq \ldots\]
and thus $\lambda = \frac{x}{\sigma^2}$

\noindent\textbf{H\"oefding inequality}\hfill\\
Consider a random variable $\xi$ with $\Ex \xi = 0$ and $\xi\ni\clo{a,b}$ almost surely.
\[\Ex\brac{\lambda \xi}\leq \exp\big( \frac{\lambda^2(b-a)^2}{8} \big)\]

Indeed, put
\[\phi(\lambda) = \log \Ex\big(\lambda \xi\big) = \log \int_a^b e^{\lambda x} p(x) dx\]

Differentiating with respect to $\lambda$:
\[\phi'(\lambda) = \frac{\int_a^b x e^{\lambda x} p(x) dx}{\int_a^b e^{\lambda x} p(x) dx}\]

and 
\[\phi''(\lambda) = \frac{\int_a^b x^2 e^{\lambda x} p(x) dx}{\int_a^b e^{\lambda x} p(x) dx} - \bigg( \frac{\int_a^b x e^{\lambda x} p(x) dx}{\int_a^b e^{\lambda x} p(x) dx} \bigg)^2\]

Notice that
\[f(x) = \frac{\int_a^b e^{\lambda x} p(x) dx}{\int_a^b e^{\lambda x} p(x) dx}\]

is a density function of a random variable $\eta\in \clo{a,b}$. Furthermore $\Ex \eta = \phi'(\lambda)$ and $\Var \eta^2 = \phi''(\lambda)$. The second moment is given by
\[\Ex \eta^2 = \phi''(\lambda) + \big(\phi'(\lambda)\big)^2\]

Now
\[\bigg(\eta - \frac{b-a}{2}\bigg)^2 \leq \bigg(\frac{b-a}{2}\bigg)^2\]

whence 
\[\phi''(\lambda) \leq \bigg(\frac{b-a}{2}\bigg)^2 \]

integrating $\phi''$ twice over $\clo{0,\lambda}$ yields
\[\phi(\lambda) \leq \frac{\lambda^2}{2}\bigg(\frac{b-a}{2}\bigg)^2\]

which implies that $\xi$ is subgaussian.

% subsection subgaussain_random_variables (end)

\subsection{Estimation} % (fold)
\label{sub:estimation}

Consider $\brac{\xi_i}_{i=1}^n$ iid, and let $Y_i = \mu + \xi_i$, $\mu\in \Real$ -- unknown. Let $p_\xi(x)$ be the density function of $\xi$.

The joint density of observed $\brac{Y_i}_{k=1}^n$ is 
\[\prod_{i=1}^n p_\xi(y_i - \mu)\]

Suppose $p_0(x)$ is the true unknown density of $Y_i$ which we wish to approximate with a parametric family of distributions.

The empirical CDF is given by
\[\hat{F}_n(y) = \frac{1}{n}\sum_{k=1}^n 1_{\ploc{-\infty, y}}(y_i)\]

use a generalised function to compute a generalised derivative of the simple step function $\hat{F}_n$:
\[ \hat{p}(y) = \frac{1}{n} \sum_{k=1}^n \delta_y(y_i) \]

% a linera operator over the space of functions with finite support

The $\delta$ method, the method of moments
\[M(p_0) = \int_{-\infty}^\infty x p_0(x) dx\]

In general we wish to estimate some $\Phi(p_0)$. With this method we do this by
\[\hat{\Phi}\big((y_i)_{i=1}^n\big) = \Phi\big(\hat{p}(\cdot)\big)\]

Examples
\begin{itemize}
	\item $M(p_0)$: $\hat{M} = \frac{1}{n}\sum_{k=1}^n y_k$;
	\item $D(p_0)$: $\hat{D} = \frac{1}{n}\sum_{k=1}^n y_k^2 - \big(\frac{1}{n}\sum_{k=1}^n y_k\big)^2$;
	\item $\phi(p_0) = \Ex_{p_0} e^{is\xi}$: $\hat{\phi} = \frac{1}{n}\sum_{k=1}^n e^{isy_k}$;
	\item $Q_\alpha(p_0)$ -- quantile: $\hat{Q}_\alpha$ is defined as $\abs{\obj{\induc{k=1,\ldots,n} y_k \leq \hat{Q}_\alpha}} = \floor{n\alpha}$.
\end{itemize}

In order to understand how well $p_0$ is approximated, we need a metric on the density functions $d$. With this metric we find
\[\theta^*(p_0) \defn \text{argmin}_{\theta \in\Theta} d\Big(p_0, p(\cdot,\theta)\Big) \]

The list of possible metrics: \begin{description}
	\item[Covariance distance]\hfill \\
	\[D_c(f,g)\defn \sup_{A\in \borel{\Real}} \abs{\int_A f dx - \int_A g dx}\]
	it is not employed much in estimation, since even using Scheffe's theorem, it is hard to use.
	For the empirical density we have
	\[D_c(\hat{p}, p(\cdot,\theta)) = \frac{1}{2} \int \abs{\hat{p} - p(\cdot,\theta)}dx = \frac{1}{2}\]
	\item[Kullback-Leibler]\hfill \\
	\[D_k(f,g) = \int f \log \frac{g}{f} dx\]
	it is used even though it is not in fact a metric due to asymmetry.
	It is easily calculated on the observed empirical data.
	Suppose $Y-i$ are iid with $p_0$ -- the true density, and let $p_1$ be an estimate, then $D_k(p_0^n, p_1^n) = nD_k(p_0,p_1)$.
\end{description}

\noindent\textbf{Scheffe's theorem}:
\[D_c(f,g) = \frac{1}{2} = \int \abs{f-g}dx\]
Indeed suppose $A_0 = \obj{ f\leq g }$. Then
\[D_c(f,g)\geq \int_{A_0} f-g dx\]
However (correction is needed.)
\[\int \abs{f-g} dx = \int_{A_0} f-g dx - \int_{A_0^c} g-f dx = \int_{A_0} f-g dx  - \int_{A_0} -f+g dx = 2\int_{A_0} f-g dx \]
Thus 
\[D_c(f,g)\leq \frac{1}{2} \int \abs{f-g} dx \]

Conversely for any measurable $A$ we have $A = (A\cap A_0) \cup (A\cap A_0^c)$, whence
\[\abs{\int_A f-g dx} = \abs{\int_{A\cap A_0} f-g dx + \int_{A\cap A_0^c} f-g dx} = \abs{\int_{A\cap A_0} f-g dx - \int_{A\cap A_0^c} g-f dx}\]

thus 
\[\abs{\int_A f-g dx} \leq \max\obj{ \int_{A\cap A_0} f-g dx, \int_{A\cap A_0^c} g-f dx } \leq \ldots\]

however
\[\ldots \leq \max\obj{ \int_{A_0} f-g dx, \int_{A_0^c} g-f dx } \leq \int_{A_0} f-g dx  = \frac{1}{2}\int \abs{f-g} dx\]

\noindent\textbf{Pinsker inequality}\hfill \\
\[2 D_c(f,g) \leq D_k(f,g)\]

next time...

% subsection estimation (end)

% section lecture_10 (end)

\section{Lecture \# 11} % (fold)
\label{sec:lecture_11}

Multifractal analysis

The definition of $\text{dim}(A)$.
\begin{description}
	\item[Box dimensionality] For a covering $(S_k)$ of $A$ with $\text{diam}(S)\leq \epsilon$:
	\[A \subseteq \sum_{k=1}^{N^*_\epsilon(A)} S_k \]
	where $N^*_\epsilon(A)$ is the number of templates $S_k$ in the most efficient covering of $A$ with templates of diameter $\epsilon$.
	So $\text{dim}_B(A) = \lim_{\epsilon \to 0} \frac{\log N^*_\epsilon(A)}{-\log \epsilon}$
	\item[Hausdorf] The Hausdorf outer-measure on $\mathcal{P}(\Xcal)$
	\[H_\alpha(A) = \lim_{\epsilon\downarrow 0} \inf \sum_k \text{diam}(S_k)^\alpha\]
	Indeed, $H_\alpha$ is an outer measure, since sub-additivity is due to $\inf$: \[H_\alpha(A\cup B) \geq H_\alpha(A)+H_\alpha(B)\]
	The definition of Hausdorff dimensionality is
	$\text{dim}_H(A) = \sup\big\{\big. \alpha\geq 0\big.\big\rvert H_\alpha(A) > 0 \big.\big\}$
\end{description}

For any $A\subseteq \Xcal$ the Hausdorff dimensionality is not greater than the Box dimensionality, due to templates in $H$ being arbitrary.

Suppose $\alpha = \text{dim}_B(A)$ and consider $H_\alpha(A)$ for the covering in Box dimensionality:
\[\inf_{*} \sum_n \text{diam}(S_n)^\alpha \leq \sum_k \text{diam}(S_k)^\alpha = \sum_k \epsilon^\alpha = N^*_\epsilon(A) \epsilon^\alpha\]
where the infimum is taken over the coverings of diameter at most $\epsilon$.

On the other hand $\alpha = \text{dim}_B(A)$:
\[\alpha = \lim_{\epsilon\to 0}\frac{\log N^*_\epsilon(A)}{-\log \epsilon}\]
whence $\log N^*_\epsilon(A) = -\alpha \big(\log \epsilon + o(1)\big)$
and
\[N^*_\epsilon(A) = \epsilon^{-\alpha}e^{-\alpha o(1)}\]

thereore
\[H_\alpha(A) = \lim_{\epsilon} \inf_{*} \sum_n \text{diam}(S_n)^\alpha \leq \lim_{\epsilon} \epsilon^{-\alpha}e^{-\alpha o(1)} \epsilon^\alpha \]


\noindent\textbf{Frostman's lemma}\hfill\\
Consider a metric space $(X,d)$ with a finite measure $\mu$ on $\mathcal{B}(X)$. Let $M\subseteq X$ be such that $\mu(M) > 0$. Suppose $\alpha\geq 0$ is such that $\mu(S) \leq c \text{diam}(S)^\alpha$ for any measurable template $S\subseteq M$ with sufficiently small diameter. Then $H_\alpha(M)>0$ implies that $\text{dim}_H(M) \geq \alpha$.

Indeed, consider the sum for this $\alpha$:
\[\sum_n \text{diam}(S_n)^\alpha \geq \sum_k \frac{1}{c} \mu(S_k) \geq \frac{1}{c} \mu(M)\]
where the sum is taken over members of a covering of diameter at most $\epsilon$. Thus
\[H_\alpha(M) = \lim_{\epsilon\to 0}\inf_* \sum_n \text{diam}(S_n)^\alpha \geq \frac{1}{c} \mu(M)\]
whence by definition of $\dim_H(M)$ it is true that $\dim_H(M) \geq \alpha$.


Consider tha box dimensionality of a cantor set $K$.
Choose $\epsilon = 3^{-n}$ whence $n=-\log_3 \epsilon$. Then $N_\epsilon(K) = 2^n$ and $N_\epsilon^*(K) \leq 2^n = \epsilon^{-\log_3 2}$. Thus
\[\text{dim}_H(K) \leq \text{dim}_B(K) = \lim_{\epsilon\to 0}\frac{\ln N_\epsilon^*(K)}{-\ln \epsilon}\leq \log_3 2 < 1\]

But for $\mu_\infty$ -- the measure on the cantor set, we have 
\[\mu_\infty(S_k) \leq 2^{-n} = \text{diam}(S_k)^{\log_3 2}\]

\subsection{Multifractals} % (fold)
\label{sub:multifractals}

Consider a measure $\mu$ on $(\Xcal, d)$. It has singularity of order $\alpha$ at $x\in \Xcal$ with $x\in \{\mu(S)>0\}$ if for any $\epsilon>0$
\[\mu(B_\epsilon(x)) \overset{\text{asy}}{\sim} \epsilon^\alpha\]
where $B_\epsilon(x) = \obj{\induc{y\in \Xcal}d(x,y)\leq\epsilon}$ is a closed ball in $(\Xcal, d)$.

Let $(\Omega, \Tcal)$ be a metrizable topological space and $\mu$ a finite measure on $\big(\Omega, \borel(\Omega)\big)$. Then for all measurable $B$ and every $\epsilon>0$ there exist $F$ closed and $G$ open in $(\Omega, \Tcal)$ with $F\subseteq B \subseteq G$ such that $\mu(G\setminus F) < \epsilon$.

A multifractal spectrum is a function $f$ such that 
\[f(\alpha) = \text{dim}_H\Big(\big\{\big. x\in \text{supp}\mu\big.\big\rvert \big.\big\} \mu \text{ has singularity order } \alpha \text{ at } x\Big)\]

For any covering $(S_k)$ of the support of $\mu$ with $\text{diam}(S_k)\leq \epsilon$ let
\[R_{\mu,\epsilon}(q) = \inf_* \sum_k \big(\mu(S_k)\big)^q\]
The R\'enyi exponent is $\tau_\mu(q)$ defined as follows:
\[\tau_\mu(q) = \lim_{\epsilon\to 0} \frac{\log R_{\mu,\epsilon}(q) }{\log \epsilon}\]
The R\'enyi exponent could be estimated numerically, in contrast to the multifractal spectrum (if the measure is given approximately as a set of atomic masses).

The good news is that the spectrum and the exponent are related.

An observation: for $R_{\mu,\epsilon}(q)$ there is the following estimate (using the Box dimensionality templates) for ``good'' sets is
\begin{align*}
	R_{\mu,\epsilon}(q)
	&= \inf_* \sum_k \big(\mu(S_k)\big)^q \\
	&\approx \sum_\alpha \sum_k \mu(S_k)^{\alpha q} \\
	&\approx \sum_\alpha \epsilon^{\alpha q} N^*_\epsilon(A_\alpha) \\
	&\approx \sum_\alpha \epsilon^{\alpha q - \text{dim}_B(A_\alpha)}\\
	&\approx \sum_\alpha \epsilon^{\alpha q - f(\alpha)}
\end{align*}
where $A_\alpha = \Big\{\Big. \Big.\Big\rvert \Big.\Big\}$

Now approximately
\[\tau_\mu(q) = \lim_{\epsilon\to0} \frac{\ln \sum_\alpha \epsilon^{\alpha q - f(\alpha)}}{\ln \epsilon} = \min_\alpha \big( \alpha q - f(\alpha) \big)\]
the Legendre transform of $f(\cdot)$. Thus multifractal spectrum is recovered from
\[f^*(\alpha) = \min_q \Big( \alpha q - \tau_\mu(q) \Big)\]
The R\'enyi exponent $\tau_\mu(q)$ is convex, since it is defined as a lower envelope.

It is possible to say that for any arbitrary function $f$ sufficiently well-behaved the function
\[f^*(q) = \min_\alpha\big(\alpha q - f(\alpha)\big)\]
is convex (?).

Thus for any $\alpha$ and $q$ we have $f_\mu(\alpha)\leq \alpha q - \tau(q)$ or $f_\mu(\alpha) + \tau(q)\leq \alpha q$, which is symmetric. Thus
\[f(\alpha)\leq f^*(\alpha) \defn \min_q \Big( \alpha q - \tau_\mu(q) \Big)\]

% were $f(\alpha)$ convex, $f^*(\alpha) = f(\alpha)$.
For what $q$ is the minimum attained in the definition of $f^*(\alpha)$? First order conditions imply $\alpha - \tau_\mu'(q) = 0$ provided the derivative exists. Using the symmetric condition above we get $f'(\alpha) = q$, whence it follows that the derivatives of $\tau$ and $f$ are mutually inverse functions.

The Legendre transform classically is defined as the first integral of the inverse of the (monotonous) derivative of a convex function.

% subsection multifractals (end)

% section lecture_11 (end)

\section{Lecture \# 12} % (fold)
\label{sec:lecture_12}

Distances in statistics:
\begin{description}
	\item[Total variation] Is defined as the maximal discrepancy of probability measures $Q$ and $P$ (with densities $q$ and $p$ respectively)
	\[D(p,q) = \sup_A\Big\lvert P(A) -  Q(A) \Big\rvert = \sup_A\Big\lvert \int_A p dx -  \int_A q dx \Big\rvert\]
	Using Scheffe's equivalent representation \[D(p,q) = \frac{1}{2}\int|p-q| dx\]
	\item[Kullback-Leibler] Is the relative entropy between $P$ and $Q$:
	\[ KL(p,q) = \int p \log\frac{p}{q} dx = \int \frac{p}{q}\log\bigg(\frac{p}{q}\bigg) q dx = \text{KL}\big( P \lvert \rvert Q\big)\]
	if the random variables $(\xi_k)_{k=1}^n\sim p$ and $(\eta_k)_{k=1}^n\sim q$ are each iid then the KL devergence is $KL(p^{(n)},q^{(n)}) = n KL(p,q)$.
	% \[\text{KL}\big( P \lvert \rvert Q\big) = \int \frac{dP}{dQ} \log \frac{dP}{dQ} dQ =  \int \frac{p}{q} \log \frac{p}{q} q dx\]
\end{description}

Let's show that (Pinsker's inequality)
\[K(p,q)\geq 2 D(p,q)\]
Put $A_0 = \{ p \geq q \}$ and $A_0^c = \{p < q\}$. Then 
\[D(p,q) = \frac{1}{2}\int_{A_0} p-q dx + \frac{1}{2}\int_{A_0^c} q - p dx \]
using the fact that $p$ and $q$ are densities of $P$ and $Q$ respectively:
\[D(p,q) = \int_{A_0} p-q dx = P(A_0) - Q(A_0)\]

Jensen's inequality for a convex $f$ yields
\[\Ex f(X) \geq f\big(\Ex X\big)\]

Since $f(x) = -\log x$ is convex the following is true:
\[KL(p,q) = \int_{A_0} -\log\frac{q}{p} p dx + \int_{A_0^c} -\log\frac{q}{p} p dx \]
\[\ldots = P(A_0) \int_{A_0} -\log\frac{q}{p} \frac{p}{P(A_0)} dx + P(A_0^c) \int_{A_0^c} -\log\frac{q}{p} \frac{p}{P(A_0^c)} dx \]
whence using Jensen's
\[KL(p,q) \geq  -P(A_0) \log \int_{A_0}\frac{q}{p} \frac{p}{P(A_0)} dx - P(A_0^c) \log \int_{A_0^c} \frac{q}{p} \frac{p}{P(A_0^c)} dx\]

reducing
\[\ldots \geq -P(A_0) \log \int_{A_0}\frac{q}{P(A_0)} dx -P(A_0^c) \log \int_{A_0^c} \frac{q}{P(A_0^c)} dx \]

yields
\[KL(p,q) \geq -P(A_0) \log \frac{Q(A_0)}{P(A_0)} -P(A_0^c) \log \frac{Q(A_0^c)}{P(A_0^c)} \]

Putting $\alpha = P(A_0)$ and $\beta = Q(A_0)$
\[KL(p,q) \geq -\alpha \log \frac{\beta}{\alpha} -(1-\alpha) \log \frac{1-\beta}{1-\alpha} = \alpha \log \frac{\alpha}{\beta} + (1-\alpha) \log \frac{1-\alpha}{1-\beta} \]

Finally, consider for an arbitrary $\lambda$
\[f(\beta) = \alpha \log \frac{\alpha}{\beta} + (1-\alpha) \log \frac{1-\alpha}{1-\beta} - \lambda (\alpha - \beta)^2\]

differentiating gives:
\[f'(\beta) = -\frac{\alpha}{\beta} + \frac{1-\alpha}{1-\beta} + 2\lambda (\alpha - \beta) = (\beta - \alpha)\Big(\frac{1}{(1-\beta)\beta} - 2\lambda\Big)\]

when $\lambda < 2$ we have
\[\frac{1}{(1-\beta)\beta} - 2\lambda > 0\]
implying that $f'(\beta) = 0$ for $\alpha=\beta$.
Therefore in the extremum we have $f(\alpha) = 0$ and $f(\beta)\geq f(\alpha) = 0$ for all $\beta\in [0,1]$, since $f(0)=f(1)=+\infty$.

\subsection{Hellinger's distance} % (fold)
\label{sub:hellinger_s_distance}

Consider the total variation distance
\[D(p,q) = \frac{1}{2}\int |p-q| dx = \frac{1}{2}\int (\sqrt{p}-\sqrt{q})(\sqrt{p}+\sqrt{q}) dx\]
whence using the Cauchy-Schwartz-Bunyakovski inequality one gets
\[D(p,q) \leq \frac{1}{2}\Big(\int (\sqrt{p}-\sqrt{q})^2dx\Big)^\frac{1}{2}\Big(\int (\sqrt{p}+\sqrt{q})^2dx\Big)^\frac{1}{2}\]
since $(a+b)^2\leq 2 a^2 + 2 b^2$ we have
\[\int (\sqrt{p}+\sqrt{q})^2dx \leq \int 2p+2q dx = 2 \int p+q dx = 4\]
Therefore
\[D(p,q) \leq \Big(\int (\sqrt{p}-\sqrt{q})^2dx\Big)^\frac{1}{2} \frac{1}{2}\Big(4\Big)^\frac{1}{2}\]

% subsection hellinger_s_distance (end)

\subsection{$\chi^2$-distance} % (fold)
\label{sub:chi2_distance}

The $\chi^2$ distance is an approximation of the Kullback-Leibler distance when $p\approx q$.

Consider the Taylor expansion of $\log(1+x)$. Note that 
\[\frac{d^k}{dx^k} \log(1+x) = -(-1)^kk!\frac{x^{-k}}{k}\]
whence Taylor-expanding around $a=0$ we get
\[\log(1+x) = \log(1) + \sum_{k\geq1} \frac{1}{k!}\bigg.\frac{d^k}{dx^k} \log(1+a)\bigg\rvert_{a=0} (x-0)^k = \sum_{k\geq1} \frac{-(-1)^k}{k!}k!\frac{x^k}{k} = -\sum_{k\geq1} \frac{(-1)^kx^k}{k}\]
truncating at the second term:
\[\log(1+x) = x - \frac{x^2}{2} + o(x^2) \]
whence $\log(1+x)\approx x-\frac{x^2}{2}$ for sufficiently small $x$ we get.
Therefore we get
\begin{align*}
	KL(p,q) &= -\int p\log\frac{q}{p}dx = -\int p \log \Big(1+\big(\frac{q}{p}-1\big)\Big) dx \\
	& \approx -\int p \Big(\big(\frac{q}{p}-1\big) - \frac{1}{2} \big(\frac{q}{p}-1\big)^2 \Big) dx = -\int (q-p) \Big(1 - \frac{1}{2} \big(\frac{q}{p}-1\big) \Big) dx \\
	& = -\int (q-p) dx - \int (q-p) (-1) \frac{1}{2} \frac{q-p}{p} dx\\
	& = \frac{1}{2}\int \frac{|q-p|^2}{p} dx
\end{align*}
Therefore for sufficiently close densities $p$ and $q$ the Kullback-Leibler divergence is well approximated by the $\chi^2$ distance
\[KL(p,q) \approx \frac{1}{2}\int \frac{|p-q|^2}{p}dx\]

% subsection chi2_distance (end)

\subsection{The maximum likelihood} % (fold)
\label{sub:the_maximum_likelihood}

Suppose there is a sample $\big(Y_k\big)_{k=1}^n\sim p^{(n)}_0$ -- the true density. Consider a parametric family $p^{(n)}(x;\theta)$ for $\theta\in \Theta$.
We wish to find 
\[\theta^*(p_0)=\text{argmin}_{\theta\in \Theta} KL(p^{(n)}_0, p^{(n)}(\cdot;\theta))\]

However minimizing the Kullback-Leibler divergence is equivalent to 
\[\theta^*(p_0)=\text{argmax}_{\theta\in \Theta} \int p^{(n)}_0 \log p^{(n)}(\cdot;\theta) dx^n\]

Using the empirical density to estimate $\int p^{(n)}_0 \log p^{(n)}(\cdot;\theta) dx^n$ we get the likelihood
\[\hat{L} = \int \delta(x-Y^{(n)}) \log p^{(n)}(\cdot;\theta) dx^n = \log p^{(n)}(Y^{(n)};\theta)\]
whence 
\[\hat{\theta}(Y^{(n)}) = \text{argmax}_{\theta\in \Theta} \log p^{(n)}(Y^{(n)};\theta)\]

if $Y^{(n)}$ is iid and $Y_i = \theta + \epsilon_i$ with $\epsilon_i\sim p_\epsilon$ iid with $\Ex \epsilon_i = 0$ and $\Ex \epsilon_i^2 = \sigma^2$, then
\[\hat{L} = \sum_{i=1}^n\log p_\epsilon(Y_i-\theta)\]
Using the law of large numbers we get
\[\frac{1}{n}\hat{L} \to \Ex_{p_0}\log p_\epsilon(Y_1-\theta)\]
whence
\[\hat{\theta}_n \to \text{argmax}_{\theta}\Ex_{p_0}\log p_\epsilon(Y_1-\theta) = \text{argmin}_{\theta} KL\Big(p_0,p_\epsilon(\cdot - \theta)\Big)\]

% subsection the_maximum_likelihood (end)

\subsection{Properties of MLE} % (fold)
\label{sub:properties_of_mle}
Illustrated for the case of a simple constant and spread estimation.

If $\epsilon\sim \Ncal(0,\sigma^2)$, then 
\[\hat{L} = -\frac{1}{2\sigma^2}\sum_{i=1}^n\big(Y_i-\theta\big)^2 - \frac{n}{2}\log(2\pi \sigma^2)\]
implying that $\hat{\theta} = \frac{1}{n}\sum_{i=1}^n Y_i$.
If $\epsilon$ are iid with $D(0,\sigma^2)$ then the ML estimate is unbiased $\Ex \hat{\theta} = \theta$.
Its variance is $\Ex (\hat{\theta}-\theta)^2 = \frac{\sigma^2}{n}$.

using the Central Limit theorem we get
\[\Pr\big(\sqrt{n} \frac{\hat{\theta}-\theta}{\sigma} \geq z\big)\to \int_z^\infty \frac{1}{\sqrt{2\pi}} e^{-\frac{s^2}{2}}ds \]

The sample variance is estimated using $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(Y_i - \hat{\theta})^2$ whence
\[\hat{\sigma}^2 = \sigma^2 + \frac{1}{n}\sum_{i=1}^n (\epsilon_i^2 - \sigma^2) - \frac{1}{n} \sum_{i=1}^n \epsilon_i^2\]
Therefore
\[\Ex \hat{\sigma}^2 = \sigma^2 - \frac{1}{n}\sigma^2 \]
and the estimate is biased.

However the estimates $\hat{\sigma}^2$ and $\hat{\theta}$ are independent. It is based on the fact that for Gaussian random variables only lack of correlation implies independence.
\begin{align*}
	\Ex (\hat{\theta}-\theta) (Y_i - \hat{\theta})
	&= \Ex (\hat{\theta}-\theta) \big(\epsilon_i + (\theta - \hat{\theta}) \big) \\
	&= \Ex (\hat{\theta}-\theta)\epsilon_i - \Ex (\hat{\theta}-\theta)^2 \\
	&= \Ex \Big(\frac{1}{n}\sum_{j=1}^n Y_j-\theta\Big)\epsilon_i - \frac{1}{n}\sigma^2 \\
	&= \frac{1}{n}\sum_{j=1}^n \Ex (Y_j - \theta)\epsilon_i - \frac{1}{n}\sigma^2 \\
	&= \frac{1}{n}\sum_{j=1}^n \Ex \epsilon_j\epsilon_i - \frac{1}{n}\sigma^2 \\
	&= \frac{1}{n}\sigma^2 - \frac{1}{n}\sigma^2 = 0
\end{align*}

% subsection properties_of_mle (end)

% section lecture_12 (end)

\end{document}
