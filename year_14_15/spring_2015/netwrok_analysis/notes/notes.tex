\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Ntrl}{\mathbb{N}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\Ex}{\mathbb{E}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Structural analysis and visualization of networks}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}

\selectlanguage{english}

\maketitle

\begin{description}
	\item[Email] \hfill \\
	lzhukov@hse.ru
	\item[Course webpage] \hfill \\
	\url{http://www.leonidzhukov.net/hse/2015/networks/}
	\url{http://www.leonidzhukov.net/hse/2014/socialnetworks/}
\end{description}

Programming: iPython notebooks
Visualization: yEd, Gephi

Linear algebra prerequisites:
	Spares matrices
	Eigenanalysis


Graph $G=(V,E)$. The set $V$ is the set of vertices and $E$ is a subset of $V\times V$.
An element $(u,v)\in E$ is an edge starting at $u$ and ending in $v$.
The incidence matrix is defined as $a_{ij}=1_E\brac{(i,j)}$, so it denotes and edge $i\to j$.

Random graphs are pure mathematics theory created by Erd\"os and Renyui.
Statistical physics for analysis of complex networks.

Network, social network, complex network just another name for a graph.

Power law (scale free) few vertices with high degree, many nodes with few neighbours.

Complex means that reduction of a system actually destroys the systems.
Cannot predict the whole by the studying the parts.

Facebook network -- many worlds network -- typical for a power law random graph.

Complex networks usually have the following characteristics: \begin{enumerate}
	\item Power law distribution of the vertex degree;
	\item Small diameter and average path length;
	\item High propensity to cluster: the number of triangles in the network.
\end{enumerate}

Let's introduce the following local features of the graph: the vertex degree $\delta^+,\delta^-:V\to \Ntrl$
\begin{align*}
	\delta^+(v) &\defn \#\obj{\induc{u\in V}\, (u,v)\in E }\\
	\delta^-(v) &\defn \#\obj{\induc{u\in V}\, (v,u)\in E }
\end{align*}

``Any two people are on average separated no more than by six intermediate connections.''
\begin{itemize}
\item ``The small-world problem'', Stanley Milgram, 1967
\item ``An experimental study of the small-world problem'', Travers J., Milgram S., 1969
\end{itemize}

Bethe lattice $(V,E)$ is an infinite cycle-free graph with every node having the same number of neighbours $z > 1$.
Given an edge $(v,u)\in E$ the end vertex $u$ is connected to $z-1$ other neighbours, which means that $N_k = N_{k-1}\cdot (z-1)$ with $N_1 = z$, since the centre vertex is connected to $z$ other vertices.
Thus $N_k = z \brac{z-1}^{k-1}$.
And the total number of nodes is \[S_n \defn 1+\sum_{k = 1}^n N_k = 1 + z \frac{\brac{z-1}^n-1}{(z-1)-1}\] (check this!)

For any network $G=(V,E)$ its \textbf{order} is $\abs{V}$ and size is $\abs{E}$.

% As for the format of homework, python + \Latex is acceptable.

% Homework: complete the notebook.!!! -- Completed on 2014-12-17

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

THe power law distribution is very natural in the study of networks:
\[p(x) \defn \frac{C}{x^\beta} 1_{\clop{x_0,+\infty}}(x)\]
where $\beta > 0$ is the power.

The indefinite integral of $p(x)$ is equal to
\[\int p(x) dx = \begin{cases}
    C \frac{x^{1-\beta}}{1-\beta},& \text{if } \beta\neq 1\\
    C \ln x, & \text{if } \beta = 1
\end{cases}\]

If $\beta > 1$ then $p(x)$ is a density function and the normalisation constant is determined by
\[\induc{C \frac{x^{1-\beta}}{1-\beta}}^\infty_{x_0} = C \brac{ - \frac{x_0^{1-\beta}}{1-\beta}} = 1\]
whence $C\defn (\beta - 1) x_0^{\beta-1}$. In its final form the density looks like:
\[p(x) \defn \frac{\beta - 1}{x_0} \brac{\frac{x}{x_0}}^{-\beta} 1_{\clop{x_0,+\infty}}(x)\]

The \textbf{survival} function $H(x) = 1 - F(x)$ is useful in network analysis. For the power law the survival function is \[H(x) \defn \brac{\frac{x_0}{x}}^{\beta-1}\]

Median is more suitable for estimating the power law distribution parameters. The quantile function is given by:
\[q_\alpha \defn \brac{1-\alpha}^{-\frac{1}{\beta - 1}} x_0 \]

Power law is scale invariant. Indeed 
\[H(s x) = \brac{\frac{x_0}{s x}}^{\beta - 1} = \brac{s}^{1-\beta} H(x)\]

Consider the node degree distribution of an undirected network.
There will be a majority of nodes with low degree,
and there will be very few vertices with extremely high degree.

Node degree -- the number of nearest neighbours
Degree distribution \[P(k) \defn \frac{n_k}{\sum_{k}n_k}\]
the model is $P(k) = C k^{-\gamma}$.
Normalization -- the Riemann Zeta function.
\[C \sum_{k\geq 1} k^{-\gamma} = 1 \Leftrightarrow C \defn \frac{1}{\zeta(\gamma)}\]

In maximum likelihood vary $x_0$ to get the idea of how $\alpha$ depends on it.

Use Kolmogorov-Smirnov test against the exponential distribution to find the ``best'' values of $x_0$.
Look for the minimal value of the K-S statistic and choose $x_0$.

% read the references

\subsubsection{The maximum likelihood estimation} % (fold)
\label{ssub:the_mle}

Suppose $\brac{x_i}_{i=1}^n$ is an iid sample from some random variable distributed according to the power law.
The log-likelihood is given by \[\log\mathcal{L} = \sum_{k=1}^n \beta \log\frac{x_0}{x_k} + n \log\frac{\beta - 1}{x_0}\]
where $x_0\leq \min_{k=1\ldots n}x_k$, since otherwise the log-likelihood would be $-\infty$.
The first-order conditions are given by \begin{align*}
	- \sum_{k=1}^n \log\frac{x_k}{x_0} + \frac{n}{\beta - 1} = 0\\
	n \brac{ \beta - 1 } \frac{1}{x_0} > 0
\end{align*}

Therefore $\hat{x}_0 \defn \min_{k=1\ldots n}x_k$ and the ML-optimal estimator of $\beta$ is
\[\hat{\beta} \defn 1 + \frac{n}{\sum_{k=1}^n ( \log x_k - \log x_0 )}\]

% subsubsection the_mle (end)

\subsubsection{The OLS estimation} % (fold)
\label{ssub:the_ols_estimation}

Another possibility is to estimate the parameters of the power law by means of a regression of the frequencies of binned sample on the ``typical'' values of the bins.

Suppose $\brac{f_i}_{i=1}^B$ and $\brac{b_i}_{i=1}^B$ are bin frequencies and centres respectively, constructed on data $\brac{x_k}_{k=1}^n$ which supposedly came from a power law distribution.

Then it is possible to fit the following regression model to the histogram data to get the parameters of the law:
since $f_x \sim p(x) \Delta x$
\[\log f_i \sim \brac{\log{(\beta-1)} + (\beta-1) \log x_0} + (-\beta) \log b_i\]
It is a fast but a rather crude way of estimating.

% subsubsection the_ols_estimation (end)

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

Graph models

Erd\"os-Rnyi model

A random graph is a element of the set $\prod_{\omega\in \Pwr_2(V)} \obj{0,1}$.
A large collection of Bernulli random variables, indicating whether an edge is present or not.

$G_{n,m}$ model -- a graph is randomly selected form a set  $C_N^m$ graphs, with $N\defn \frac{n(n-1)}{2}$ of with $n$ nodes and $m$ edges.

$G_{n,p}$ -- the model of a random graph in which an edge between
any two vertices is established with probability $p$. The total
number of edges is again $N$. The size of the graph is a thus a
random number.

The models are asymptotically equivalent.

The mean node degree is given by
\[\epsilon(G) = \frac{2 \Ex(m)}{n} = p\frac{2 n(n-1)}{2 n} \approx p n\]
The graph density is the \[\rho \defn 2\frac{\Ex(m)}{n(n-1)}\]

The degree distribution of a random graph.

The chance that a given node $v$ has $k$ neighbours is given by:
\[\pr\brac{\delta(v) = k} = C^k_{n-1} p^k \brac{1-p}^{n-k-1}\]

Since the binomial distribution is asymptotically poisson if $p n = \lambda$ and $n\to \infty$:
\[C^k_{n-1} p^k \brac{1-p}^{n-k-1} \to \]

Use these models as a benchmark (in the null hypothesis) to compare against the phenomena in the real life.

Treating the model parametrically to witness phase transitions.
\begin{description}
	\item[$p=0$] \hfill \\ the graph is a set of isolated vertices;
	\item[$p=p_c$] \hfill \\ the graph obtains a spanning tree;
	\item[$p=1$] \hfill \\ the graph is totally connected.
\end{description}

Suppose $p$ changes with $n$. 


The chance that a node does not belong the the Giant Connected Component,
in the case of Poisson node degree distribution is given by
\[ u = \sum_{k\geq 0} \pr(k) u^k = \sum_{k\geq 0} e^{-\lambda}\frac{\lambda^k}{k!}u^k = e^{\lambda (u-1)}\]
the necessary condition for a nonzero $u$ is that $\lambda>1$
(ensures that the graph of the exponent intersects the $45\,^{\circ}$ line at a nonzero point).

\begin{description}
	\item[$p\to p_c-$] \hfill \\ no components larger than $O(\log n)$;
	\item[$p=p_c$] \hfill \\ the largest component has $O(n^\frac{2}{3})$;
	\item[$p\to p_c+$] \hfill \\ the gigantic connected component is of the order $O(n)$.
\end{description}
The critical values is $p_c n = 1$.

Threshold values at which interesting phenomena appear -- subgraphs of order $g$:
\begin{description}
	\item[$p\sim n^\frac{-g}{g-1}$] there almost surely exists a tree of order $g$;
	\item[$p\sim n^{-1}$] there is a cyclce of order $g$;
	\item[$p\sim n^\frac{-2}{g-1}$] a clique of order $g$ (a complete subgraph).
\end{description}


The clustering coefficient: ratio of closed triangles to all possible triangles
\[C(k) = \frac{p k(k-1) 2 }{ k( k-1 ) 2} = p\]
The sparser the graph the more negligible is the clustering coefficient.

The average number of nodes $s$ hops away from the current node is $\lambda^s$.
In the critical regime of the edge probability, all nodes belong to the the GCC with high probability.
Thus $d \sim \frac{\log n}{\log \lambda}$.

Fixing the issues with degree distribution.
The configuration model.
Take $V$ vertices and a finite sequence of node degrees $\brac{d_v}_{v\in V}$ with $\sum_{v\in V} d_v$ -- even.

% Slide~17.
The constructed graph might be a lopped multigraph.
Solution: use special graphical degree sequences.
The probability that $v,u\in V$ are connected is given by
\[p_{uv}\frac{d_u d_v}{2 m - 1}\]

Does not allow for an asymptotically non-zero clustering coefficient.

Very different from the Erd\"os-Renyi model!

A random graphs does not have communities.

Non-zero clustering coefficient hint at non-random processes underlying the edge formation.

% section lecture_3 (end)

\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}

Dynamical grove (?) model.

Small Worlds model [Watts and Strogatz; 1998]

Previously considered networks were static fixed number of vertices and edges.

Let's have look at evolving graphs.
Growing networks (evovling with time)
\begin{itemize}
	\item Citation networks;
	\item Collaboration networks;
	\item Web;
	\item Social networks.
\end{itemize}

Consider pure growth model of an network (random growing network)
\begin{itemize}
	\item $t=0$ there are $n_t$ unconnected nodes;
	\item growth . on every time step $t\geq 1$ add a node with $m\leq n_0$ edges $k_t(t) = m$ attached randomly to the existing nodes;
	\item from $m$ edges between (?) existing nodes uniformly at random \[\Pi(k_i) = \frac{1}{n_0+t-1}\] (? probability of an edge getting connected to some existing node)
\end{itemize}

The expected degree of any node $i$ (the time the node was introduced):
\[k_i(t) = m + \sum_{k=0}^k\frac{m}{n_0+i-1 + k}\]

for $t>>n_0$
\[k_i(t) \aaprox m \brac{1 + \log\frac{t}{i}}\]

The longer the node has been in the system, the more degree it has: ``the first move-in advantage''.

If interpreted as a function of the time of join $i$ for a fixed $t$ (a snapshot), then it can show that a node the more recent nodes have lesser chances of having a high degree.


Find the nodes that at time $t$ have degree less than $k$: $\obj{\induc{i}\,k_i(t) < k}$

\[k_i(t) = m\brac{1+\log\frac{t}{i}}<k\,\implies\, i . t e^\frac{m-k}{m}\]

Fraction of nodes with degrees $k_i(t)<k$:
\[F(k) = \Pr\brac{k_i(t)<k} = \frac{n_0+t-i}{n_0+1} \approx 1 - e^\frac{m-k}{m}\]
Thus the degree distribution is approximately exponential.


Another way of getting at the same result: use continuous approximation!

Since $\Pi(k_i) = \frac{1}{n_0+t-i} \approx \frac{1}{t}$, the following must be true:
\[k_i(t+\delta t) = k_i(t) - \frac{m}{t}\delta t\]
whence the following first order differential equation emerges:
\[\frac{d}{dt}k_i(t) = \frac{m}{t}\]
with initial conditions $k_i(t=i) = m$.
the solution id $k_i(t) = m\brac{1+\log\frac{t}{i}}$.

% [Barabasi and Albert; 1999]
\begin{description}
	\item[$t=0$] there are $n_0$ nodes;
	\item[growth] on ever step add a node with $m$ edges ($m\leq n_0$), $k_i(i)=m$;
	\item[Preferential attachment] probability of linking it to an existing node $i$ is proportional to the node degree $k_i$
	\[Pi(k_i) = \frac{k_i}{\sum_j k_j}\]
\end{description}

Preferential attachment represents the heuristic rule that ``similar attract''.

\[k_i(t+\delta t) = k_i(t) + m \Pi(k_i)\delta t\]
\[\frac{d}{dt}k_i(t) = m \Pi(k_i) = m \frac{m k_i}{2 m t}\]
which has the solution: time evolution of a node $i$ degree
\[k_i(t) = m\brac{\frac{t}{i}}^\tfrac{1}{2}\]

Nodes with degree less than $k$: $\frac{m^2}{k^2} t<i$

The power law emerges:
\[F(k) = \Pr\brac{k_i(t)<k} = \frac{n_0+t-i}{n_0+1} \approx \frac{n_0+t-\frac{m^2}{k^2} t}{n_0+1} \approx 1 - \frac{m^2}{k^2} t\]


Barabasi model has heavier tails than the random graph: indeed it is obvious from the exponent versus hyperbola comparison.

Had such model been a reality there would be extremely many poor, and a few extremely rich, who get richer with time.

The average path length (analytical result):
\[\brkt{L}\approx \frac{\log N}{\log\log N}\]

Clustering coefficient (simulation result):
\[C\approx n^{-\frac{3}{4}}\]
$C$ is infinitesimal as $N\to \infty$.

Barabasi (1999) preferential attachment model has been rediscovered multiple times:
\begin{itemize}
	\item Yule process 1925;
	\item Polya's urn model;
	\item Herbert simon;
	\item Evolution of citation networks, cumulative advantage model; Derek de Solla Price, 1976;
\end{itemize}

\subsection{The small world model} % (fold)
\label{sub:the_small_world_model}

Devised by Watts and Strogatz in 1998.

Image a two-tier ring graph, with structural triangles.

% subsection the_small_world_model (end)
The motivation of the small world model: keep high clustering, get small diameter (the greatest distance between any two nodes).

Evolution is to construct shortcuts so as to decrease the average path length.

Strogatz proposed to re-wire the graph: pick any edge, sever it, and wire it elsewhere.

start with regular lattice with $n$ nodes, $k$ edges per vertex $k<<n$.
randomly connect with 

Small world region:
\begin{itemize}
	\item high clustering coefficient;
	\item small average path length.
\end{itemize}

% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Centrality measures of graphs

Which vertices in the network are important?

Is it possible to order the nodes in the network, so that the first node is the mos important, and the last is the least important.

What is meant by importance?

let's have a look at graph theoretic understanding of the centrality.

\noindent\textbf{Definition}\hfill\\
THe \textbf{eccentricity} of vertex $u\in V$ is the maximum distance between the vertex $u\in V$ and any other $v\in V$ in the graph $G$ 
\[\text{ecc}(u) \defn \max_{v\in V}\induc{d}_G(u,v)\]

The diameter is really the largest eccentricity: the larges possible distance between any two nodes in a graph.
\[\text{diam}(G)\defn \max_{u\in V} \text{ecc}(u)\]

The radius of a graph is the minimum eccentricity
\[\text{rad}(G)\defn \min_{u\in V} \text{ecc}(u)\]

The \textbf{central point} of a graph $G$ is a vertex, with eccentricity equal to the radius of $G$:
\[\text{diam}(G)\defn \max_{u\in V} \text{ecc}(u)\]

A graph \textbf{centre} is the set of all central nodes of $G$. The \textbf{periphery} of $G$ is the set of nodes with eccentricities equal to the diameter of $G$.

The just defined notions are not robust with respect to ``small'' changes to the graph: the may undergo extreme changes when the graph is perturbed. But what change is ``small'' for a given graph? (adding a chain may actually change the topology quite significantly)


Periphery is a graph is used in the sense other than graph theoretic (may be with respect to the vertex degree distribution).

The notion of vertex importance (from Sociology): the location of the node in a social graph is correlated with the influence of that vertex in the network (the ``ties'').

Notions of centrality usually deal with undirected graphs, corresponding to symmetric relations. Well what about prestige network relations?


The degree centrality is just the number of nearest neighbours for a graph incidence matrix $A$:
\[C_D(u) = \detla_u = \sum_{v\in V} A_{uv}\]

Normalized graph centrality, defined as $C_D^*(u) \defn \frac{C_D(u)}{\abs{V}-1}$, is useful for comparison between graphs, not just within one graphs.

High centrality means direct contact with many other nodes. Low centrality degree -- an in active vertex, a peripheral vertex.

Calculate how close the node is to any other vertex in the system. This gives us the closeness centrality metric
\[C_C(u) \defn \frac{1}{\sum_{v\in V} d(u,v)}\]
It can be normalise by $\tfrac{1}{\abs{V}-1}$.

Nodes in the centre can quickly interact with all others, have short communication path to others, and minimal number of steps to reach others.

There are problems for disconnected graphs, where infinite distances emerge. In theses cases use the harmonic centrality
\[C_H(u)\defn \brac{\sum_{v\in V} \frac{1}{d(u,v)}}^{-1}\]

The most interesting definition of centrality is betweenness centrality.

Consider the network as a set of interconnected communication hubs. The most important nodes are those, which are indispensable for proper network operation. 

Consider a graph $G = \brac{V,E}$.
The betweenness centrality measure of $G$ is defined as follows:
\[C_B(u) = \sum_{s\neg t\neq u}\frac{\sigma_{st}(u)}{\sigma_{st}}\] 

The normalised version of betweenness is just scaled by
\[\frac{2}{(n-1)(n-2)}\]

It is possible to extend the betweenness centrality to edges.

Probability that a communication from $s$ to $t$ will go through $u$ (geodesics) is given by $\frac{\sigma_{st}(u)}{\sigma_{st}}$.

Usage of these measures greatly depends on what is understood by ``centrality''.

\subsection{Eigenvector centrality} % (fold)
\label{sub:eigenvector_centrality}

Consider a recursive definition of centrality: An important nodes have important neighbours and connections.

Let $\brac{I^{t+1}_u}_{u\in V}$ be the importance vector at some iteration $t\geq1$, with components corresponding to each node. Then define the importance as the limit of the following process
\begin{align*}
	\sum_{j\in V} A_{ij} I^t_j \to I^{t+1}_i \\
	\frac{1}{\lambda}\sum_{j\in V} A_{ij} I_j = I_i
\end{align*}

Thus if the iterations converge, then they converge to an eigenvector of the adjacency matrix.

A node with large centrality will be connected with nodes of large centrality. Thus highly important nodes are likely to have highly important neighbours.

It is a kind of \emph{self-reinforced centrality}.

% subsection eigenvector_centrality (end)

\subsection{Katz centrality} % (fold)
\label{sub:katz_centrality}

Weighted count of the path coming to the node (Katz; 1953). The weight of a path of length $n$ is counted with attenuation factor $\beta^n$.
\begin{align*}
	k_u &= \sum_{v\in V} \brac{E_{uv} + \beta A_{uv} + \beta^2 A_{uv} + \ldots}\\
	k = \brac{\sum_{k\geq0} \beta^k A^k} \mathbf{1} \\
	k = \brac{E - \beta A}^{-1} \mathbf{1} \\
	\brac{E - \beta A} k = \mathbf{1} 
\end{align*}

The parameter $\beta$ controls the measure, if it is $1$ then Katz centrality reduces to the ... NO IT DOES NOT!

Two-parametric centrality measure

Let $\beta$ be the degree to which node's importance is a function of it's neighbours' importance, and $\alpha$ be a normalization parameter.
\[c_u(\alpha,\beta) \defn \sum_{v\in V}\brac{\alpha+\beta c_v} A_{uv}\]
In the matrix form:$c = \alpha A\mathbf{1} + A\mathbf{1} c$.

% subsection katz_centrality (end)

\textbf{Centralization} of the whole network is measure of how ``central'' is the most central node in relation to all other vertices.

%% Slide p.~18
\[C_x(G) \defn \frac{\sum_{v\in V} C_x(\bar{u}_G) - C_x(v) }{\max_H \sum_{v\in V} C_x(\bar{u}_H) - C_x(v)}\]
where $C_x$ is any vertex centrality measure and $\bar{p}_G$ is the central node of the graph $G$. The maximum is take over the graph of the same order as $G$.

Prestige is a measure of centrality of directed graphs.
Degree prestige $\delta_{\text{in}}(u)$ is the vertex in-degree.

The main contribution of the Page Rank algorithm is in how to deal with ``directedness'' of the graph.

It is insightful to compare the centrality measures between each other and see how they are correlated.
\begin{itemize}
	\item Pearson's correlation measures strength of linear association;
	\item Spearman's rank correlation -- strength of monotone relationship;
	\item Kendall $\tau$, count the number of pairwise agreements and disagreements between a pair of ranked data series. 
	\[2\frac{n_c - n_d}{n(n-1)}\]
	$n_c$ is the number of concordant pairs, and $n_d$ -- discordant pairs (agreements $\tau=1$, disagreements $\tau=-1$).
\end{itemize}

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

\subsection{WEB search algorithms} % (fold)
\label{sub:web_search_algorithms}



% subsection web_search_algorithms (end)

% section lecture_6 (end)

\section{Lecture \# 7} % (fold)
\label{sec:lecture_7}

Assortative mixing and structural equivalence.
\begin{description}
	\item[Global] average node degree, average clustering coefficient, average path length;
	\item[local] $\ldots$;
\end{description}

Nodes $u, v\in V$ are \textbf{strongly} structurally equivalent if they share neighbours: $N^+(u) = N^+(v)$ and $N^-(u) = N^-(v)$. The problem is the connectivity pattern (directions and number of edges) is almost identical, yet $u\in N^-(v)$ and $v\in N^+(u)$.

In adjacency matrices this means that the $u$ and $v$ columns and rows are identical. However this definition is too strict: it requires \textbf{identical} connectivity.

Structural similarity of nodes

\begin{description}
	\item[Jaccard] \hfill\\
	For $u,v/in V$ the metric is defined as \[J(v,u) = \frac{\big\lvert N(v) \cap N(v) \big\rvert}{\big\lvert N(v) \cup N(v) \big\rvert}\] It show how much overlap there is between two nodes.
	\item[Cosine] \hfill\\
	Defined as the cosine of the angle between the column-vectors $u,v$ of the adjacency matrix $A$:
	\[\sigma_{uv} = \frac{ A_v'A_u }{\sqrt{ A_v'A_v }\sqrt{ A_u'A_u }}\]
	\item[Pearson]\hfill\\
	The straightforward sample correlation of $A_v$ and $A_u$ in the adjacency matrix $A$.
	\item[Binary adjacency matrix] \hfill\\
	Then the degree $k_v = \sum_{u\in V} A_{uv} = \sum_{u\in V} A_{uv}^2$. Furthermore $n_{uv}$ -- the number of shared neighbours is given by 
	\[n_{uv}\sum_{w\in V} A_{uw}A_{vw}\]
\end{description}

\noindent\textbf{Regular equivalence}\hfill\\
Two vertices are regularly equivalent if they are equally related to equivalent others.

In terms of role assignment (colouring): nodes of the same colour have neighbours of similar colours. Only special colourings permit this (slide~10).


Requires similar patterns in the vertex connectivity .

Recursive definition:
\[\sigma_{uv} = \alpha \sum_{s,t\in V} A_{us} A_{vt} \sigma_{st} + \delta_{uv} = \alpha \sum_{s\in V} A_{us} \sum_{t\in V} \sigma_{st} A_{tv}' + \delta_{uv}\]
where $\sigma_{uv}$ is the similarity score. In the matrix notation $\Sigma = \alpha A\Sigma A' + I$.

Another similarity metric
\[\sigma_{uv} = \sum_{t\in V} A_{ut}\sigma_{tv} + \delta_{uv}\]
which in the matrix form is $\Sigma = A \Sigma + I$.

Similarity is extremely important!

\subsection{SimRank} % (fold)
\label{sub:simrank}
Let $s(u,v)$ be the similarity metric between two nodes, and $i(\cdot)$ be the set of incoming neighbours. Then $s()$ is defined as 
\[ s(u,v) = \frac{C}{\abs{i(u)}\abs{i(v)}} \sum_{s\in i(u)} \sum_{t\in i(v)} s(s,t)\]
with $s(u,u)=1$. In the matrix notation
\[ S = \brac{\frac{C}{k_u k_v}}_{u,v}\odot A' S A\]

Staring the random walks from nodes $u$ and $v$, which follow the edges in the opposite direction (not to each other!), then $S_{uv}$ is the expected number of steps before the walks meet.

% subsection simrank (end)

\subsection{Mixing} % (fold)
\label{sub:mixing}

Suppose nodes have certain properties, label, colours of whatever.
Mixing means patterns in connectivity
\begin{description}
	\item[Like-to-like] Assortiative mixing: attributes of connected nodes tend to be more similar rather than disconnected;
	\item[Opposite-to-opposite] Disassortative mixing: the presentce of an edge between vertices makes attributes more likely to have low similarity.
\end{description}

Mixing is allowed with respect to any attribute, be it observed or latent., or even vertex degree (as in preferential attachment).

How much more often do attributes match across edges than expected at random? The modularity metric
\[ Q = \frac{m_c-\brkt{m_c}}{m} = \frac{1}{2m}\sum_{u,v\in V} \Big( A_{u,v} - \frac{k_u k_v}{2m} \Big) \delta(c_u,c_v)\]
where $c:V\to \mathcal{C}$ is the vertex labelling, class or attributes, and $m_c$ -- number of edges with the same attributes, and $\brkt{m_c} =\Ex(\#_c)$.

The term $\frac{k_u k_v}{2m}$ is responsible to capturing the expected number or connections had the graph been random.

\subsubsection{Mixing with scalars} % (fold)
\label{ssub:mixing_with_scalars}

Suppose $X:V\to \Real$ is a random variable on the vertices. Average and covariance over edges
\[\brkt{x} = \frac{\sum_{v\in V}k_v x_v}{\sum_{v\in V}k_v} = \frac{\sum_{v\in V}k_v x_v}{2m} = \frac{1}{2m}\sum_{v,u\in V}A_{uv} x_v\]

%%% Missed some slides~19
% subsubsection mixing_with_scalars (end)

Node degree as an attribute.

Disassortative mixing -- high degree vertices are connected to low degree nodes, yields a start like structure.
Assortative mixing -- interconnected high degree nodes -- the core, low degree vertices -- periphery,

% subsection mixing (end)

% section lecture_7 (end)

\section{Lecture \# 8} % (fold)
\label{sec:lecture_8}

Network communities, Graph cliques, k-plex and k-cores.

What might be a good definition of a ``community'' in a graph?

First run a connected component extraction procedure, and then work with each connected component separately.

Main properties of a community \begin{itemize}
	\item mutuality of connections (ties);
	\item compactness: reachability in few number of steps;
	\item High density of edges within the group;
	\item Separation of a group from other groups.
\end{itemize}

A clique a complete fully connected subgraph of $G = (V,E)$.
Every pair of node connected by an edge is a $2$-clique.

A \textbf{maximal} clique is a complete subgraph which cannot be expanded by adding any adjacent node. A \textbf{maximum} clique is the largest possible clique in a given graph. A clique number of a graph is the size of the maximal clique.

algorithms: \begin{itemize}
	\item finding a maximal clique of a given size $k$ is $O(n^kk^2)$;
	\item computing a maximum clique is $O(3^\frac{n}{2})$;
	\item but for sparse graphs finding a max-clique is easier.
\end{itemize}

Relaxing the definition of a clique, which does not require complete connectivity (like an $\epsilon$-clique?).

Maximum cliques might overlap, and yet might not provide insightful information on the community structure of the graph.

\subsection{Relaxation} % (fold)
\label{sub:relaxation}

A $k$-\textbf{plex} of size $n$ is a maximal subset of $n$ vertices such that each vertex is connected at least to $n-k$ other in the subset. The minimum degree is not less than $n-k$.

A $k$-\textbf{core} is a maximal subset of vertices such that each node is connected to at least $k$ other vertex in the subset.

% subsection relaxation (end)

A network community is a group of vertices similar to each other.

In community detection nodes are assigned to communities and the main requirement is assignment of a vertex to at most one community.

Similarity based vertex clustering:
\begin{itemize}
	\item Jaccard, cosine, Pearson, Euclidean distance (dissimilairty);
	\item group together vertices with high similarity.
\end{itemize}

Hierarchical clustering may be used to cluster graphs using the similarity-adjacency matrix.
\begin{itemize}
	\item Assign each node to an individual group;
	\item Find two groups with the highest similarity and merge them:
		in order to calculate the similarity (aggregate) between groups it is possible to use: \begin{itemize}
			\item single-linkage (the most similar);
			\item complete-linkage (the least similar);
			\item single-linkage (mean similarity between members).
		\end{itemize}
	\item Repeat until all nodes have been joined into a single group.
\end{itemize}
This results in a dendrogramm of hierarchical clustering.

With a threshold on the similarity, the hierarchical clustering would produce a bunch of non-overlapping clusters.

nodes $\to$ similarity matrix $\to$ data mining.

\subsection{Graph partitioning} % (fold)
\label{sub:graph_partitioning}

Edges needed to be removed in order to split the graph in to pieces with as tightly connected subsets as possible


Graph partitioning is an NP hard problem: \begin{itemize}
	\item number of way to divide a network of $n$ nodes into two groups: $C^2_n$;
	\item Partitioning the graph into $k$ non-empty subgroups: Stirling number of the second kind \[\big[\begin{matrix}n\\k\end{matrix}\big]\];
	\item the number of all possible partitions is given by the Bell number.
\end{itemize}

\subsubsection{Heuristic approach} % (fold)
\label{ssub:heuristic_approach}

Focus on edges that connect communities (briges): use edge betweenness defines as the number of shortest paths $\pi_{st}$ through the edge $\epsilon$
\[C_b(\epsilon) = \sum_{s\neq t} \frac{\pi_{st}(\epsilon)}{\pi_{st}}\]
Construct communities by progressively removing edges.

\begin{itemize}
	\item[input] graph $G=(V,E)$;
	\item[output] dendrogramm;
	\item[output] Repeat until the set of edge is nonenpty:
	for all $e\in E$ compute $C_B(e)$ and remove the edge with the largest $C_B(e)$.
\end{itemize}

Community quality. Compare the fraction of edges within the cluster to expected fraction had edges been distributed at random:
\[Q = \frac{1}{2m}\sum_{ij} \big(\A_{ij} - \frac{k_ik_j}{2m} \delta_{c_i,c_j}\big)\]

% subsubsection heuristic_approach (end)

% subsection graph_partitioning (end)

% section lecture_8 (end)

\section{lecture \# 9} % (fold)
\label{sec:lecture_9}

Mid-term = analyse your own friend's network on the 24th of March.

\subsection{graph partitioning algorithms} % (fold)
\label{sub:graph_partitioning_algorithms}

A community is a group of nodes coupled between each other tighter that with the rest of the world.

We consider only the sparse graphs with $m<<n^2$ (ie. $m = O(n^\frac{1}{2})$). Community detection is NP-hard.
Properly designed and correct greedy algorithms approximate the true and correct solution. THe approach is recursive top-down ``divide-and-conquer''.

What is the criteria of the best partition?\begin{itemize}
	\item Minimize the ``communication'' between graphs partitions; Gives rise to the \textbf{Min}imum \textbf{cut}, which is not the best, since it could easily cut out singly connected vertices.
	\item 
\end{itemize}

Optimization criteria for a graph $G=(V,E)$ with $V = W\uplus U$:
\begin{description}
	\item[graph cut] \[\text{cut}(W, U) = \sum_{i\in U}{j \in W}e_{ij}\]
	\item[Ration cut] \[\frac{\text{cut}(W, U)}{|W|}  + \frac{\text{cut}(W, U)}{|U|} \]
	\item[Normalised cut] \[\frac{\text{cut}(W, U)}{\text{vol}(W)}  + \frac{\text{cut}(W, U)}{\text{vol}(U)} \]
	\item[Quotient cut] also known as \emph{Conductance}: \[\frac{\text{cut}(W, U)}{\min\big\{\text{vol}(W),\text{vol}(U)\big\}} \]
\end{description}
where the volume $\text{vol}(W) = \sum_{i\in W} \sum_{i\in V} e_{ij} = \sum_{i\in W} \delta_i$;

Using the linear algebra solve the mincut problem (remember Mirkin?).

Optimization methods:
\begin{itemize}
	\item Local search algorithms (greedy);
	\item Spectral graph partitioning;
	\item Randomized min cut;
\end{itemize}

% subsection graph_partitioning_algorithms (end)

\subsection{special clustering} % (fold)
\label{sub:special_clustering}

Suppose we have two sets of node $U$ and $W$ ($U\uplus W = V$) and let $s_v = 2\cdot1_{v\in W}-1$ the indicator vector of being a member of the either class.

The number of edges crossing the partition $U\vert W$:
\begin{align*}
	\text{cut}(U,W)
	&= \frac{1}{4}\sum_{i,j\in V} (s_i - s_j)^2 1_{(i,j)\in E} \\
	&= \text{summing each edge twice}
	&= \frac{1}{8}\sum_{i,j\in V} (s_i - s_j)^2 a_{ij} \\
	&= \frac{1}{4}\sum_{i,j\in V} (s_i^2 + s_j^2 - 2s_i s_j) a_{ij} \\
	&= \frac{1}{8}\sum_{i\in V} \sum_{j\in V} (s_i^2 + s_j^2) a_{ij} - \frac{1}{4}\sum_{i,j\in V} s_i s_j a_{ij} \\
	&= \frac{1}{8}\sum_{i\in V} s_i^2 \sum_{j\in V} a_{ij} + \frac{1}{8}\sum_{j\in V} \sum_{i\in V} s_j^2 a_{ij} - \frac{1}{4} \sum_{i,j\in V} s_i s_j a_{ij} \\ 
	&= \frac{1}{4}\sum_{i\in V} s_i^2 \delta_i - \frac{1}{4} \sum_{i,j\in V} s_i s_j a_{ij} \\
	&= \frac{1}{4} s'\Delta s - \frac{1}{4} s'As = \frac{1}{4} s'\big(\Delta - A \big)s
\end{align*}
where $\Delta$ is a diagonal matrix of vertex degrees: $\Delta_{ii} = \delta_i$ for $i\in V$.

The matrix $\Delta-A$ is called the Laplacian matrix, and it is the discrete version of the Laplacian operator -- second derivative operation. (The finite difference method).

The graph cut problem is to minimize $Q(s) = \frac{s'Ls}{4}$ with respect to $s\in \{-1,1\}^V$ and subject to $\big \lvert \sum_{i\in V}s_i \big\rvert < B$. Unfortunately this problem is that of integer minimization, which is $NP$-hard. With balancing constraint this requires $\frac{n!}{\big(\frac{n}{2}\big)!}$ (shouldn't this be the number of paired brackets?)

Let's solve an approximation -- a relaxed problem:
consider $s\in [-1,1]^V$ subject to an $L_2$ penalty, $\nrm{s}_2 = |V|$, and $\sum_{j\in V} s_j = 0$. Finally having some optimal $\hat{s}$ project in onto $\{-1,1\}^V$ with $\text{sign}(s_j)$.
In fact the original problem necessarily solves this one, but not the other way around.

The optimization problem:
\[\frac{1}{4} x'Lx - \frac{1}{4}\lambda(x'x - |V|) - \frac{1}{2}\mu x'1 \to \min_{x,\mu,\lambda}\]
The FOC is $\frac{1}{2}Lx - \frac{1}{2} \lambda x - \frac{1}{2}\mu 1 = 0$, $x'1 = 0$ and $x'x=|V|$, while the SOC is $\frac{1}{2}L - \frac{1}{2}\lambda I_V$.

Therefore $Lx = \lambda x + \mu 1$ and $x'Lx - \lambda x'x = 0$ whence $x'Lx = \lambda |V|$. Also $1'Lx = \lambda 1'x + \mu 1'1 = \mu |V|$. Thus
\begin{align*}
	Lx &= x \lambda + 1 \mu\\
	x'Lx &= x'x \lambda + x'1 \mu = x'x \lambda\\
	1'Lx &= 1'x \lambda + 1'1 \mu = 1'1 \mu
\end{align*}
and the $x=0$ does not work. Hence $\lambda = \frac{x'Lx}{x'x}$ and $\mu = \frac{1'Lx}{1'1} = \frac{x'L1}{1'1}$. Therefore the goal is to find $x$ such that 
\[Lx = x \frac{x'Lx}{x'x} + 1 \frac{1'Lx}{1'1}\]
since $x'x = |V|$ and $1'1=|V|$, this reduces to
\[(|V|I_V - x x' - 1 1' ) Lx = 0\]

% The fiddler vector.

The minimization of the Rayleigh-Ritz quotient
\[\min_{x\perp x_1}\frac{x'Lx}{x'x}\]
where $x_1$ is the eigenvector with the least eigenvalue $\lambda = 0$.

The number of zero eigenvalues is the number of connected components.

For a totally connected graph $\lambda_2$ -- the second smallest eigenvalue, is always $1$.

So the basic idea is to find the second least eigenvector of the graph laplacian.

For a min cut solve $Lx = \lambda x$, for the normalised cut solve $Lx=\lambda D x$.

To actually partition, use the zero-thresholding rule.

% subsection special_clustering (end)

another optimality criterion:
modularity
\[\text{mod} = \frac{1}{2m} \sum_{u,v\in V} \Big(A_{u,v} - \frac{k-vk_u}{2m}\Big)\delta_{c_u=c_v}\]
where $c_v$ is the class of the node $v$. In a two-cluster case 
\[\delta_{c_u=c_v} = \frac{1}{2}(s_u s_v + 1)\]
whence the modularity becomes
\[\text{mod} = \frac{1}{4m}s'Bs\]
where $s\in \{-1,1\}^V$.
Relaxation: $s's = |V|$ and therefore we get
\[\frac{1}{4m} x'Bx - \lambda(x'x - |V|)\]
with $B=A - K'(2m)^{-1}K$.

Since the modularity metric is maximized, we seek the eigenvector with the largest eigenvalue.

% Critiques of modularity: the 

In application do recursive splitting.


% section lecture_9 (end)

\section{Lecture \# 10} % (fold)
\label{sec:lecture_10}
\subsection{Community detection (continued)} % (fold)
\label{sub:community_detection_continued}

\begin{itemize}
	\item vertex clustering method (vertex similarity);
	\item graph partitioning (sparse cuts).
\end{itemize}

What about fuzzy clustering? Or overlapping communities (multiple allegiances)? In an egocentric graph, you is the point of community overlap.

One edge overlap, multi-node overlap.

The $k$-clique algorithm.

A $k$-clique is a clique with $\geq k$ nodes. A $k$-clique community is a union of all $k$-cliques which can be reached through a series adjacent $k$-cliques. $k$-cliques are adjacent iff they share $k-1$ nodes (the degree of overlap). Varying the overlap degree allows relaxation of the community detection.

Finding $k$-cliques is NP hard.
\begin{itemize}
	\item Find all maximal cliques;
	\item Create a new meta-graph made from cliques from the clique overlap matrix;
	\item set a threshold ($k-1$) and select the remaining overlaps.
\end{itemize}

\textbf{kNN} label propagation
Decide on the label of a node based on $k$ of its nearest neighbours.
\begin{itemize}
	\item Initialize the labels on all nodes randomly;
	\item Iterate in the order of a random permutation:
	\item For every node replace its label with the most frequent label among its neighbours with uniform tie-breaking;
	\item Stop when no relabelling takes place.
\end{itemize}

Fast community detection
\begin{itemize}
	\item Assign every node to its own community;
	\item Phase I: \begin{itemize}
		\item For every node evaluate modularity gain from removing this vertex from its community and placing it in the community of its neighbour;
		\item Place the node in the community with the higherst modularity gain;
		\item repeat until saturation (impossible to make a node transfer). 
	\end{itemize}
	\item Phase II: \begin{itemize}
		\item Nodes in communities are fused together into ``super-node'' prerequisites;
		\item
	\end{itemize}
	\item Repeat until convergence;
\end{itemize}

% subsection community_detection_continued (end)

\subsubsection{Random walk community detection} % (fold)
\label{ssub:random_walk_community_detection}

\noindent\textbf{Walktrap community}. Use the random walk to calculate the similarity between the vertices;

At each time step compute the stochastic transition kernel $P = D^{-1}A'$ with $D=\text{diag}\big(\delta_i\big)$ where $\delta_i = \sum_j A_{ij}$. The probability $P_{ij}$ is the chance of moving from $j$ to $i$. Then $P^t$ is the matrix showing the probability of moving from $i$ to $j$ in $t$ steps.

The main assumption is that for nodes $i$ and $j$ in the same community the probability $P_{ij}^t$ is high and $P_{ik}^t \approx P_{kj}^t$ for all $k$.  

The diffusion distance between the nodes $I$ and $j$ is 
\[ r_{ij}(t) = \sqrt{\sum_{k=1}^n\frac{(P_{ik}^t - P_{kj}^t)}{\delta_k} } = \nrm{ D^{-\frac{1}{2}}P^t - P^tD^{-\frac{1}{2}} }\]

between communities $A$ and $B$:
\[ r_{AB}(t) = \sqrt{\sum_{k=1}^n\frac{(P_{Ak}^t - P_{kB}^t)}{\delta_k} } = \nrm{ D^{-\frac{1}{2}} P_A^t  - P_B^t D^{-\frac{1}{2}} }\]

%% Missing 6 slides until the 24th

Local clustering algorithm
\begin{itemize}
	\item For $t=1\ldots t_M$ compute $q_t = M r_{t-1}$ and set
	$r_t(i) = q_t(i)$ if $q_t(i) > \epsilon \delta_i$, and zero otherwise.
	\item order the nodes according to the ratio $\frac{q_t(i)}{\delta_i}$.
	\item MISSING from slide 24.
\end{itemize}
No need to deal with the entire graph.



% subsubsection random_walk_community_detection (end)

% section lecture_10 (end)

\end{document}
