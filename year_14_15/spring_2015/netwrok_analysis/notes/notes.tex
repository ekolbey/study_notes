\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx, url}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xfrac, mathptmx}

\newcommand{\obj}[1]{{\left\{ #1 \right \}}}
\newcommand{\clo}[1]{{\left [ #1 \right ]}}
\newcommand{\clop}[1]{{\left [ #1 \right )}}
\newcommand{\ploc}[1]{{\left ( #1 \right ]}}

\newcommand{\brac}[1]{{\left ( #1 \right )}}
\newcommand{\induc}[1]{{\left . #1 \right \vert}}
\newcommand{\abs}[1]{{\left | #1 \right |}}
\newcommand{\nrm}[1]{{\left\| #1 \right \|}}
\newcommand{\brkt}[1]{{\left\langle #1 \right\rangle}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\newcommand{\Real}{\mathbb{R}}
\newcommand{\Cplx}{\mathbb{C}}
\newcommand{\Ntrl}{\mathbb{N}}
\newcommand{\Pwr}{\mathcal{P}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\Ex}{\mathbb{E}}

\newcommand{\defn}{\mathop{\overset{\Delta}{=}}\nolimits}

\usepackage[english, russian]{babel}
\newcommand{\eng}[1]{\foreignlanguage{english}{#1}}
\newcommand{\rus}[1]{\foreignlanguage{russian}{#1}}

\title{Structural analysis and visualization of networks}
\author{Nazarov Ivan, \rus{101мНОД(ИССА)}\\the DataScience Collective}
\begin{document}

\selectlanguage{english}

\maketitle

\begin{description}
	\item[Email] \hfill \\
	lzhukov@hse.ru
	\item[Course webpage] \hfill \\
	\url{http://www.leonidzhukov.net/hse/2015/networks/}
	\url{http://www.leonidzhukov.net/hse/2014/socialnetworks/}
\end{description}

Programming: iPython notebooks
Visualization: yEd, Gephi

Linear algebra prerequisites:
	Spares matrices
	Eigenanalysis


Graph $G=(V,E)$. The set $V$ is the set of vertices and $E$ is a subset of $V\times V$.
An element $(u,v)\in E$ is an edge starting at $u$ and ending in $v$.
The incidence matrix is defined as $a_{ij}=1_E\brac{(i,j)}$, so it denotes and edge $i\to j$.

Random graphs are pure mathematics theory created by Erd\"os and Renyui.
Statistical physics for analysis of complex networks.

Network, social network, complex network just another name for a graph.

Power law (scale free) few vertices with high degree, many nodes with few neighbours.

Complex means that reduction of a system actually destroys the systems.
Cannot predict the whole by the studying the parts.

Facebook network -- many worlds network -- typical for a power law random graph.

Complex networks usually have the following characteristics: \begin{enumerate}
	\item Power law distribution of the vertex degree;
	\item Small diameter and average path length;
	\item High propensity to cluster: the number of triangles in the network.
\end{enumerate}

Let's introduce the following local features of the graph: the vertex degree $\delta^+,\delta^-:V\to \Ntrl$
\begin{align*}
	\delta^+(v) &\defn \#\obj{\induc{u\in V}\, (u,v)\in E }\\
	\delta^-(v) &\defn \#\obj{\induc{u\in V}\, (v,u)\in E }
\end{align*}

``Any two people are on average separated no more than by six intermediate connections.''
\begin{itemize}
\item ``The small-world problem'', Stanley Milgram, 1967
\item ``An experimental study of the small-world problem'', Travers J., Milgram S., 1969
\end{itemize}

Bethe lattice $(V,E)$ is an infinite cycle-free graph with every node having the same number of neighbours $z > 1$.
Given an edge $(v,u)\in E$ the end vertex $u$ is connected to $z-1$ other neighbours, which means that $N_k = N_{k-1}\cdot (z-1)$ with $N_1 = z$, since the centre vertex is connected to $z$ other vertices.
Thus $N_k = z \brac{z-1}^{k-1}$.
And the total number of nodes is \[S_n \defn 1+\sum_{k = 1}^n N_k = 1 + z \frac{\brac{z-1}^n-1}{(z-1)-1}\] (check this!)

For any network $G=(V,E)$ its \textbf{order} is $\abs{V}$ and size is $\abs{E}$.

% As for the format of homework, python + \Latex is acceptable.

% Homework: complete the notebook.!!! -- Completed on 2014-12-17

\section{Lecture \# 2} % (fold)
\label{sec:lecture_2}

THe power law distribution is very natural in the study of networks:
\[p(x) \defn \frac{C}{x^\beta} 1_{\clop{x_0,+\infty}}(x)\]
where $\beta > 0$ is the power.

The indefinite integral of $p(x)$ is equal to
\[\int p(x) dx = \begin{cases}
    C \frac{x^{1-\beta}}{1-\beta},& \text{if } \beta\neq 1\\
    C \ln x, & \text{if } \beta = 1
\end{cases}\]

If $\beta > 1$ then $p(x)$ is a density function and the normalisation constant is determined by
\[\induc{C \frac{x^{1-\beta}}{1-\beta}}^\infty_{x_0} = C \brac{ - \frac{x_0^{1-\beta}}{1-\beta}} = 1\]
whence $C\defn (\beta - 1) x_0^{\beta-1}$. In its final form the density looks like:
\[p(x) \defn \frac{\beta - 1}{x_0} \brac{\frac{x}{x_0}}^{-\beta} 1_{\clop{x_0,+\infty}}(x)\]

The \textbf{survival} function $H(x) = 1 - F(x)$ is useful in network analysis. For the power law the survival function is \[H(x) \defn \brac{\frac{x_0}{x}}^{\beta-1}\]

Median is more suitable for estimating the power law distribution parameters. The quantile function is given by:
\[q_\alpha \defn \brac{1-\alpha}^{-\frac{1}{\beta - 1}} x_0 \]

Power law is scale invariant. Indeed 
\[H(s x) = \brac{\frac{x_0}{s x}}^{\beta - 1} = \brac{s}^{1-\beta} H(x)\]

Consider the node degree distribution of an undirected network.
There will be a majority of nodes with low degree,
and there will be very few vertices with extremely high degree.

Node degree -- the number of nearest neighbours
Degree distribution \[P(k) \defn \frac{n_k}{\sum_{k}n_k}\]
the model is $P(k) = C k^{-\gamma}$.
Normalization -- the Riemann Zeta function.
\[C \sum_{k\geq 1} k^{-\gamma} = 1 \Leftrightarrow C \defn \frac{1}{\zeta(\gamma)}\]

In maximum likelihood vary $x_0$ to get the idea of how $\alpha$ depends on it.

Use Kolmogorov-Smirnov test against the exponential distribution to find the ``best'' values of $x_0$.
Look for the minimal value of the K-S statistic and choose $x_0$.

% read the references

\subsubsection{The maximum likelihood estimation} % (fold)
\label{ssub:the_mle}

Suppose $\brac{x_i}_{i=1}^n$ is an iid sample from some random variable distributed according to the power law.
The log-likelihood is given by \[\log\mathcal{L} = \sum_{k=1}^n \beta \log\frac{x_0}{x_k} + n \log\frac{\beta - 1}{x_0}\]
where $x_0\leq \min_{k=1\ldots n}x_k$, since otherwise the log-likelihood would be $-\infty$.
The first-order conditions are given by \begin{align*}
	- \sum_{k=1}^n \log\frac{x_k}{x_0} + \frac{n}{\beta - 1} = 0\\
	n \brac{ \beta - 1 } \frac{1}{x_0} > 0
\end{align*}

Therefore $\hat{x}_0 \defn \min_{k=1\ldots n}x_k$ and the ML-optimal estimator of $\beta$ is
\[\hat{\beta} \defn 1 + \frac{n}{\sum_{k=1}^n ( \log x_k - \log x_0 )}\]

% subsubsection the_mle (end)

\subsubsection{The OLS estimation} % (fold)
\label{ssub:the_ols_estimation}

Another possibility is to estimate the parameters of the power law by means of a regression of the frequencies of binned sample on the ``typical'' values of the bins.

Suppose $\brac{f_i}_{i=1}^B$ and $\brac{b_i}_{i=1}^B$ are bin frequencies and centres respectively, constructed on data $\brac{x_k}_{k=1}^n$ which supposedly came from a power law distribution.

Then it is possible to fit the following regression model to the histogram data to get the parameters of the law:
since $f_x \sim p(x) \Delta x$
\[\log f_i \sim \brac{\log{(\beta-1)} + (\beta-1) \log x_0} + (-\beta) \log b_i\]
It is a fast but a rather crude way of estimating.

% subsubsection the_ols_estimation (end)

% section lecture_2 (end)

\section{Lecture \# 3} % (fold)
\label{sec:lecture_3}

Graph models

Erd\"os-Rnyi model

A random graph is a element of the set $\prod_{\omega\in \Pwr_2(V)} \obj{0,1}$.
A large collection of Bernulli random variables, indicating whether an edge is present or not.

$G_{n,m}$ model -- a graph is randomly selected form a set  $C_N^m$ graphs, with $N\defn \frac{n(n-1)}{2}$ of with $n$ nodes and $m$ edges.

$G_{n,p}$ -- the model of a random graph in which an edge between
any two vertices is established with probability $p$. The total
number of edges is again $N$. The size of the graph is a thus a
random number.

The models are asymptotically equivalent.

The mean node degree is given by
\[\epsilon(G) = \frac{2 \Ex(m)}{n} = p\frac{2 n(n-1)}{2 n} \approx p n\]
The graph density is the \[\rho \defn 2\frac{\Ex(m)}{n(n-1)}\]

The degree distribution of a random graph.

The chance that a given node $v$ has $k$ neighbours is given by:
\[\pr\brac{\delta(v) = k} = C^k_{n-1} p^k \brac{1-p}^{n-k-1}\]

Since the binomial distribution is asymptotically poisson if $p n = \lambda$ and $n\to \infty$:
\[C^k_{n-1} p^k \brac{1-p}^{n-k-1} \to \]

Use these models as a benchmark (in the null hypothesis) to compare against the phenomena in the real life.

Treating the model parametrically to witness phase transitions.
\begin{description}
	\item[$p=0$] \hfill \\ the graph is a set of isolated vertices;
	\item[$p=p_c$] \hfill \\ the graph obtains a spanning tree;
	\item[$p=1$] \hfill \\ the graph is totally connected.
\end{description}

Suppose $p$ changes with $n$. 


The chance that a node does not belong the the Giant Connected Component,
in the case of Poisson node degree distribution is given by
\[ u = \sum_{k\geq 0} \pr(k) u^k = \sum_{k\geq 0} e^{-\lambda}\frac{\lambda^k}{k!}u^k = e^{\lambda (u-1)}\]
the necessary condition for a nonzero $u$ is that $\lambda>1$
(ensures that the graph of the exponent intersects the $45\,^{\circ}$ line at a nonzero point).

\begin{description}
	\item[$p\to p_c-$] \hfill \\ no components larger than $O(\log n)$;
	\item[$p=p_c$] \hfill \\ the largest component has $O(n^\frac{2}{3})$;
	\item[$p\to p_c+$] \hfill \\ the gigantic connected component is of the order $O(n)$.
\end{description}
The critical values is $p_c n = 1$.

Threshold values at which interesting phenomena appear -- subgraphs of order $g$:
\begin{description}
	\item[$p\sim n^\frac{-g}{g-1}$] there almost surely exists a tree of order $g$;
	\item[$p\sim n^{-1}$] there is a cyclce of order $g$;
	\item[$p\sim n^\frac{-2}{g-1}$] a clique of order $g$ (a complete subgraph).
\end{description}


The clustering coefficient: ratio of closed triangles to all possible triangles
\[C(k) = \frac{p k(k-1) 2 }{ k( k-1 ) 2} = p\]
The sparser the graph the more negligible is the clustering coefficient.

The average number of nodes $s$ hops away from the current node is $\lambda^s$.
In the critical regime of the edge probability, all nodes belong to the the GCC with high probability.
Thus $d \sim \frac{\log n}{\log \lambda}$.

Fixing the issues with degree distribution.
The configuration model.
Take $V$ vertices and a finite sequence of node degrees $\brac{d_v}_{v\in V}$ with $\sum_{v\in V} d_v$ -- even.

% Slide~17.
The constructed graph might be a lopped multigraph.
Solution: use special graphical degree sequences.
The probability that $v,u\in V$ are connected is given by
\[p_{uv}\frac{d_u d_v}{2 m - 1}\]

Does not allow for an asymptotically non-zero clustering coefficient.

Very different from the Erd\"os-Renyi model!

A random graphs does not have communities.

Non-zero clustering coefficient hint at non-random processes underlying the edge formation.

% section lecture_3 (end)

\section{Lecture \# 4} % (fold)
\label{sec:lecture_4}

Dynamical grove (?) model.

Small Worlds model [Watts and Strogatz; 1998]

Previously considered networks were static fixed number of vertices and edges.

Let's have look at evolving graphs.
Growing networks (evovling with time)
\begin{itemize}
	\item Citation networks;
	\item Collaboration networks;
	\item Web;
	\item Social networks.
\end{itemize}

Consider pure growth model of an network (random growing network)
\begin{itemize}
	\item $t=0$ there are $n_t$ unconnected nodes;
	\item growth . on every time step $t\geq 1$ add a node with $m\leq n_0$ edges $k_t(t) = m$ attached randomly to the existing nodes;
	\item from $m$ edges between (?) existing nodes uniformly at random \[\Pi(k_i) = \frac{1}{n_0+t-1}\] (? probability of an edge getting connected to some existing node)
\end{itemize}

The expected degree of any node $i$ (the time the node was introduced):
\[k_i(t) = m + \sum_{k=0}^k\frac{m}{n_0+i-1 + k}\]

for $t>>n_0$
\[k_i(t) \aaprox m \brac{1 + \log\frac{t}{i}}\]

The longer the node has been in the system, the more degree it has: ``the first move-in advantage''.

If interpreted as a function of the time of join $i$ for a fixed $t$ (a snapshot), then it can show that a node the more recent nodes have lesser chances of having a high degree.


Find the nodes that at time $t$ have degree less than $k$: $\obj{\induc{i}\,k_i(t) < k}$

\[k_i(t) = m\brac{1+\log\frac{t}{i}}<k\,\implies\, i . t e^\frac{m-k}{m}\]

Fraction of nodes with degrees $k_i(t)<k$:
\[F(k) = \Pr\brac{k_i(t)<k} = \frac{n_0+t-i}{n_0+1} \approx 1 - e^\frac{m-k}{m}\]
Thus the degree distribution is approximately exponential.


Another way of getting at the same result: use continuous approximation!

Since $\Pi(k_i) = \frac{1}{n_0+t-i} \approx \frac{1}{t}$, the following must be true:
\[k_i(t+\delta t) = k_i(t) - \frac{m}{t}\delta t\]
whence the following first order differential equation emerges:
\[\frac{d}{dt}k_i(t) = \frac{m}{t}\]
with initial conditions $k_i(t=i) = m$.
the solution id $k_i(t) = m\brac{1+\log\frac{t}{i}}$.

% [Barabasi and Albert; 1999]
\begin{description}
	\item[$t=0$] there are $n_0$ nodes;
	\item[growth] on ever step add a node with $m$ edges ($m\leq n_0$), $k_i(i)=m$;
	\item[Preferential attachment] probability of linking it to an existing node $i$ is proportional to the node degree $k_i$
	\[Pi(k_i) = \frac{k_i}{\sum_j k_j}\]
\end{description}

Preferential attachment represents the heuristic rule that ``similar attract''.

\[k_i(t+\delta t) = k_i(t) + m \Pi(k_i)\delta t\]
\[\frac{d}{dt}k_i(t) = m \Pi(k_i) = m \frac{m k_i}{2 m t}\]
which has the solution: time evolution of a node $i$ degree
\[k_i(t) = m\brac{\frac{t}{i}}^\tfrac{1}{2}\]

Nodes with degree less than $k$: $\frac{m^2}{k^2} t<i$

The power law emerges:
\[F(k) = \Pr\brac{k_i(t)<k} = \frac{n_0+t-i}{n_0+1} \approx \frac{n_0+t-\frac{m^2}{k^2} t}{n_0+1} \approx 1 - \frac{m^2}{k^2} t\]


Barabasi model has heavier tails than the random graph: indeed it is obvious from the exponent versus hyperbola comparison.

Had such model been a reality there would be extremely many poor, and a few extremely rich, who get richer with time.

The average path length (analytical result):
\[\brkt{L}\approx \frac{\log N}{\log\log N}\]

Clustering coefficient (simulation result):
\[C\approx n^{-\frac{3}{4}}\]
$C$ is infinitesimal as $N\to \infty$.

Barabasi (1999) preferential attachment model has been rediscovered multiple times:
\begin{itemize}
	\item Yule process 1925;
	\item Polya's urn model;
	\item Herbert simon;
	\item Evolution of citation networks, cumulative advantage model; Derek de Solla Price, 1976;
\end{itemize}

\subsection{The small world model} % (fold)
\label{sub:the_small_world_model}

Devised by Watts and Strogatz in 1998.

Image a two-tier ring graph, with structural triangles.

% subsection the_small_world_model (end)
The motivation of the small world model: keep high clustering, get small diameter (the greatest distance between any two nodes).

Evolution is to construct shortcuts so as to decrease the average path length.

Strogatz proposed to re-wire the graph: pick any edge, sever it, and wire it elsewhere.

start with regular lattice with $n$ nodes, $k$ edges per vertex $k<<n$.
randomly connect with 

Small world region:
\begin{itemize}
	\item high clustering coefficient;
	\item small average path length.
\end{itemize}

% section lecture_4 (end)

\section{Lecture \# 5} % (fold)
\label{sec:lecture_5}

Centrality measures of graphs

Which vertices in the network are important?

Is it possible to order the nodes in the network, so that the first node is the mos important, and the last is the least important.

What is meant by importance?

let's have a look at graph theoretic understanding of the centrality.

\noindent\textbf{Definition}\hfill\\
THe \textbf{eccentricity} of vertex $u\in V$ is the maximum distance between the vertex $u\in V$ and any other $v\in V$ in the graph $G$ 
\[\text{ecc}(u) \defn \max_{v\in V}\induc{d}_G(u,v)\]

The diameter is really the largest eccentricity: the larges possible distance between any two nodes in a graph.
\[\text{diam}(G)\defn \max_{u\in V} \text{ecc}(u)\]

The radius of a graph is the minimum eccentricity
\[\text{rad}(G)\defn \min_{u\in V} \text{ecc}(u)\]

The \textbf{central point} of a graph $G$ is a vertex, with eccentricity equal to the radius of $G$:
\[\text{diam}(G)\defn \max_{u\in V} \text{ecc}(u)\]

A graph \textbf{centre} is the set of all central nodes of $G$. The \textbf{periphery} of $G$ is the set of nodes with eccentricities equal to the diameter of $G$.

The just defined notions are not robust with respect to ``small'' changes to the graph: the may undergo extreme changes when the graph is perturbed. But what change is ``small'' for a given graph? (adding a chain may actually change the topology quite significantly)


Periphery is a graph is used in the sense other than graph theoretic (may be with respect to the vertex degree distribution).

The notion of vertex importance (from Sociology): the location of the node in a social graph is correlated with the influence of that vertex in the network (the ``ties'').

Notions of centrality usually deal with undirected graphs, corresponding to symmetric relations. Well what about prestige network relations?


The degree centrality is just the number of nearest neighbours for a graph incidence matrix $A$:
\[C_D(u) = \detla_u = \sum_{v\in V} A_{uv}\]

Normalized graph centrality, defined as $C_D^*(u) \defn \frac{C_D(u)}{\abs{V}-1}$, is useful for comparison between graphs, not just within one graphs.

High centrality means direct contact with many other nodes. Low centrality degree -- an in active vertex, a peripheral vertex.

Calculate how close the node is to any other vertex in the system. This gives us the closeness centrality metric
\[C_C(u) \defn \frac{1}{\sum_{v\in V} d(u,v)}\]
It can be normalise by $\tfrac{}{\abs{V}-1}$.

Nodes in the centre can quickly interact with all others, have short communication path to others, and minimal number of steps to reach others.

There are problems for disconnected graphs, where infinite distances emerge. In theses cases use the harmonic centrality
\[C_H(u)\defn \brac{\sum_{v\in V} \frac{1}{d(u,v)}}^{-1}\]

The most interesting definition of centrality is betweenness centrality.

Consider the network as a set of interconnected communication hub. The most important nodes are those, which are indispensable for proper network operation. 

Consider a graph $G = \brac{V,E}$.
The betweenness centrality measure of $G$ is defined as follows:
\[C_B(u) = \sum_{s\neg t\neq u}\frac{\sigma_{st}(u)}{\sigma_{st}}\] 

The normalised version of betweenness is just scaled by
\[\frac{2}{(n-1)(n-2)}\]

It is possible to extend the betweenness centrality to edges.

Probability that a communication from $s$ to $t$ will go through $u$ (geodesics) is given by $\frac{\sigma_{st}(u)}{\sigma_{st}}$.

Usage of these measures greatly depends on what is understood by ``cnetrality''.

\subsection{Eigenvector centrality} % (fold)
\label{sub:eigenvector_centrality}

Consider a recursive definition of centrality: An important nodes have important neighbours and connections.

Let $\brac{I^{t+1}_u}_{u\in V}$ be the importance vector at some iteration $t\geq1$, with components corresponding to each node. Then define the importance as the limit of the following process
\begin{align*}
	\sum_{j\in V} A_{ij} I^t_j \to I^{t+1}_i \\
	\frac{1}{\lambda}\sum_{j\in V} A_{ij} I_j = I_i \\
\end{align*}

Thus if the iteration converge, then they converge to an eigenvector of the adjacency matrix.

A node with large centrality will be connected with nodes of large centrality. Thus highly important nodes are likely to have highly important neighbours.

It is a kind of \emph{self-reinforced centrality}.

% subsection eigenvector_centrality (end)

\subsection{Katz centrality} % (fold)
\label{sub:katz_centrality}

Weighted count of the path coming to the node (Katz; 1953). The weight of a path of length $n$ is counted with attenuation factor $\beta^n$.
\begin{align*}
	k_u &= \sum_{v\in V} \brac{E_{uv} + \beta A_{uv} + \beta^2 A_{uv} + \ldots}\\
	k = \brac{\sum_{k\geq0} \beta^k A^k} \mathbf{1} \\
	k = \brac{E - \beta A}^{-1} \mathbf{1} \\
	\brac{E - \beta A} k = \mathbf{1} 
\end{align*}

The parameter $\beta$ controls the measure, if it is $1$ then Katz centrality reduces to the ... NO IT DOES NOT!

Two-parametric centrality measure

Let $\beta$ be the degree to which node's importance is a function of it's neighbours' importance, and $\alpha$ be a normalization parameter.
\[c_u(\alpha,\beta) \defn \sum_{v\in V}\brac{\alpha+\beta c_v} A_{uv}\]
In the matrix form:$c = \alpha A\mathbf{1} + A\mathbf{1} c$.

% subsection katz_centrality (end)

\textbf{Centralization} of the whole network is measure of how ``cnetral'' is the most central node in relation to all other vertices.

%% Slide p.~18
\[C_x(G) \defn \frac{\sum_{v\in V} C_x(\bar{u}_G) - C_x(v) }{\max_H \sum_{v\in V} C_x(\bar{u}_H) - C_x(v)}\]
where $C_x$ is any vertex centrality measure and $\bar{p}_G$ is the central node of the graph $G$. The maximum is take over the graph of the same order as $G$.

Prestige is a measure of centrality of directed graphs.
Degree prestige $\delta_{\text{in}}(u)$ is the vertex in-degree.

The main contribution of the Page Rank algorithm is in how to deal with ``directedness'' of the graph.

It is insightful to compare the centrality measures between each other and see how they are correlated.
\begin{itemize}
	\item Pearson's correlation measures strength of linear association;
	\item Spearman's rank correlation -- strength of monotone relationship;
	\item Kendall $\tau$, count the number of pairwise agreements and disagreements between a pair of ranked data series. 
	\[2\frac{n_c - n_d}{n(n-1)}\]
	$n_c$ is the number of concordant pairs, and $n_d$ -- discordant pairs (agreements $\tau=1$, disagreements $\tau=-1$).
\end{itemize}

% section lecture_5 (end)

\section{Lecture \# 6} % (fold)
\label{sec:lecture_6}

\subsection{WEB search algorithms} % (fold)
\label{sub:web_search_algorithms}



% subsection web_search_algorithms (end)

% section lecture_6 (end)

\end{document}
