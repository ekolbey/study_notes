{
 "metadata": {
  "name": "",
  "signature": "sha256:879dd31853c0170d8401da1ba98148918d9d4efda148fd0a26aa71677081357e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<center>Structural Analysis and Visualization of Networks</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<center>Home Assignment #2: Network models</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<center>Student: *Nazarov Ivan*</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<hr />\n",
      "General Information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Due Date:** 18.02.2015 23:59 <br \\>\n",
      "**Late submission policy:** -0.2 points per day <br \\>\n",
      "\n",
      "\n",
      "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *Nazarov* *Ivan* HA*1***\n",
      "\n",
      "Support your computations with figures and comments. <br \\>\n",
      "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr \\>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Problem statement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Task 1\n",
      "\n",
      "Consider Barabasi and Albert dynamical grow model. Two main ingredients of this model are *network growing* and *prefferential attachment*. Implement two restricted B&A-based models:  \n",
      "\n",
      " **Model A**  \n",
      "  Lack of prefferential attachment, that is at each time-step form edges uniformly at random while network keeps growing.\n",
      "\n",
      " **Model B**  \n",
      "  Lack of growing, that is fix total number of nodes, on each time-step randomly choose one and form edges with prefferential attachment."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Task 2\n",
      "Consider the following  \"Vertex copying model\" of growing network.\n",
      "\n",
      "At every time step a random vertex from already existing vertices is selected and duplicated together with all edges, such that  every edge of the  vertex\n",
      "* is copied with probability $q$\n",
      "* is rewired to any other randomly selected vertex with probability $1-q$\n",
      "\n",
      "\n",
      "Starting state is defined by some small number of randomly connected vertices.\n",
      "\n",
      "The model can generate both directed and undirected networks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Objectives\n",
      "1. Generate networks according to the models above ($N > 1000$ nodes)\n",
      "2. Compute CDF/PDF, describe the distribution and compute\\describe its properties.\n",
      "3. Illustate the following dependencies: \n",
      "    * average path length to the number of nodes\n",
      "    * average clustering coefficient to the number of nodes\n",
      "    * average node degee to the nodes \"age\"\n",
      "4. Is scale-free property conserved in these models?\n",
      "\n",
      "Analyse results with respect to various parameter settings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr/><hr/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Solution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Preamble\n",
      "We shall include all the necessary modules here, before any analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## As usual, attach the numpy and the netwrokx modules\n",
      "import networkx as nx\n",
      "import numpy as np\n",
      "import numpy.random as rnd\n",
      "import numpy.matlib as ml\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "import scipy.sparse as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The original model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First and foremost, but solely for reference purposes, let's state the Barabasi-Albert dynamical growth model in its original form (**Barabasi, Albert (1999)**).\n",
      "\n",
      "The evolution of the network is represented by a sequence of graphs ${(G_n)}_{n\\geq0}$ with $G_n = \\big(V_n, E_n\\big)$.\n",
      "The process starts with a graph $G_0$ of $m_0$ nodes. The initial number of vertices is usually a small number.  At every moment $n$ the network $G_n$ undergoes evolution, which can be divided in two phases  \n",
      " * a new vertex $w'\\notin V_n$ is introduced into the network: $V_{n+1} = V_n \\cup \\{w'\\}$;\n",
      " * the vertex forms $m$ ($\\leq m_0$) links to pre-existing nodes chosen at random without replacement according to some distribution $D_n$ over the nodes $V_n$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the Barabasi-Albert growth model $D_n$ is the ``preferential connection'' distribution: for every $v\\in V_n$  \n",
      "\n",
      "$$\\mathbb{P}\\big(w'\\leadsto v\\big) = \\frac{\\delta_n(v)}{\\sum_{u\\in V_n}\\delta_n(u) }$$\n",
      "where $\\delta_n(v)$ is the degree of vertex $v$ in graph $G_n$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## The original attachment distribution of Barabasi-Albert\n",
      "def ba_valency( G, nbunch = None ) :\n",
      "## Get the distribution of the valency of the existing nodes base on the observed\n",
      "##  connectivity and degrees.\n",
      "    deg = G.degree( nbunch )\n",
      "## For some reason the call to $order( G )$ takes exceptonally long time to return\n",
      "##  But still, we need the numebr of edges here.\n",
      "    deg_sum = float( sum( deg.values( ) ) )\n",
      "## The vector of valencies is in the same order as the thr nbunch (or nx.nodes(.))\n",
      "    nbunch = nx.nodes( G ) if nbunch is None else nbunch\n",
      "    return np.fromiter( ( deg[ n ] / deg_sum for n in nbunch ),\n",
      "        count = len( nbunch ), dtype = np.float )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Analysis of Barabasi-Albert model\n",
      "#### Motivation\n",
      "\n",
      "In their exposition Barabasi-Albert never state that the initial network $G_0$ is connected, although it should be noted that, unless $m\\neq m_0$, the first ever attachment step brings about a connected star graph. Nevertheless, the model is stated as follows : (see p.551 c.1 L 30-35 of **Barabasi, Albert (1999)**) \n",
      "> To incorporate the growing character of the network, starting with a small number ($m_0$) of vertices, at every time step we add a new vertex with $m$ ($\\leq \udbff\udc14m_0$) edges that link the new vertex to $m$ different vertices already present in the system.\n",
      "\n",
      "Furthermore : (see p.551 c.1 L 40-42 of **Barabasi, Albert (1999)**)\n",
      "> After $t$ time steps, the model leads to a random network with $t\udbff\udc04 + m_0$ vertices and $mt$ edges.\n",
      "\n",
      "Note that had the initial network $G_0$ been connected, at least minimally, it would have had at least $mt + (m_0-1)$ edges by the end of step $t$. The $m_0-1$ term comes form the fact that the minimal spanning tree is the minimal connected graph."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, the case $m < m_0$ does not seem unnatural as it models the situation when the edge fromation is dominated by vertex discovery rather than preferential attachment.\n",
      "\n",
      "In light of this observation the previous definition of attachment probability dstribution suffers from the following drawback: isolated vertices remain almost surely isolated.\n",
      "\n",
      "In my opinion, this does not seem to be what authors had had in mind when they devised their model and attempted to check if both evolutionary phases were indeed necessary for the development of the power law scaling (see p.551 c.2 L 8-9 of **Barabasi, Albert (1999)** ) : \n",
      "> In model B, we start with $N$ vertices and no edges. At each time step, we randomly select a vertex and connect it with probability $\\Pi\udbff\udc0a(k_i) =\udbff\udc06 \\frac{k_i}{\udbff\udc0b\\sum_j k_j}$ to vertex $i$ in the system.\n",
      "\n",
      "This definition of attachment probabilities would have no effect on the Barabasi-Albert model and its type-B restriction even in the case when $m < m_0$, if the evolution began with a connected graph $G_0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "References:\n",
      "* Barab\u00e1si, A. L., & Albert, R. (1999). \"_Emergence of scaling in random networks._\" Science, 286(5439), 509-512.\n",
      "Chicago\t"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Bayesian preferential attachment\n",
      "Nevertheless it is easy to patch this \"vagueness\" in the model.\n",
      "Let's imagine for a moment that for some observed graph $G = (V,E)$ we are faced with the problem of estimating the probabilities ${(\\theta_v)}_{v\\in W}$ of getting attached to a node in some $W\\subseteq V$. Then the vertex degrees of the nodes in $W$ can be regarded as the evidence of attachement, whence the likelihood function should be given by:  \n",
      "\n",
      "$$L\\Big(G; {(\\theta_v)}_{v\\in W}\\Big) = \\prod_{v\\in W} \\theta_v^{\\delta(v)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Brief theory\n",
      "For this task of parameter estimation, let's employ Bayesian approach instead of the common MLE, since the latter tends to overfit the model because it fails to incorporate the uncertainty of the parameters.\n",
      "\n",
      "Cosider a prior $\\pi(\\cdot)$ on the space of possible probability vectors ${(\\theta_v)}_{v\\in W}$ -- a $(n-1)$-dimensional simplex $S^\\circ_{n-1}\\subseteq {[0,1]}^n$ where $n=\\big\\rvert W\\big\\lvert$. Using Bayes' theorem, the posterior distribution of ${(\\theta_v)}_{v\\in W}$ given the observed data is  \n",
      "\n",
      "$$p\\Big(\\Big.{(\\theta_v)}_{v\\in W}\\,\\big.\\big\\rvert\\,G\\Big.\\Big)\n",
      "\\propto p\\Big(\\Big.G\\,\\big.\\big\\rvert\\,{(\\theta_v)}_{v\\in W}\\Big.\\Big) \\pi\\Big({(\\theta_v)}_{v\\in W}\\Big)\n",
      "= L\\Big(G; {(\\theta_v)}_{v\\in W}\\Big) \\pi\\Big({(\\theta_v)}_{v\\in W}\\Big)$$\n",
      "\n",
      "In general it is convenient to pick a prior $\\pi(\\cdot)$ from an \"**eigen-family**\" of this prior-to-posterior tranfromation. Such prior families are know nas the **conjugate prior families**. For this particular kind of likelihood such a family is the **Dirichlet distribution** given by  \n",
      "\n",
      "$$\\text{Dir}\\big({(\\alpha_v)}_{v\\in W}\\big) = \\frac{\\Gamma\\big( \\sum_{v\\in W} \\alpha_v \\big)}{ \\prod_{v\\in W} \\Gamma\\big(\\alpha_v \\big)} \\prod_{v\\in W} \\theta_v^{\\alpha_v-1}$$\n",
      "\n",
      "It is easy to demonstrate that the posterior is $\\text{Dir}\\big({(\\alpha_v)}_{v\\in W} + {(\\delta(v))}_{v\\in W}\\big)$. This implies that the posterior probability of a random vertex being connected to a node $v\\in W$ is given by  \n",
      "$$p\\Big(\\Big. v\\,\\big.\\big\\rvert\\,G\\Big.\\Big)\n",
      "= \\int p\\Big(\\Big. v\\,\\big.\\big\\rvert\\,{(\\theta_v)}_{v\\in W}\\Big.\\Big) p\\Big(\\Big. {(\\theta_v)}_{v\\in W}\\, \\big.\\big\\rvert\\,G\\Big.\\Big) d\\mathbf{\\theta} = \\ldots $$  \n",
      "\n",
      "which in our case resolves into  \n",
      "$$\\ldots = \\int \\theta_v \\text{Dir}\\big({(\\alpha_v)}_{v\\in W} + {(\\delta(v))}_{v\\in W}\\big) d\\mathbf{\\theta}\n",
      "= \\frac{\\alpha_v + \\delta(v)}{\\sum_{u\\in W} \\alpha_u + \\delta(u)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Final probability vector\n",
      "Since there is no reason to assume any specific prior distribution of the values of the attachment probability vectors, let's use an uninformative prior, which assigna equal probability of every vector in the simplex. In the case of a Dirichlet family, this sets the hyperparameter ${(\\alpha_v)}_{v\\in W}$ of the prior to a vector of ones: $\\alpha_v=1$ for each $v\\in W$.\n",
      "\n",
      "Therefore the attachment probability distribution in $G_n$ is given by  \n",
      "\n",
      "$$p\\Big(\\Big. w'\\leadsto v\\,\\big.\\big\\rvert\\,G_n\\Big.\\Big) = \\frac{\\delta_n(v)+1}{\\big\\lvert V_n\\big\\rvert + \\sum_{u\\in S}\\delta_n(u)}$$  \n",
      "\n",
      "Listed below is the procedure, which computes the required distribution for a given graph $G$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Bayesian valency distribution. Funny thing is that this correction\n",
      "##  turns out to is\n",
      "def ba_bayes_valency( G, nbunch = None ) :\n",
      "## Get the degree of nodes\n",
      "    deg = G.degree( nbunch )\n",
      "## Get the adjusted sum of vertex degrees\n",
      "    nbunch = nx.nodes( G ) if nbunch is None else nbunch\n",
      "    deg_sum = float( sum( deg.values( ) ) + len( nbunch ) )\n",
      "## Compute the posterior distribution of vertex valency\n",
      "    return np.fromiter( ( ( deg[ n ] + 1.0 ) / deg_sum for n in nbunch ),\n",
      "        count = len( nbunch ), dtype = np.float )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayesian smoothing reduces the zero-degree penalty on the isolated vertices, thus enabling vertex discovery during the early stages of evolution. Notable thing is that for an uninformative prior the posterior distribution given $G$ is equivalent to the original attachment distribution for the same graph with an extra loop at every vertex."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The long run distribution\n",
      "Using the heuristic \"mean field\" approach it is possible to derive an approximation to the digree distribution un the long run.\n",
      "\n",
      "Suppose vertices are born at intervals $\\frac{1}{n}$ with $m$ edges. Then at time $t\\in [0,1]$ there are $nt$ vertices and the sum of all node degrees is $2mnt$. Across the span of an infinitesimal interval $[t, t+dt]$ extra $n dt$ vertices are born, each with $m$ edges.\n",
      "\n",
      "Suppose a vertex $v$ is born at time $s$ with $m$ edges and at time $t>s$ this node $v$ has degree $k_s(t)$.  Then during $[t, t+dt]$ the vertex $v$ gets an incoming edge with probability $n dt \\frac{k_s(t)+1}{2mnt + nt} = \\frac{k_s(t)+1}{(2m+1)t} dt$. Ov average thus $v$ gets $\\frac{m}{2m+1} \\frac{k_s(t)+1}{t}dt$ edges, which is by how much its degree increses :  \n",
      "\n",
      "$$ \\frac{k_s(t+dt)-k_s(t)}{dt} = \\frac{m}{2m+1} \\frac{k_s(t)+1}{t}$$\n",
      "\n",
      "Note that the degree depends on the time thevertex was born and not on the vertex itself."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the continuous time limit as $n\\to \\infty$ the following differential equation approximates the dynamics of degree of the vertex $v$ :  \n",
      "\n",
      "$$\\dot{k}_s(t) = \\frac{m}{2m+1} \\frac{k_s(t)+1}{t}$$\n",
      "\n",
      "with initial value at time of birth being $m$ : $k_s(t=s) = m$.\n",
      "\n",
      "The solution to this ordinary differential equation for these initial conditions is given by  \n",
      "$$\\delta_s(t) = (m+1) \\Big(\\frac{t}{s}\\Big)^\\frac{m}{2m+1}$$\n",
      "Put $\\alpha = \\frac{m}{2m+1}$ and note that $\\frac{1}{\\alpha} = 2 + \\frac{1}{m}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the continuous time limit the probability that at time $t$ a vertex born at $s$ has degree less than $k$ is given by \n",
      "$$F(k) = \\mathbb{P}(\\delta_s(t)\\leq k) = \\mathbb{P}\\Bigg(t \\Big(\\frac{m+1}{k+1}\\Big)^\\frac{1}{\\alpha} \\leq s\\Bigg) = \\frac{t - t \\Big(\\frac{m+1}{k+1}\\Big)^\\frac{1}{\\alpha}}{t} = 1 - \\Big(\\frac{m+1}{k+1}\\Big)^\\frac{1}{\\alpha}$$\n",
      "\n",
      "The distribution function is given by \n",
      "$$\\frac{d}{d k} F(k) = \\frac{1}{\\alpha} \\Big( k+1 \\Big)^{-\\frac{1}{\\alpha}-1} \\Big( m+1 \\Big)^\\frac{1}{\\alpha} = \\Big( k+1 \\Big)^{-3-\\frac{1}{m}} \\Big(2+\\frac{1}{m}\\Big) \\Big( m+1 \\Big)^{2+\\frac{1}{m}}$$\n",
      "\n",
      "Thus means that the limiting degree distribution of the Barabasi-Albert model with Bayesian valency behaves approximately like the power law with similar exponent as in the original model.\n",
      "\n",
      "For a more formally rigorous treatment of the original model see _**Bollobas, Riordan et al., (2001)**_."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "References:\n",
      "* Bollob\u00e1s, B., Riordan, O., Spencer, J., & Tusn\u00e1dy, G. (2001). \"_The degree sequence of a scale\u2010free random graph process._\" Random Structures & Algorithms, 18(3), 279-290."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model evolution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### _Complete model_\n",
      "A has been mentioned above the basic growth step in the complete Barabasi-Albert consisted of two phases:\n",
      " 1. growth;\n",
      " 2. attachment according to valency distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def barabasi_albert( n, m = 10, callback = None, avclu = False, avlen = False ) :\n",
      "## Make sure the callback is a proper function\n",
      "    callback_fn = callback if callable( callback ) else lambda G, _: None\n",
      "## Barabasi-Albert evolution always begins with the isolated graph\n",
      "    G = nx.empty_graph( m )\n",
      "## Identify vertices by consequitive integers. \n",
      "    t = G.number_of_nodes( )\n",
      "## To speed up the computations keep track of the node degree locally,\n",
      "##  but instead of using a dictionary, store them in a linear array.\n",
      "    degree = np.zeros( n + m, dtype = np.int ) ; degree_half_sum = 0\n",
      "## In the original Barabasi-Albert model the first new vertex\n",
      "##  is attached to all m vertices, thus making the graph connected.\n",
      "    U = range( m )\n",
      "## Note that the graph itself is used just to keep track of the edges.\n",
      "    result = list( )\n",
      "## Age-degree: the dependence of the average node degree on the age of a vertex\n",
      "    avg_age_deg = np.zeros( n + 1, dtype = np.float )\n",
      "## Shortest path matrix for Floyd Warshall\n",
      "    pi = 1 - ml.identity( n + m, np.float ) ; pi[ pi == 1 ] = np.inf\n",
      "## Setup the average clustering metric storage\n",
      "    avg_tri_clu = np.full( n + m, np.inf, dtype = np.float )\n",
      "    avg_path_ln = np.full( n + m, np.inf, dtype = np.float )\n",
      "    npmin = np.minimum\n",
      "    while t < n + m :\n",
      "## Add pending edges\n",
      "        G.add_edges_from( ( t, u ) for u in U )\n",
      "## Record the vertex degree\n",
      "        degree[ t ] += len( U ) ; degree[ U ] += 1\n",
      "        degree_half_sum += len( U )\n",
      "## Create a new vertex using stack allocation -- on top of others.\n",
      "        t = G.number_of_nodes( )\n",
      "## Run the Floyd-Warshall algorithm\n",
      "        if avlen :\n",
      "            if False :\n",
      "## Incrementally update the shortest path matrix by every new bidirectional edge\n",
      "                for u in U :\n",
      "## Compute the path through this edge\n",
      "                    path = pi[ :t, u ] + 1 + pi[ t-1, :t ]\n",
      "## Choose the optimal edge direction and whether the new path is the shortest\n",
      "                    pi[:t,:t] = npmin( pi[:t,:t], npmin( path, path.T ) )\n",
      "            else :\n",
      "## Add the shortest paths from the affected vertices to the new node\n",
      "                pi[ U, t-1 ] = pi[ t-1, U ] = 1\n",
      "## Compute the shortest path from the exisitng nodes to the new one\n",
      "                for u in U :\n",
      "#                     pi[:t,:t] = npmin( pi[:t,:t], pi[ :t, u ] + pi[ u, :t ] )\n",
      "                    pi[ :t, t-1 ] = npmin( pi[ :t, t-1 ], pi[ :t, u ] + pi[ u, t-1 ] )\n",
      "## Restore the distance symmetry due to bidirectionality\n",
      "                    pi[ t-1, :t ] = pi[ :t, t-1 ].T\n",
      "## At this point the matrix pi has the following property:\n",
      "##  * for nodes $i,j\\in V$ the value $\\pi_{ij}$ is the shortest path throught nodes other than $\\omega$.\n",
      "##  * for any $x\\in V$ the element $\\pi_{x\\omega}$ (and $\\pi_{\\omega x}) is the laength of the shortest\n",
      "##    path from $i$ to the new node $\\omega$ with intermediate vertices exclusively in $V$.\n",
      "## Compare the shortest paths through the new vertex with the current paths\n",
      "                pi[:t,:t] = npmin( pi[:t,:t], pi[ :t, t-1 ] + pi[ t-1, :t ] )\n",
      "            avg_path_ln[ t-1 ] = np.mean( pi[:t, :t], dtype = np.float64 ) * t / ( t - 1.0 )\n",
      "#             avg_path_ln[ t-1 ] = nx.average_shortest_path_length( G )\n",
      "#             assert( np.all( nx.floyd_warshall_numpy( G ) == pi[:t,:t] ) )\n",
      "## Compute the average clustering coefficient\n",
      "        if avclu :\n",
      "            avg_tri_clu[ t-1 ] = nx.average_clustering( G )\n",
      "## The evolution step is complete, call the required function\n",
      "        result.append( callback_fn( G, degree[ :t ] ) )\n",
      "## Compute the average node degree of created vertices of a given age.\n",
      "## A[0:t-m0] -- current mean degrees of vertices of age 0:t-m0 (right ends not included)\n",
      "## D[0:m0] -- the current degrees of the oldest (initial) m0 vertices\n",
      "## D[m0:t] -- the degrees of nodes created at iterations 0:t-m0 respectively\n",
      "        avg_age_deg[ :(t-m) ] += ( degree[ t-1:m-1:-1 ] - avg_age_deg[ :(t-m) ] ) / np.arange( t-m, 0, -1, dtype = np.float )\n",
      "## Now incorporate the degree of the initial $m$ nodes, which are the oldest ones.\n",
      "        avg_age_deg[ t-m ] = np.mean( degree[ :m ] )\n",
      "## The initial nodes contribute to the average node degree of the next generaton of vertices\n",
      "## The nodes which the new vertex is attached to are picked\n",
      "##  according to the preferential attachment distribution.\n",
      "        probabilities = degree[ :t ] / ( 2.0 * degree_half_sum )\n",
      "        U = rnd.choice( t, size = m, p = probabilities, replace = False )\n",
      "    return G, result, avg_age_deg, avg_tri_clu, avg_path_ln\n",
      "## https://networkx.github.io/documentation/latest/reference/classes.graph.html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G1, R1, A1, C1, L1 = barabasi_albert( 300, m = 1, avlen = True, avclu = False )\n",
      "x = np.arange( len( L1 ) )\n",
      "plt.plot( L1 )\n",
      "# plt.plot( 10+x, np.log( 10+x ) / np.log( np.log( 10+x ) ), \"r-\" )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### _Model A_\n",
      "In **model A**, the first restricted verison of the evolution, the network grows with time but the newly added nodes are attached to the existing ones uniformly at random.\n",
      "The code below implements the basic steps of both the Bayesian and the first restricted model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "## This evolution step is similar to the original BA step, except it\n",
      "##  relaxes the requirement that m_0 be equal to m, and it implemets\n",
      "##  the egalitarian attachment and bayesian valency.\n",
      "def barabasi_albert_model_a( n, m0 = 10, m = 5, bayes = False, callback = None, avclu = False, avlen = False ) :\n",
      "    result = list( )\n",
      "    callback_fn = callback if callable( callback ) else lambda G, _: None\n",
      "    G = nx.empty_graph( m0 )\n",
      "    degree = np.zeros( n + m0, dtype = np.int ) ; degree_half_sum = 0\n",
      "    avg_age_deg = np.zeros( n + 1, dtype = np.float )\n",
      "## Average path length and clustering\n",
      "    pi = 1 - ml.identity( n + m0, np.float ) ; pi[ pi == 1 ] = np.inf\n",
      "    avg_tri_clu = np.full( n + m0, np.inf, dtype = np.float )\n",
      "    avg_path_ln = np.full( n + m0, np.inf, dtype = np.float )\n",
      "    npmin = np.minimum\n",
      "## Begin the simulation\n",
      "    t = G.number_of_nodes( )\n",
      "    while t < n + m0 :\n",
      "## Select either the bayesian valency distribution, or uniform attachment\n",
      "        theta = None if not bayes else ( degree[ :t ] + 1.0 ) / ( 2.0 * degree_half_sum + t )\n",
      "        U = rnd.choice( t, size = m, p = theta, replace = False )\n",
      "        G.add_edges_from( ( t, u ) for u in U )\n",
      "## Accumulate degree information\n",
      "        degree[ t ] += m ; degree[ U ] += 1\n",
      "        degree_half_sum += m\n",
      "## Get the average clustering coefficient\n",
      "        if avclu :\n",
      "            avg_tri_clu[ t ] = nx.average_clustering( G )\n",
      "## Compute the shortest paths - 1\n",
      "        if avlen :\n",
      "            pi[ t, t ] = 0 ; pi[ U, t ] = pi[ t, U ] = 1\n",
      "            pi[:t,:t] = npmin( pi[:t,:t], pi[ :t, t ] + pi[ t, :t ] )\n",
      "## Preallocate the next vertex\n",
      "        t = G.number_of_nodes( )\n",
      "## Compute the shortest paths - 2\n",
      "        if avlen :\n",
      "            for u in U :\n",
      "                pi[:t,:t] = npmin( pi[:t,:t], pi[ :t, u ] + pi[ u, :t ] )\n",
      "            avg_path_ln[ t-1 ] = np.mean( pi[:t, :t], dtype = np.float64 ) * t / ( t - 1.0 )\n",
      "#             assert( np.all( nx.floyd_warshall_numpy( G ) == pi[:t,:t] ) )\n",
      "        result.append( callback_fn( G, degree[ :t ] ) )\n",
      "## Finalize this step's statistics\n",
      "        avg_age_deg[ :(t-m0) ] += ( degree[ t-1:m0-1:-1 ] - avg_age_deg[ :(t-m0) ] ) / np.arange( t-m0, 0, -1, dtype = np.float )\n",
      "        avg_age_deg[ t-m0 ] = np.mean( degree[ :m0 ] )\n",
      "#     assert( np.all( nx.floyd_warshall_numpy( G ) == pi ) )\n",
      "    return G, result, avg_age_deg, avg_tri_clu, avg_path_ln"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### _Model B_\n",
      "**Model B** retains the preferential attachment phase, but foregoes the growth step. Instead, a random existing node is picked and is linked with other nodes, preferentially chosing vertices with high degree. Initally this model begins its evolution from a set of isolated nodes.\n",
      "\n",
      "For this model to work as intended the only option is to use the Bayesian valency distribution, since the originally proposed probabilities handle poorly the isolated nodes, as it has been alread stated. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###### A couple of nuances\n",
      "Let $w'\\in V$ be a randomly picked vertex. Since multiple connections and loops are forbidden in the model, the preferential attachment of a vertex $w'$ must use conditional node valency distribution. If a subset $S\\subseteq V$, $w'\\in S$, is **prohibited**, then for any vertex $v\\in V\\setminus S$  \n",
      "\n",
      "$$p\\Big(\\Big. w'\\leadsto v\\,\\big.\\big\\rvert\\, w'\\not\\!\\leadsto S,\\,G \\Big.\\Big)\n",
      "= \\frac{p\\Big(\\Big. w'\\leadsto v\\,\\big.\\big\\rvert G \\Big.\\Big)}{p\\Big(\\Big. w'\\not\\!\\leadsto S\\,\\big.\\big\\rvert G \\Big.\\Big)}\n",
      "= \\frac{\\delta_n(v)+1}{\\big\\lvert V\\setminus S\\big\\rvert + \\sum_{u\\in V\\setminus S}\\delta_n(u)} $$  \n",
      "\n",
      "In particular, the prohibited set for a vertex $w'$ is itself and the set of its immediate neighbours:  \n",
      "\n",
      "$$S = \\big\\{w'\\big\\}\\cup N_G(w')$$\n",
      "\n",
      "Another subtle point is the connection strategy when $\\big\\lvert N_G(w') \\big\\rvert\\geq n-1-m$. If $\\big\\lvert N_G(w') \\big\\rvert < n-1$, the model forcefully connects $w'$ to all of the remaining vertices. If however $w'$ is saturated, then the attachment step for this node is skipped as this vertex is just unable to form new connections."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "## Another relaxation of the complete Barabasi-Albert model, which eliminates\n",
      "##  the growth phase. This restriction uses the Bayesian valency distribution.\n",
      "##  when performing preferential attachment.\n",
      "def barabasi_albert_model_b( n, m = 5, callback = None, avclu = False, avlen = False ) :\n",
      "    callback_fn = callback if callable( callback ) else lambda G, _: None\n",
      "    G = nx.empty_graph( n )\n",
      "## Since the Bayesian correction amounts to adding loops, initialize the\n",
      "##  degree with ones.\n",
      "    degree = np.zeros( n, dtype = np.int64 ) ; degree_sum = 0\n",
      "    avg_age_deg = np.zeros( n, dtype = np.float )\n",
      "## Average path length and clustering\n",
      "    pi = 1 - ml.identity( n, np.float ) ; pi[ pi == 1 ] = np.inf\n",
      "    avg_tri_clu = np.full( n, np.inf, dtype = np.float )\n",
      "    avg_path_ln = np.full( n, np.inf, dtype = np.float )\n",
      "    npmin = np.minimum\n",
      "## Begin the simulation\n",
      "    result = list( )\n",
      "    t = 0\n",
      "    while t < n :\n",
      "        t += 1\n",
      "## 1. Pick an existing one at random (cf. p.551 c.3 L 9 of [Barabasi, Albert; 1999])\n",
      "        v = rnd.randint( n, size = 1 )[ 0 ]\n",
      "## Get its neighbours\n",
      "        S = nx.neighbors( G, v )\n",
      "## If it has more neighbours than n-1-m, connect it to the remaining nodes\n",
      "##  and continue. If the node is staturated, ignore it.\n",
      "        m_prime = min( m, n - 1 - len( S ) )\n",
      "        if m_prime > 0 :\n",
      "## Compute the conditional valency of each vertex in the current configuration\n",
      "## Get the correction terms due to conditioning. The \"background\" ones\n",
      "##  which come from the Baesian correction have been embedded into the \n",
      "##  degree array.\n",
      "            c_term = degree[ v ] + 1 + np.sum( degree[ S ] ) + len( S )\n",
      "## Calculate the probability\n",
      "            theta = ( degree + 1.0 ) / ( 0.0 + degree_sum + n - c_term )\n",
      "## Zero out the prohibited vertices, so that they'll never be picked.\n",
      "            theta[ v ] = 0 ; theta[ S ] = 0\n",
      "## 2. Pick m' other nodes to preferentially link the chosen one to.\n",
      "            U = rnd.choice( n, size = m_prime, p = theta, replace = False )\n",
      "            G.add_edges_from( ( v, u ) for u in U )\n",
      "            degree[ v ] += m_prime ; degree[ U ] += 1\n",
      "            degree_sum += m_prime*2 ;\n",
      "## Get the average clustering coefficient\n",
      "        if avclu :\n",
      "            avg_tri_clu[ t-1 ] = nx.average_clustering( G ) if m_prime > 0 else avg_tri_clu[ t-2 ]\n",
      "## Compute the shortest paths\n",
      "        if avlen :\n",
      "            if m_prime > 0 :\n",
      "                pi[ U, v ] = pi[ v, U ] = 1\n",
      "                if True :\n",
      "                    iii = range( n )\n",
      "                    del iii[ v ]\n",
      "                    ii = np.ix_(iii, iii)\n",
      "                    for u in U :\n",
      "                        M = npmin( pi, pi[ :, u ] + pi[ u, : ] )\n",
      "                        assert( np.all( pi[ii] == M[ii] ) )\n",
      "                        print pi[ii]\n",
      "                        print M[ii]\n",
      "                        pi = npmin( pi, pi[ :, u ] + pi[ u, : ] )\n",
      "                else :\n",
      "                    for u in U :\n",
      "                        pi[ :, v ] = npmin( pi[ :, v ], pi[ :, u ] + pi[ u, v ] )\n",
      "                        pi[ v, : ] = pi[ :, v ].T\n",
      "                pi = npmin( pi, pi[ :, v ] + pi[ v, : ] )\n",
      "            avg_path_ln[ t-1 ] = np.mean( pi ) * n / ( n - 1.0 ) if m_prime > 0 else avg_path_ln[ t-2 ]\n",
      "#             try :\n",
      "#                 avg_path_ln[ t-1 ] = nx.average_shortest_path_length( G )\n",
      "#             except :\n",
      "#                 pass\n",
      "#             assert( np.all( nx.floyd_warshall_numpy( G ) == pi ) )\n",
      "        result.append( callback_fn( G, degree ) )\n",
      "## In this model all nodes are the same age, which means that the degree-age\n",
      "##  relationship is just the average degree with respect to the generation.\n",
      "        avg_age_deg[ t-1 ] = np.mean( degree ) if m_prime > 0 else avg_age_deg[ t-2 ]\n",
      "#     assert( np.all( nx.floyd_warshall_numpy( G ) == pi ) )\n",
      "    return G, result, avg_age_deg, avg_tri_clu, avg_path_ln"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G1, R1, A1, C1, L1 = barabasi_albert_model_b( 10, m = 3, avlen = True, avclu = False )\n",
      "plt.plot( L1 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Vertex copying model\n",
      "This model is another version of a growing network that shares features similar to preferential attachemnt.\n",
      " * the evolution begins with a small collection of randomly connected vertices $G_0$;\n",
      " * transition form $G_n = (V_n, E_n)$ to $G_{n+1} = (V_{n+1},E_{n+1})$ :\n",
      "   1. a \"role model\" vertex $w$ is chosen uniformly at random from the existing vertices $V_n$;\n",
      "   2. a new node $w'$ is created;\n",
      "   3. every edge $(u,w)\\in E_n$ is either kept with probability $q$, in which case an edge $(u,w')$ is created, or is abandoned with probability $1-q$; let $S$ be the set of endpoints of kept edges;\n",
      "   4. for each endpoint $u$ of an abandoned edge a random node $u'\\in V_n\\setminus \\big(S \\cup \\{u\\}\\big)$ is picked and an edge $(u',w)$ is added to the graph;\n",
      "  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vertex_copying_model( G, n, q = 0.5, callback = None, avclu = False, avlen = False ) :\n",
      "    callback_fn = callback if callable( callback ) else lambda G, _: None\n",
      "    result = list( )\n",
      "## Initialize the local degree accumulator\n",
      "    m0 = G.number_of_nodes( ) ; t = m0\n",
      "    degree = np.zeros( n + m0, dtype = np.int )\n",
      "## The output and \n",
      "    deg = G.degree( )\n",
      "    degree[ :m0 ] = [ deg.get( k, 0 ) for k in xrange( m0 ) ]\n",
      "    avg_age_deg = np.zeros( n + 1, dtype = np.float )\n",
      "## Average path length and clustering\n",
      "    pi = 1 - ml.identity( n + m0, np.float ) ; pi[ pi == 1 ] = np.inf\n",
      "    pi[ :m0, :m0 ] = nx.floyd_warshall_numpy( G )\n",
      "    avg_tri_clu = np.zeros( n + m0, dtype = np.float )\n",
      "    avg_path_ln = np.zeros( n + m0, dtype = np.float )\n",
      "    try :\n",
      "        avg_path_ln[ :m0 ] = nx.average_shortest_path_length( G )\n",
      "    except :\n",
      "        avg_path_ln[ :m0 ] = np.inf\n",
      "    npmin = np.minimum\n",
      "## Begin the simulation\n",
      "    while t < n + m0 :\n",
      "## Duplicate a random vertex\n",
      "        neighbors = G.neighbors( rnd.randint( t, size = 1 )[ 0 ] )\n",
      "## For each neighbour, determine whether it is kept or abandoned\n",
      "        survives = rnd.random_sample( size = len( neighbors ) ) < q\n",
      "## Compile a list of surviving nodes\n",
      "        survivors = set( u for m, u in zip( survives, neighbors ) if m )\n",
      "## Construct the pool of possible target nodes\n",
      "        targets = set( survivors )\n",
      "        for s, w in zip( survives, neighbors ) :\n",
      "            if not s :\n",
      "## This neighbour should be abandoned. Pick another one at random, \n",
      "                w0 = w\n",
      "##  ... bearing in mind that the new vertex must be other than any\n",
      "##  of the survivors or the orignal neighbouring node.\n",
      "                while w in survivors or w == w0 :\n",
      "                    w = rnd.randint( t, size = 1 )[ 0 ]\n",
      "                targets.add( w )\n",
      "## Add edges and keep track of the vertex degrees\n",
      "        G.add_edges_from( ( t, u ) for u in targets )\n",
      "        degree[ t ] += len( targets ) ; degree[ list( targets ) ] += 1\n",
      "## Get the average clustering coefficient\n",
      "        if avclu :\n",
      "            avg_tri_clu[ t-1 ] = nx.average_clustering( G )\n",
      "## First part of Floyd-Warshall\n",
      "        if avlen :\n",
      "            U = list( targets )\n",
      "## Add the shortest path from t to itself\n",
      "            pi[ U, t ] = pi[ t, U ] = 1\n",
      "## Accout for the shortest path through t\n",
      "            pi[:t,:t] = npmin( pi[:t,:t], pi[ :t, t ] + pi[ t, :t ] )\n",
      "## Preallocate the next vertex\n",
      "        t = G.number_of_nodes( )\n",
      "## Run the second part of Floyd-Warshall\n",
      "        if avlen :\n",
      "            for u in U :\n",
      "                pi[:t,:t] = npmin( pi[:t,:t], pi[ :t, u ] + pi[ u, :t ] )\n",
      "            avg_path_ln[ t-1 ] = np.mean( pi[:t, :t], dtype = np.float64 ) * t / ( t - 1.0 )\n",
      "#             assert( np.all( nx.floyd_warshall_numpy( G ) == pi[:t,:t] ) )\n",
      "        result.append( callback_fn( G, degree[ :t ] ) )\n",
      "## Finalize this step's statistics\n",
      "        avg_age_deg[ :(t-m0) ] += ( degree[ t-1:m0-1:-1 ] - avg_age_deg[ :(t-m0) ] ) / np.arange( t-m0, 0, -1, dtype = np.float )\n",
      "        avg_age_deg[ t-m0 ] = np.mean( degree[ :m0 ] )\n",
      "    return G, result, avg_age_deg, avg_tri_clu, avg_path_ln"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Simulation study"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check the degree distribution and its depndence on the age of the node for the following models:\n",
      " * The original Barabasi-Albert model (\"**B-A**\");\n",
      " * The Brarabasi-Albert model with the Bayesian valency distribution (\"**BB-A**\");\n",
      " * Type-A restriction: Barabasi-Albert model with uniform attachment (\"**mdlA**\");\n",
      " * Type-b restriction: Barabasi-Albert model with stunted growth and Bayesian valency distribution (\"**mdlB**\");\n",
      " * The vertex-copying model (\"**VCM**\")."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Analysis toolkit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A useful tool for exploring the tail behaviour of sample is the **M**ean **E**xcess plot, defined as the  \n",
      "\n",
      "$$M(u) = \\mathbb{E}\\Big(\\Big. X-u\\,\\big.\\big\\rvert\\,X\\geq u \\Big.\\Big)$$\n",
      "of which the emprical counterpart is  \n",
      "$$\\hat{M}(u) = {\\Big(\\sum_{i=1}^n 1_{x_i\\geq u}\\Big)^{-1}}\\sum_{i=1}^n (x_i-u) 1_{x_i\\geq u}$$\n",
      "The key properties of $M(u)$ are\n",
      " * it steadily increases for a power-law tails;\n",
      " * it level for exponential tails;\n",
      " * it decays for a finite tail.\n",
      "When dealing with the empirical mean-excesses one looks for the trend in the large thresholds to discern behaviour, necessarily bearing in mind that in that region the varinace of the $\\hat{M}(u)$ grows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from scipy.stats import rankdata\n",
      "def mean_excess( data ) :\n",
      "    data = np.array( sorted( data, reverse = True ) )\n",
      "    ranks = rankdata( data, method = 'max' )\n",
      "    excesses = np.array( np.unique( len( data ) - ranks ), dtype = np.int )\n",
      "    thresholds = data[ excesses ]\n",
      "    mean_excess = np.cumsum( data )[ excesses ] / ( excesses + 0.0 ) - thresholds\n",
      "    return thresholds, mean_excess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the degree distribution, at last in the tails, is expected to be power law, define a procedure for estimating the exponent in :\n",
      "$$1-F(x) = \\Big( \\frac{x}{x_0} \\Big)^{-\\alpha}$$\n",
      "where $x_0$ is the tail threshold."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## ML estimator of the power law in the \"tail\" (x\u2265u):\n",
      "##  x_k \\sim C x^{-\\alpha} 1_{[u,+\u221e)}(x).\n",
      "def mle_alpha( data, threshold ) :\n",
      "    tail = np.array( [ v for v in data if v >= threshold ] )\n",
      "    sum_log = np.sum( np.log( tail ) ) / ( len( tail ) + 0.0 )\n",
      "    alpha = 1.0 + 1.0 / ( sum_log - np.log( threshold ) )\n",
      "    return alpha"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It might be insightful to inspect the dependence of the the node degree on its age. If $A_{it}$ is the average degree of a node of age $i$ by the end of generation $t\\geq i$, then  \n",
      "\n",
      "$$ A_{it} = \\frac{ \\sum_{j=i}^t d_{t-i,j} }{t-i+1} $$  \n",
      "\n",
      "where $d_{kt}$ is the degree of a node born at $k$ by the end of generation $t\\geq k$ and $t-i+1$ is the number of nodes of age $i$. For convenience set $A_{it} = 0$ for all $i>t$. In the orgiginal Barabasi-Albert modela and in its type-A restriction $A_{0t} = m$, while in the model B it is no more than $m$ and in the vertex copying model it is no more that the degree of some existing vertex."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to compute the average of a sequence of vaeus coming in real-time, while still consuming $O(1)$ memory it is necessary to use an iterative formula for the mean. For sequence $\\big(x_t\\big)_{t\\geq 1}$ the mean could be computed iteratively: \n",
      "\n",
      "$$\\bar{x}_t = \\bar{x}_{t-1} + \\frac{1}{t}\\big(x_t - \\bar{x}_t\\big)$$\n",
      "\n",
      "which immediately follows from associativity of finite sums: $\\sum_{k=1}^t x_k = \\sum_{k=1}^{t-1} x_k + x_t$.\n",
      "\n",
      "In the case at hand, the iterative formula translates into\n",
      "$$ A_{it} = A_{i,t-1} + \\frac{ d_{t-i,t} - A_{i,t-1} }{t-i+1} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let me elaborate a little bit on how the the simulation procedures keep track of the node degrees and the average degree with respect to the age.\n",
      "\n",
      "The nodes are created one-by-one and each is idetified by the number of nodes before its birth. This way it is easy to choose the nodes which were born at a given iteration and therefore the nodes of a certain age. For instance, if there are $t$ nodes in the graph $G$, then the next node is created with index $t$ making all the vertices into a sequence $0,1,\\ldots, t-1,t$ of $t+1$ elements. At the same time the node of age $t\\geq\\tau\\geq0$ can be pinpointed by the index $t-\\tau$.\n",
      "\n",
      "The oldest nodes, wthose which existed before the first generation ever are assigned age $t+1$ (pre-simulation era).\n",
      "\n",
      "The degree array has the following structure at any particular generation $t$:\n",
      " * first $m_0$ integers (indices $0, \\ldots, m_0-1$) hold the degrees of the ancestral vertices;\n",
      " * the next $t-m_0$ values ( $m_0, \\ldots, t-m_0-1$ ) contain the degrees of vertiecs, created during the simulation;\n",
      " * the youngest vertex is located at index $t$ in the degree array;\n",
      " \n",
      "The avergage degree array has similar structure :\n",
      " * the first $t-m_0$ (indices $0$ throguh $t-m_0-1$ values contain the values for the created nodes;\n",
      " * the last, $t-m_0+1$-th value (exactly at $t-m_0$) is the combined average degree of the initial $m_0$ nodes.\n",
      " \n",
      "The degree accounting and the average degree per age tracking are a bit contrived, but it is a necesary payment for speed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Computing shortest paths\n",
      "\n",
      "Since the graph is undirected and path lenghts are measured in the number of hops between a pair of vertices, the first thing that comes to mind is the breadth-first-search algorithm with complexity $O\\big(\\lvert V\\rvert + \\lvert E\\rvert \\big)$. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import deque\n",
      "def bfs_shortest_path( G, s = 0 ) :\n",
      "## Initalize the array of shortest paths\n",
      "    pi = np.full( G.number_of_nodes( ), np.inf, np.float )\n",
      "## Cache deque operations for faster access (Python won't have to make lookups)\n",
      "    deq = deque.popleft ; enq = deque.append\n",
      "## Initialize with the source vertex\n",
      "    Q = deque( [ s ] ) ; pi[ s ] = 0\n",
      "## While the (de)queue is not empty\n",
      "    while Q :\n",
      "## get the virst vertes to have been added\n",
      "        v = deq( Q )\n",
      "## For each neighbour...\n",
      "        for n in G[ v ] :\n",
      "## Check if it has not been visited before\n",
      "            if pi[ n ] != np.inf : continue\n",
      "## Add it to the queue and update its distance from the source\n",
      "            enq( Q, n ) ; pi[ n ] = pi[ v ] + 1\n",
      "    return pi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However in order to compute the length of a shortest path between each pair $u,v\\in V$, it is necessary to run the BFS starting at each vertes $s\\in V$. Thus the complxity of the algorithm becomes $O\\big(\\lvert V\\rvert^2 + \\lvert V\\rvert \\lvert E\\rvert \\big)$. The overall complexity of hte simulation is $O\\Big(\\lvert V\\rvert^3 + \\lvert V\\rvert^2 \\lvert E\\rvert \\Big)$, without average culstering computations, which are approximately $O(\\lvert V\\rvert^3)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def avg_shortest_path( G ) :\n",
      "    asl = 0.0\n",
      "    for n in G :\n",
      "        asl += sum( bfs_shortest_path( G, n ) )\n",
      "    return asl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The main drawback of this apporach is that in a growing network with dynamically added edges, the shortest paths of unaffected  nodes haveto be recomputed. That is why a modified Floyd-Warshall algorithm has been used for this purpose.\n",
      "\n",
      "In its original form FW has complexity $O\\Big(\\lvert V\\rvert^3 \\Big)$, which would have made the overall complexity of the simulation to be $O\\Big(\\lvert V\\rvert^4\\Big)$. When the growing nature of the Barabasi-Albert model is taken into account, the overall complexity of the simulation, without hte clustering, becomes $O\\big(\\lvert E\\rvert \\lvert V\\rvert^2\\big)$, which though looks larger, still runs faster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The very nature of Floyd-Warshall algorithm, which is dynamical mathematical programming, permits easy modification for growing networks. The main idea is that if $\\rho_n$ is the matrix of shortest path lengths for a graph $G_n$ with $n$ nodes, then adding  an $n+1$-th vertex \n",
      "\n",
      "Suppose the shortest path between vertices $u,v\\in V$ is through the vertices $\\pi_{uv}$. Then it is easy to verufy that for each pair of node $s,t\\in \\pi_{uv}$ the shortest path $\\pi_{st}\\subseteq \\pi_{uv}$.\n",
      "\n",
      "If a vertex $\\omega$ is added ($V' = V\\cup\\{\\omega\\}$) with edges $\\big\\{\\big. (\\omega, u)\\big.\\big\\rvert u\\in U\\big.\\big\\}$ for some $U\\subseteq V$, then in order to construct the shortest path matrix it is necessary :\n",
      " 1. to compute the shortest path $\\pi_{u\\omega}$ for every $u\\in V'$;\n",
      " 2. to update the shortset paths which have nodes from $U$ in them.\n",
      "\n",
      "Consider any $x,y\\in V'$ and a sequence of nested sets of vertices $\\big(S_k\\big)_{k=0}^p$ such that $S_k\\subset S_{k+1}\\subseteq U\\cup\\{\\omega\\}$ for each $k=0,\\ldots,p-1$, where $p=\\lvert U \\rvert$, $S_0 = V\\setminus U$ and $S_p = S_0 \\cup \\big(U\\cup\\{\\omega\\}\\big)$.\n",
      "\n",
      "Initially $\\big(D_{xy}\\big)_{x,y\\in V}$ is the matrix of shortest path lengths prior to growth and it contains the lengths of paths using edges except $\\big\\{\\big. (\\omega, u)\\big.\\big\\rvert u\\in U\\big.\\big\\}$.\n",
      "\n",
      "In fact the matrix $D$ can de identified with a complete graph with the weight of an edge $(s,t)$ given by $D_{st}$. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last step requires some elaboration: consider a shortest path in $V$ between edges $x,y\\in V$. If it has no affected vertices it in $\\pi_{xy}\\cap U = \\emptyset$, then the shortest path cannot have gotten shorter, for it would not have been the shortest in the first place.\n",
      "\n",
      "If $\\lvert \\pi_{xy}\\cap U\\rvert = 1$, ie, when only one of the affected vertices is on the shrotest path between $x$ and $y$ then again, it remains the shortest. Finally, when $u,v\\in U$ lie on $\\pi_{xy}$, then the shortest path must be updated.\n",
      "\n",
      "\n",
      "Now contrary to Floyd-Warshall's oroginal correcntess proof, the vertices in $U$ should be treated as \n",
      "\n",
      "Just after the creation of $t$, setting $D_{xt} = \\min_{i\\in V} D_{xi} + D_{it}$ yields shortest paths $\\pi_{xt}$ -- from $x$ to $t$. \n",
      "Now for any $x,y\\in V'$ the matrix $D_{xy}' = D_{xy}$ for $x,y\\neq t$ again contains the shortest path\n",
      "\n",
      "\n",
      "\n",
      "Now suppose n passes have occurred, and that adj[i][j] contains the shortest distance from i to j using only the first n vertices of the graph (By first, we mean the first to be iterated over by the outer main loop.)\n",
      "\n",
      "If n = V, we are done. Otherwise, the shortest path from i to j that uses only the first n + 1 vertices of the graph either does not use the (n + 1)th vertex, denoted k, at all, in which case its value is already known, or there exists a shorter path that does use k. To find this path, we relax the edge from i to j using k as an intermediate vertex along the path, meaning that we concatenate the i-k and k-j paths to obtain the new i-j path, and update the i-j distance by summing the i-k and k-j distances. This will always yield the shortest distance from i to j that uses k. This is because, for each of the paths i-k and k-j, if the corresponding entry in adj has not yet been updated, then it contains the shortest path for that pair that uses only vertices lower than k, which is acceptable because vertices higher than k are not being considered yet, and duplicate instances of vertex k cannot shorten the path unless negative-weight cycles are present. If it has been updated, duplicate instances of vertex k cannot lengthen the path either, since we only keep the shortest path so far found at any given time. Thus, after the (n + 1)th pass, all pairs of shortest paths are known that do not use any vertex higher than k as an intermediate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyse( G, degree = None ) :\n",
      "    deg = G.degree( ).values( ) if degree is None else degree\n",
      "    alpha = mle_alpha( deg, np.median( deg ) )\n",
      "    return ( alpha, )\n",
      "#     if not nx.is_connected( G ) :\n",
      "#         return np.infty\n",
      "#     return nx.average_shortest_path_length( G )\n",
      "\n",
      "# def avg_path( G ) :\n",
      "#     sp = nx.all_pairs_shortest_path_length( G )\n",
      "#     np.fromiter()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G1, R1, A1 = barabasi_albert( 1000, m = 100, callback = analyse )\n",
      "G2, R2, A2 = barabasi_albert_model_a( 1000, m0 = 100, m = 100, bayes = True, callback = analyse )\n",
      "G3, R3, A3 = barabasi_albert_model_a( 1000, m0 = 100, m = 100, bayes = False, callback = analyse )\n",
      "G4, R4, A4 = barabasi_albert_model_b( 1000, m = 100, callback = analyse )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(1, figsize = (16,6))\n",
      "## Draw the degree-age relationship\n",
      "plt.subplot(121)\n",
      "plt.plot( A1, \"k-\", label = 'B-A', linewidth = 2 )\n",
      "plt.plot( A2, \"r-\", label = 'BB-A' )\n",
      "plt.plot( A3, \"b-\", label = 'mdlA' )\n",
      "plt.plot( A4, \"y-\", label = 'mdlB' )\n",
      "plt.legend( loc = 'upper left' )\n",
      "plt.title( \"Average vertex degree\" )\n",
      "plt.xlabel( \"Age\" )\n",
      "plt.ylabel( \"Avg. degree\" )\n",
      "\n",
      "## Draw the degree distributions\n",
      "plt.subplot(122)\n",
      "v1, f1 = np.unique( G1.degree().values(), return_counts = True )\n",
      "v2, f2 = np.unique( G2.degree().values(), return_counts = True )\n",
      "v3, f3 = np.unique( G3.degree().values(), return_counts = True )\n",
      "v4, f4 = np.unique( G4.degree().values(), return_counts = True )\n",
      "plt.loglog( v1, f1, \".k\", label = 'B-A', ms = .95 )\n",
      "plt.loglog( v2, f2, \".r\", label = 'BB-A', ms = .95 )\n",
      "plt.loglog( v3, f3, \".b\", label = 'mdlA', ms = .95 )\n",
      "plt.loglog( v4, f4, \".y\", label = 'mdlB', ms = .95 )\n",
      "plt.legend( loc = 'upper right' )\n",
      "plt.title( \"Node degree frequency\" )\n",
      "plt.xlabel( \"Degree\" )\n",
      "plt.ylabel( \"Frequency\" )\n",
      "\n",
      "## Commit to the device\n",
      "plt.show( )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(1, figsize = (16,6))\n",
      "## Mean excess plot\n",
      "plt.subplot(121)\n",
      "t1, e1 = mean_excess( G1.degree().values() )\n",
      "t2, e2 = mean_excess( G2.degree().values() )\n",
      "t3, e3 = mean_excess( G3.degree().values() )\n",
      "t4, e4 = mean_excess( G4.degree().values() )\n",
      "plt.plot( t1, e1, \"k-\", label = 'B-A' )\n",
      "plt.plot( t2, e2, \"r-\", label = 'BB-A' )\n",
      "plt.plot( t3, e3, \"b-\", label = 'mdlA' )\n",
      "plt.plot( t4, e4, \"y-\", label = 'mdlB' )\n",
      "plt.legend( loc = 'upper left' )\n",
      "plt.title( \"Mean excess plot\" )\n",
      "plt.ylabel( \"Mean Excess\" )\n",
      "plt.xlabel( \"Threshold\" )\n",
      "\n",
      "## The MLE of alpha plot\n",
      "plt.subplot(122)\n",
      "a1 = [ a[ 0 ] for a in R1 ]\n",
      "a2 = [ a[ 0 ] for a in R2 ]\n",
      "a3 = [ a[ 0 ] for a in R3 ]\n",
      "a4 = [ a[ 0 ] for a in R4 ]\n",
      "plt.plot( a1, \"k-\", label = 'B-A' )\n",
      "plt.plot( a2, \"r-\", label = 'BB-A' )\n",
      "plt.plot( a3, \"b-\", label = 'mdlA' )\n",
      "plt.plot( a4, \"y-\", label = 'mdlB' )\n",
      "plt.legend( loc = 'upper right' )\n",
      "plt.title( \"the Power Law exponent\" )\n",
      "plt.ylabel( \"Alpha\" )\n",
      "plt.xlabel( \"Generation\" )\n",
      "\n",
      "## Commit to the device\n",
      "plt.show( )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%lprun -f bfs_shortest_path avg_shortest_path( G4 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nx.average_shortest_path_length( G4 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### VCM\n",
      "Now let's examine the properties of the **V**ertex **C**opying **M**odel."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nGENR = 1000\n",
      "probs = [ 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99 ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run the simulation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G0 = nx.fast_gnp_random_graph( int( 0.01 * nGENR ), .1 )\n",
      "VCM_10_10  = [ vertex_copying_model( G0.copy( ), nGENR, prob ) for prob in probs ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G0 = nx.fast_gnp_random_graph( int( 0.01 * nGENR ), .5 )\n",
      "VCM_10_50  = [ vertex_copying_model( G0.copy( ), nGENR, prob ) for prob in probs ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G0 = nx.fast_gnp_random_graph( int( 0.01 * nGENR ), .9 )\n",
      "VCM_10_90  = [ vertex_copying_model( G0.copy( ), nGENR, prob ) for prob in probs ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G0 = nx.fast_gnp_random_graph( int( 0.10 * nGENR ), .1 )\n",
      "VCM_100_10 = [ vertex_copying_model( G0.copy( ), nGENR, prob ) for prob in probs ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G0 = nx.fast_gnp_random_graph( int( 0.10 * nGENR ), .5 )\n",
      "VCM_100_50 = [ vertex_copying_model( G0.copy( ), nGENR, prob ) for prob in probs ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G0 = nx.fast_gnp_random_graph( int( 0.10 * nGENR ), .9 )\n",
      "VCM_100_90 = [ vertex_copying_model( G0.copy( ), nGENR, prob ) for prob in probs ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The plots are below"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing the Bayesian valency against the orignal model for $m_0 = m = 1$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Define the experiment parameters\n",
      "M0 = 1 ; M = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "## Analyse the model with the specified parameters\n",
      "G0, _ = barabasi_albert( 10000, m = M )\n",
      "G1, _ = barabasi_albert_model_a( 10000, m0 = M0, m = M, bayes = True )\n",
      "\n",
      "plt.figure( 1, figsize = ( 12, 6 ) )\n",
      "deg0 = G0.degree().values()\n",
      "deg1 = G1.degree().values()\n",
      "\n",
      "## Node degree frequency plot\n",
      "plt.subplot(121)\n",
      "v0, f0 = np.unique( deg0, return_counts = True )\n",
      "v1, f1 = np.unique( deg1, return_counts = True )\n",
      "plt.title( \"Node degree frequency\" )\n",
      "plt.loglog( v0, f0, \"ro\" )\n",
      "plt.loglog( v1, f1, \"b<\" )\n",
      "plt.xlabel( \"degree\" ) ; plt.ylabel( \"frequency\" )\n",
      "\n",
      "## Mean excess plot\n",
      "plt.subplot(122)\n",
      "t0, e0 = mean_excess( deg0 )\n",
      "t1, e1 = mean_excess( deg1 )\n",
      "plt.title( \"Mean excess plot\" )\n",
      "plt.loglog( t0, e0, \"ro-\", linewidth = 2 )\n",
      "plt.loglog( t1, e1, \"bo-\", linewidth = 2 )\n",
      "plt.ylabel( \"mean excess\" ) ; plt.xlabel( \"threshold\" )\n",
      "\n",
      "## Commit to device\n",
      "plt.show( )\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Create a convenient macro definition\n",
      "%macro -q fa _i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %lprun -f barabasi_albert barabasi_albert( 1000, m = 10, avclu = True, avlen = True )\n",
      "G0 = nx.fast_gnp_random_graph( 40, .1 )\n",
      "G1, R1, A1, C1, L1 = vertex_copying_model( G0.copy(), 200, q = 0.5, avlen = True, avclu = False )\n",
      "plt.plot( L1 )\n",
      "# %lprun -f barabasi_albert barabasi_albert( 1000, m = 10, avclu = True, avlen = True )\n",
      "G1, R1, A1, C1, L1 = barabasi_albert( 100, m = 10, avlen = True, avclu = False )\n",
      "plt.plot( L1 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M0 = 23 ; M = 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## An isolated graph\n",
      "def ssi_isolated( m0 = 10, **kwargs ) :\n",
      "    return nx.empty_graph( m0 )\n",
      "\n",
      "## To generate a random connected graph of the given order\n",
      "##  simulate the model A starting with a trivial graph.\n",
      "def ssi_connected( m0 = 10, **kwargs ) :\n",
      "    G = nx.trivial_graph( )\n",
      "    for i in xrange( m0 - 1 ) :\n",
      "## For each new node pick a vertex to link it with\n",
      "        v = G.number_of_nodes( )\n",
      "        u = rnd.choice( nx.nodes( G ), size = 1, replace = False )[ 0 ]\n",
      "        G.add_edge( v, u )\n",
      "    return G"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr \\><!-- This is a Blank Line -->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below is for service purposes only and does not concern the study itself. It attempts to make a video out of the matplotlib animation sequence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## This does not seem like a seamless way to generate animated plots.\n",
      "## Ensure that ffmpeg library is installed, since without it evolution\n",
      "##  animations won't work.\n",
      "\n",
      "# http://nbviewer.ipython.org/url/jakevdp.github.io/downloads/notebooks/AnimationEmbedding.ipynb\n",
      "# http://jakevdp.github.io/blog/2012/09/05/quantum-python/\n",
      "from tempfile import NamedTemporaryFile\n",
      "from matplotlib import animation\n",
      "\n",
      "## Convert the matplotlib animation into a video clip, stored locally in\n",
      "##  the object in base64 format.\n",
      "CONTENT_TYPE = \"\"\"data:video/x-m4v;base64,{0}\"\"\"\n",
      "def anim_to_html( anim ) :\n",
      "    if not hasattr( anim, '_encoded_video' ) :\n",
      "        with NamedTemporaryFile( suffix = '.mp4' ) as f :\n",
      "            anim.save( f.name, fps = 15, extra_args = [ '-vcodec', 'libx264', '-pix_fmt', 'yuv420p' ] )\n",
      "            video = open( f.name, \"rb\" ).read( )\n",
      "            anim._encoded_video = video.encode( \"base64\" )\n",
      "    return CONTENT_TYPE.format( anim._encoded_video )\n",
      "\n",
      "## Use direct HTML output capabilities of iPython\n",
      "from IPython.display import HTML\n",
      "## This HTML node is going to be used for embedding video\n",
      "VIDEO_TAG = \"\"\"<video controls><source src=\"{0}\" type=\"video/mp4\">\n",
      "Your browser does not support the video tag.</video>\"\"\"\n",
      "def display_animation( anim ) :\n",
      "## Close and output\n",
      "    plt.close( anim._fig )\n",
      "    return HTML( VIDEO_TAG.format( anim_to_html( anim ) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}