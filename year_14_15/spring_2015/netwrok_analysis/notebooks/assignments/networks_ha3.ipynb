{
 "metadata": {
  "name": "",
  "signature": "sha256:af7be534b0597ce62b10489a25e61e6ab55b77e4b9fb6b2dc4d2bfa43285b324"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<center>Structural Analysis and Visualization of Networks</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<center>Home Assignment #3: Centralities and Assortativity</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<center>Student: *Nazarov Ivan*</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<hr />\n",
      "General Information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Due Date:** 09.03.2015 23:59 <br \\>\n",
      "**Late submission policy:** -0.2 points per day <br \\>\n",
      "\n",
      "\n",
      "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *Nazarov* *Ivan* HA*3* **\n",
      "\n",
      "Support your computations with figures and comments. <br \\>\n",
      "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
      "<br \\>\n",
      "<hr \\>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preamble\n",
      "Import the usual toolbox"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Comupte the page rank on the row sparse matrix M with the teleportation probability \\beta\n",
      "import numpy as np\n",
      "import numpy.random as rnd\n",
      "import scipy.sparse as spma\n",
      "import scipy.sparse.linalg as spla\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Problems"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Task 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to compute nontrivial cenrtality measures it is useful to represent a sparse connectivity matrix in an adjacency list format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "def sparse_adjacency( A, symmetrize = False ) :\n",
      "## Construct the adjacency list: get all nonzero indices\n",
      "    if symmetrize :\n",
      "        S = A.transpose(copy = True)\n",
      "        A = S + A\n",
      "        del S\n",
      "    r, c = A.nonzero( )\n",
      "    i, j = np.unique( r, return_index = True )\n",
      "## Initialize the dictionary and the index\n",
      "    J = dict( ) ; i0 = 0\n",
      "## Exploit the fact that the output of nonzero() is sorted lexicographically \n",
      "##  and that the np.unique() returns the index of the furst occurrence of a value\n",
      "    for x, y in zip( i[:-1], j[1:] ) :\n",
      "## the indices returned by np.unique() are shifted forward to make it\n",
      "##  easier to capture complete ranges of colunm indices of A\n",
      "        J[ x ] = c[ i0 : y ]\n",
      "        i0 = y\n",
      "## Finalize by capturing the last row\n",
      "    J[i[-1]] = c[i0:]\n",
      "## The default value is an empty adjacancy list\n",
      "    return defaultdict( lambda: np.empty( 0, np.int ), J )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Centrality measures\n",
      "#### Degree\n",
      "The degree centrality measure of a node $v\\in V$ in graph $G=\\big(V, E\\big)$ is the sum of all edges incident on it:\n",
      "\n",
      "$$C_v = \\sum_{u\\in V} 1_{(v,u)\\in E} = \\sum_{u\\in V} A_{vu} = \\delta_v$$\n",
      "\n",
      "In other words the more 1st-tier (nearest, reachable in one hop) negihbours a vertex has the higher its centrality is. This is a local metric. In order to perform intergraph comparisons, normalised degreee centrality is used:\n",
      "\n",
      "$$\\bar{C}_v = \\frac{C_v}{|V|-1}$$\n",
      "\n",
      "As usual high centrality -- a node in direct contact with many other nodes, low centrality -- a periphrial vertex."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def degree_centrality( A, norm = False ) :\n",
      "    delta = np.array( A.sum( axis = 1 ) ).flatten()\n",
      "    return delta if not norm else delta / ( A.shape[ 1 ] - 1 )\n",
      "# c_degree( A, True )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Betweenness\n",
      "This measure assesses how important a node is in terms of the global graph connectivity:\n",
      "\n",
      "$$C_B(v) = \\sum_{s\\neq v\\neq t\\in V} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$$\n",
      "\n",
      "where $\\sigma_{st}(v)$ is the number of **shortest** paths from $s$ to $t$ passing through $v$, while $\\sigma_{st}$ is the total number of paths of least legnth connecting $s$ and $t$. \n",
      "\n",
      "In hte case of unweighted graphs,  this measure can be computed in $O(|V||E|)$ time with a modified breadth-first-search algorithm, which while calculating all-pairs shortest paths computes the centrality using some clever arithmetic tricks (See Brandes (2001))."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### References\n",
      "Brandes, Ulrik (2001). \"A faster algorithm for betweenness centrality\" (PDF). Journal of Mathematical Sociology 25: 163\u2013177. doi:10.1080/0022250x.2001.9990249"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A generic breadth-first-search is given below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bfs_shortest_path( A, s = 0, pi = None, J = None ) :\n",
      "## Cache deque operations for faster access (Python won't have to make lookups)\n",
      "\tdeq = deque.popleft ; enq = deque.extend\n",
      "## Get the sparse adjacency list\n",
      "\tif J is None :\n",
      "\t\tJ = sparse_adjacency( A, symmetrize = True )\n",
      "## Initalize the array of shortest paths\n",
      "\tif pi is None :\n",
      "\t\tpi = np.full( A.shape[ 0 ], np.inf, np.float )\n",
      "## Initialize with the source vertex\n",
      "\tQ = deque( [ s ] ) ; pi[ s ] = 0\n",
      "## While the (de)queue is not empty\n",
      "\twhile Q :\n",
      "## get the virst vertes to have been added\n",
      "\t\tv = deq( Q )\n",
      "## For each neighbour...\n",
      "\t\tn = J[ v ]\n",
      "## Check if it has not been visited before\n",
      "\t\tn = n[ pi[ n ] == np.inf ]\n",
      "## Add it to the queue and update its distance from the source\n",
      "\t\tif len( n ) :\n",
      "\t\t\tenq( Q, n )\n",
      "\t\t\tpi[ n ] = 1 + pi[ v ]\n",
      "\treturn pi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While the Brandes's algorithm is just the extended version, which incementally counts the betweenness centrality. As stated in the paper, in counting other centrality measure could be easily incorporated in it. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import deque\n",
      "def betweenness_centrality( A, J = None, verbose = False ) :\n",
      "## Cache deque operations for faster access (Python won't have to make lookups)\n",
      "\tdeq = deque.popleft ; enq = deque.extend\n",
      "\tpop = deque.pop ; push = deque.append\n",
      "\tappend = list.append\n",
      "## Get the sparse adjacency list\n",
      "\tif J is None :\n",
      "\t\tJ = sparse_adjacency( A, symmetrize = True )\n",
      "## Initialize the betweenness array\n",
      "\tcb = np.zeros( A.shape[ 0 ], np.float )\n",
      "\tfor s in xrange( A.shape[ 0 ] ) :\n",
      "\t\tif (s%20)==0 : print s\n",
      "## Initalize the array of shortest paths\n",
      "\t\tpi = np.full( A.shape[ 0 ], np.inf, np.float ) ; pi[ s ] = 0\n",
      "## The array which holds the number of shortest paths through a parciular vertex\n",
      "\t\tsigma = np.zeros( A.shape[ 0 ], np.float ) ; sigma[ s ] = 1\n",
      "## Initialize queue with the source vertex, stack and predecessors -- empty.\n",
      "\t\tQ = deque( [ s ] ) ; S = deque( ) ; pred = defaultdict( list )\n",
      "## While the (de)queue is not empty\n",
      "\t\twhile Q :\n",
      "## get the virst vertes to have been added\n",
      "\t\t\tv = deq( Q )\n",
      "\t\t\tpush( S, v )\n",
      "## For each neighbour...\n",
      "\t\t\tn = J[ v ]\n",
      "## Check if it has not been visited before\n",
      "\t\t\tw = n[ pi[ n ] == np.inf ]\n",
      "## Find neighbours with the shortest path having the current vertex v \n",
      "\t\t\tu = n[ pi[ n ] == 1 + pi[ v ] ]\n",
      "## Add it to the queue and update its distance from the source\n",
      "\t\t\tif len( w ) :\n",
      "## This is the first time the vertex has been seen\n",
      "\t\t\t\tenq( Q, w )\n",
      "## Initalize its shortest path through s~v-w edge\n",
      "\t\t\t\tpi[ w ] = 1 + pi[ v ]\n",
      "## The number of shortest paths in this case is equal to the number of\n",
      "##  shortest paths s~v\n",
      "\t\t\t\tsigma[ w ] = sigma[ v ]\n",
      "## Initialize the predecessors\n",
      "\t\t\t\tfor i in w :\n",
      "\t\t\t\t\tpred[ i ] = list( [ v ] )\n",
      "## For all nodes that already had a shortest path\n",
      "\t\t\tif len( u ) :\n",
      "##  update their path count and predecessor lists\n",
      "\t\t\t\tsigma[ u ] += sigma[ v ]\n",
      "\t\t\t\tfor i in n :\n",
      "\t\t\t\t\tappend( pred[ i ], v )\n",
      "## Initalize the pair dependency array\n",
      "\t\tdelta = np.zeros( A.shape[ 0 ], np.float )\n",
      "## The stack contains vertices in non-increasing order\n",
      "\t\twhile S :\n",
      "\t\t\tw = pop( S )\n",
      "## Update dependency of each predecessor\n",
      "\t\t\tv = pred[ w ]\n",
      "\t\t\tif len( v ) :\n",
      "\t\t\t\tdelta[ v ] += sigma[ v ] * ( 1.0 + delta[ w ] ) / sigma[ w ]\n",
      "\t\t\tif w != s :\n",
      "\t\t\t\tcb[ w ] += delta[ w ]\n",
      "\treturn cb\n",
      "# T = spma.csc_matrix( [ [ 0,1,1,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1], [1,0,0,0,0] ] )\n",
      "# T = spma.csc_matrix( [ [ 0,0,1,0,0 ], [ 0,0,1,0,0 ], [1,1,0,1,1], [ 0,0,1,0,0 ], [ 0,0,1,0,0 ] ] )\n",
      "# T = spma.csc_matrix( [ [ 0,0,1,0,0 ], [ 0,0,1,0,0 ], [0,0,0,1,1], [ 0,0,0,0,0 ], [ 0,0,0,0,0 ] ] )\n",
      "# %lprun -f brandes_centrality brandes_centrality( A )\n",
      "# brandes_centrality( T )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Page Rank\n",
      "The basic idea of Page Rank, though recursive, is to assign each vertex some importance score based on the aggregate importance score of the node connected to in via incoming edges.\n",
      "\n",
      "The PageRank score has the following interpretation: the more likely it is to find a random surfer in a node, the higher is the pagerank of that node. A random surfer is an agent who follows outgoing links randomly. In terms of the random walk process it is the share of time spent at any particular node in the network.\n",
      "\n",
      "Immediately two issues become apparent:\n",
      " * _Dangling vertices_: the vertices with incoming edges only.  \n",
      "   Since the network is finte, the pure random walk process would end up in such a sink node with probility one in finite number of steps; To avoid this, the walker is forced to restart at any node in the graph chosen uniformly at random.\n",
      " * _Clique traps_. This is a generalization of sink vertices:  \n",
      "   once in a completely connected subgraph, the random surfer is unable to leave the subgraph. In this case at any given node the process is allowed with some probability to make a jump to an arbitrary vertex in $G$ chosen uniformly at random.\n",
      "\n",
      "Let $G=(V,E)$ be a directed graph and its incidence matrix be given by $\\big(A_{vu}\\big)_{u,v\\in V}$ where $A_{uv} = 1_{u\\leadsto v}$,  ie. the $u,v$-th element indicates the weight in general, or the existence in particular, of an edge $u\\leadsto v$ between vertices $u,v\\in V$. Let the out degree be defiend as $\\delta^+_i = \\sum_{j\\in V} a_{ij}$.\n",
      "\n",
      "Consider a $1$-order markov chain with trasnition kernel defined as the stochastic version $P$ of the incidence matrix $A$, defiend as\n",
      "$$q_{ij} = \\frac{ a_{ij} }{\\delta^+_i} 1_{\\delta^+_i\\neq 0} +\\frac{1}{|V|} 1_{\\delta^+_i = 0}$$\n",
      "which denotes the probability of a $i\\to j$. This transition kernel already incorporates the uniform correction for the dangilng nodes.\n",
      "\n",
      "Further adjustnet is due. In orther to escape from the clique traps, the trasition distribtuion at each vertex is further smoothed: having selected a particualt edge $i\\to j$ to follow, the process actually moves to the destination node $j$ with probability $\\beta$ and to any vertex in the graph picked at random (including $j$) with probability $1-\\beta$.  \n",
      "\n",
      "Thus the funal Pagerank process transition kernel is given by the following fomrula for the probability of ending up in one step at node $j$ having started at $i$\n",
      "$$p_{ij} = \\beta q_{ij} + (1-\\beta) \\frac{1}{|V|}$$\n",
      "\n",
      "The matrix $p_{ij}$ is in fact row-stochastic, meanin that on each row the values in the columns sum to one. For example if a column vector $(\\pi_i)_{i\\in V}$ is a prior probability distribution of being in vertex $i$, then the posterior probability is $P'\\pi$.\n",
      "\n",
      "Due to Perron-Frobenius theorem for a well behaved stochastic matrix there necessarily exists a stationary probability distribution. The PageRank is this stationary distribution: $P'\\pi = \\pi$ with $\\pi'\\mathbf{1} = 1$. Furhtermore the the largest eigenvalue of $P'$ is exactly $1$, which menas that iterating $\\pi_k = P'\\pi_{k-1}$ would yield a convergent sequence of probability vectors.\n",
      "\n",
      "In general the incidence matrices are quite large in terms of dimensions which actually requires some modification of how exactly the next refinenemt should be constructed:\n",
      "$$\\pi_{k+1}' = \\beta \\pi_k' D^{-1} A + \\big( \\beta \\pi_k'd + 1-\\beta\\big) \\frac{1'}{|V|}$$\n",
      "where $D_{ii} = \\delta^+_i$ is a diagonal matrix. This expression does not require the transpose of the incidence matrix and \n",
      "\n",
      "The relative tolerance convergence criterion is\n",
      "$$\\max_{i\\in V} \\big\\lvert \\pi_{k+1} - \\pi_k\\big\\rvert \\leq \\epsilon|\\pi_k| + \\epsilon^2$$\n",
      "this permits relative error of approximately $\\epsilon$ tolerance for non-zero values of $\\pi_k$, and yet requires higher precision if the value on hte previous step was zero. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The iterative procedure\n",
      "As have been mentioned eralier the PageRank score is the egienvector of a stochastic matrix corresponding to the unit eigenvalue. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def PageRank_iter( A, beta = 0.85, x0 = None, rel_eps = 1.0E-8, niter = 10000 ) :\n",
      "## Create a teleporation vector\n",
      "    E = np.full( A.shape[ 0 ], 1.0 / A.shape[ 0 ], np.float )\n",
      "## If the initial ranking is not provided us the uniform distribution over the nodes\n",
      "    x0 = np.copy( E ) if x0 is None else x0\n",
      "## Find the normalising constants for each row\n",
      "    out = np.array( A.sum( axis = 1 ), np.int ).flatten( )\n",
      "## Locate the dangling vertices\n",
      "    dan = np.array( out == 0 )\n",
      "##  ... and reset their normalising constant to avoid NANs\n",
      "    out[ dan ] = 1.0\n",
      "## The resulting status of the convergence procedure:\n",
      "##  0 -- convergence within the set relative tolerance\n",
      "##  1 -- exceeded the number of iterations.\n",
      "    status = 1 ; i = 0\n",
      "## First stopping rule: within the specified number of iterations\n",
      "    while i < niter :\n",
      "## The main computational step\n",
      "        x1 = beta * ( x0 / out ) * A + ( beta * np.sum( x0 * dan ) + 1 - beta ) * E\n",
      "## Second stopping rule: within the required tolerance. Correction for \n",
      "##  possible machine zeros in the denominator.\n",
      "        if np.sum( np.abs( x1 - x0 ) / ( np.abs( x0 ) + rel_eps ) ) < rel_eps :\n",
      "            status = 0\n",
      "            break\n",
      "## Proceed to the next iteration\n",
      "        x0 = x1 ; i += 1\n",
      "## return the stationary distribution and the convergence information\n",
      "    return ( x1, { 'convergence': status, 'iterations' : i } )\n",
      "## Some small test cases\n",
      "# T = spma.csc_matrix( [ [ 0,1,1,0], [0,0,1,0], [1,0,0,1], [0,0,0,0] ] )\n",
      "# T = spma.csc_matrix( [ [ 0,1,1,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1], [1,0,0,0,0] ] )\n",
      "# print PageRank_iter( T, .9, rel_eps = 1e-10 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Hubs and Authorities index\n",
      "\n",
      "The basic idea of this ranking is ...\n",
      "\n",
      "The HITS algorithm preforms the following steps in succession until the convergence criteria are met:\n",
      " * the current authority and hub scores are given by $a_i$ and $h_i$ respectively;\n",
      " * Based on the current scores calculate the next iteration:\n",
      "    * $a_{i+1} = \\alpha A' h_i$\n",
      "    * $h_{i+1} = \\beta A a_{i+1}$\n",
      "\n",
      "This implies that $h_{i+1} = \\alpha \\beta AA' h_i$ which, when re-normalised by its $l_2$-norm, is known to converge to the egienvector of $AA'$ with the largest eigenvalue.\n",
      "Simalrly, $a_{i+1} = \\alpha \\beta A'A a_i$ is the main iterative core, which converges to the eigenvector of $A'A$.\n",
      "\n",
      "The SVD of an $n\\times m$ matrix $A$ of rank $k$ is a decomposition of it into the product $U\\Sigma V'$ with $U$ -- an $n\\times k$ matrix of orthogonal columns, $V$ -- an $m\\times k$ matrix again of orthogonal columns, and a diagonal $k\\times k$ matrix of positive singular values.\n",
      "It is easy to see, that $A'A = V\\Sigma'\\Sigma V'$ and $AA'=U\\Sigma\\Sigma' U'$, whence $A'A V = V\\Sigma'\\Sigma$ and $AA'U = U\\Sigma\\Sigma'$. Therefore the left and the right singular vector are given by the columns of $U$ and $V$ respectively.\n",
      "\n",
      "The HITS algorithm essentially performs power iterations on the pair of matrices $A'A$ and $AA'$ simultaneously. That is why \n",
      ":\n",
      " 1. the authority and the hub scores are the right and the left signular vectors respectively;\n",
      " 2. the algorithm converges to a pair of singular vectors corresponding to the largest singular value.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def HITS_iter( A, xy0 = None, rel_eps = 1.0E-8, niter = 10000 ) :\n",
      "## Initialise to the uniform hub-authority vector\n",
      "    if xy0 is None:\n",
      "        xy0 = np.full( ( 2, A.shape[0] ), 1.0 / np.sqrt( A.shape[0] ), np.float )\n",
      "## A_{ij} is whether there is an edge i\\to j\n",
      "    status = 1 ; i = 0\n",
      "    while i < niter :\n",
      "        xy = xy0.copy( )\n",
      "## Compute the authorities (a' = \\alpha h' A)\n",
      "        xy[1] = xy[0] * A\n",
      "        xy[1] /= np.linalg.norm( xy[1] )\n",
      "## and the hubs score (h = \\beta A a)\n",
      "        xy[0] = A * xy[1]\n",
      "        xy[0] /= np.linalg.norm( xy[0] )\n",
      "        if np.sum( np.abs( xy - xy0 ) / ( np.abs( xy0 ) + rel_eps ) ) < rel_eps :\n",
      "            status = 0\n",
      "            break\n",
      "        xy0 = xy ; i += 1\n",
      "    return ( xy, { 'convergence': status, 'iterations' : i } )\n",
      "# A = spma.csc_matrix( [ [ 0,1,1,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1], [1,0,0,0,0] ], dtype = np.float )\n",
      "# ha, _ = HITS_iter( A )\n",
      "# h, _, a = spma.linalg.svds( A, 1 )\n",
      "# h = np.abs( np.array( h ).flatten( ) )\n",
      "# a = np.abs( np.array( a ).flatten( ) )\n",
      "# print np.sum( np.abs(ha[0]-h)/( np.abs( h ) + 1e-8) )\n",
      "# print np.sum( np.abs(ha[1]-a)/( np.abs( a ) + 1e-8) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can load .mat files with the following commands:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.io\n",
      "data = scipy.io.loadmat('./data/flickr.mat')\n",
      "\n",
      "A = spma.csc_matrix( data[ 'A' ] )\n",
      "names = data[ 'names' ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data contains sparse matrix A and list of user names.\n",
      "This is a \u201cdenser\u201d part of the Flickr photo sharing site friendship graph from 2006. Edge direction corresponds to friendship requests (following). Some of the links are reciprocal,others not.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute degree centrality, Pagerank and  HubAuthorities scores for the [flickr](https://www.dropbox.com/s/6c4ybirvffrm2tu/flickr.zip?dl=0) network. \n",
      "Provide top 50 names in each ranking, compare results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Solution\n",
      "Compute the page rank for different restart-teleport probabilities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = np.arange(0, 1, 0.01 ) ##[ 0.01, .15, .25, .50, .75, .85, .99 ]\n",
      "pr = [ ( PageRank_iter( A, beta = b ), b ) for b in beta ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get the 50 top ranking vertices for each value in the $1\\times 100$ $\\beta$-grid."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T = 50\n",
      "top_pr_test = [ np.argsort( r )[:-T-1:-1] for (r,_), b in pr ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The nature of the rankings suggests that the similarity beetween them is calculated in a discrete manner -- using Jaccard's similarity measure."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jaccard( a, b ) :\n",
      "    x,  y = set( a ), set( b )\n",
      "    return ( len( x & y ) + 0.0 ) / len( x | y )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the scipy's spatial module to easily compute the pairwise similarity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.spatial.distance import pdist, squareform\n",
      "D = squareform(pdist(top_pr_test, jaccard))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Disaplay the similarity of 50 top ranking node in Flickr network between PageRank for ach pair of $\\beta$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_,_,vt = np.linalg.svd( D )\n",
      "i = np.argsort( vt[0,:] )\n",
      "plt.figure( figsize=(16,6) )\n",
      "plt.subplot(121)\n",
      "plt.title(\"\"\"Jaccard similarity between PageRank rankings\"\"\")\n",
      "plt.imshow( D )\n",
      "plt.colorbar()\n",
      "plt.subplot(122)\n",
      "plt.imshow( D[np.ix_(i,i)] )\n",
      "plt.title(\"\"\"Re-ordered Jaccard similarity between PageRank rankings\"\"\")\n",
      "plt.colorbar()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In general close settings of $\\beta$ tend to produce close rankings. This plot clearly shows that, the pageranks for $\\beta\\in \\big[0.375 0.625\\big]$ produce constitute the largest spectral cluster of similar rankings. Now display the top"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Lets compute the pagerank which is more determined\n",
      "##  by the adjacency matrix rather than the teleportation,\n",
      "pr, _ = PageRank_iter( A, beta = .85, rel_eps = 1.0E-7)\n",
      "## Settle on this paramter\n",
      "top_pr = np.argsort( pr )[:-T-1:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Compute the Hubs-Authorities scores\n",
      "ha, _ = HITS_iter( A )\n",
      "top_hs = np.argsort( ha[0] )[:-T-1:-1]\n",
      "top_as = np.argsort( ha[1] )[:-T-1:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## And finally compute the degree centrality\n",
      "cd = degree_centrality( A, norm = False )\n",
      "top_cd = np.argsort( cd )[:-T-1:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cb = betweenness_centrality( A ) if False else np.empty(0, np.int64)\n",
      "top_cb = np.argsort( cb )[:-T-1:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names[top_cd]\n",
      "names[]\n",
      "names[top_as]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<hr />\n",
      "Task 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are the [Facebook friendship graphs](https://www.dropbox.com/s/l4z8x81a9bolpw5/universities.zip?dl=0) from several US universities from 2005 (one year after fb launch).\n",
      "\n",
      "Data contains a A matrix (sparse) and a \"local_info\" variable, one row per node: \n",
      "a student/faculty status flag, gender, major, second major/minor (if applicable), dorm/house, year, and high school. \n",
      "Missing data is coded 0.\n",
      "\n",
      "Compute node degree assortativity (mixing by node degree) and assortativity coefficient (modularity) for gender, major, dormitory, year, high school for all universities and compare the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}