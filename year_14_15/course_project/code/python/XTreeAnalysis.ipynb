{
 "metadata": {
  "name": "",
  "signature": "sha256:2289a03af2bb02d0ebb6e694945e0fbb5d1b75b885c111f25eb427186f54a113"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Crossing times\n",
      "Suppose ${( t_i, x_i)}^N_{k=1}$ is a sample of some continuous process ${( X_t )}_{t\\in T}$ where $X_i = X(t_i)$.<!--, with $T\\subseteq \\mathbb{R}$ an inteval.-->\n",
      "\n",
      "The crossing times are defined as follows: for a given base grid spacing $\\delta>0$ and for $n,k\\geq0$ they are the stopping times $$T^n_{k+1} = \\inf\\Big\\{ t > T^n_k\\,: \\, \\big | X_t - X_{T^n_k} \\big | \\geq \\delta 2^n \\Big\\}$$ with $T^n_0 = 0$. The forcing of the zero-th xcrossing time to $0$ is done to align the grid with the process, so in effect, without losss of generality we may consider processes, which start at the origin.\n",
      "\n",
      "The parameter $\\delta$ is the spacing of the finest grid, with respect to which the leaves of the crossing tree are computed. Parent nodes of these leaves represent crossings of a coraser grid, namely $2\\delta$. The choice of $\\delta$ affects the tree in the following way in the case of a sampled process:\n",
      "**_THIS SECTION MUST BE DISCUSSED_**\n",
      "1. The grid with too low a value of $\\delta$ would be crossed by straight line segments between each pair consecutive sample observations. This could poison the distribution with some unfavouralbe yet unknown mixture and lead to excessive number of seemingly meaningless crossings.\n",
      "2. Too large $\\delta$ lead to a very poor and under sampled crossing tree.\n",
      "\n",
      "By construction, for a binary crossing grid, $N^l_k$, the number of subcrossings in any complete crossing between $T^l_k$ and $T^l_{k+1}$ is always an even number. This is due to the fact, that each crossing is registered as soon as two unidirectional subcrossings are encoutered, as seen by the following.\n",
      "\n",
      "Suppose $T^{n+1}_k$ and $T^n_m$ are aligned in that $T^{n+1}_k=T^n_m$, and $T^{n+1}_{k+1}<+\\infty$, i.e. the $n+1$ grid crossing is complete. First of all $T^n_{m+1},T^n_{m+2}\\leq T^{n+1}_{k+1}$, since otherwise the process would have crossed left the $\\pm\\delta 2^n$ band before it left the $\\pm \\delta 2^{n+1}$ band, which is twice as wide.\n",
      "\n",
      "The process is continuous which means that almost surely for any $\\epsilon>0$ there is $\\eta>0$ such that for any $t$ with $\\big|t-T^n_{m+1}\\big| < \\eta$ it holds that $\\big |X_{T^n_{m+1}}-X_t\\big|<\\epsilon$.\n",
      "Also for every $t\\in\\big[T^n_k, T^n_{k+1}\\big)$ it cannot be otherwise but $\\big | X_t - X_{T^n_k} \\big | < \\delta 2^n$. \n",
      "In particular, for $\\epsilon = \\frac{1}{2}\\delta 2^n$ it means that for a small while after $T^n_{m+1}$ the process is almost surely still within the $\\pm \\delta 2^{n+1}$ band. Therefore $T^n_{m+1} < T^{n+1}_{k+1}$.\n",
      "\n",
      "Thus it is true that $$1\\leq \\bigg | \\frac{1}{\\delta 2^n} \\big(X_{T^n_{m+1}} - X_{T^n_m}\\big)\\bigg | < 2$$. The next crossing of $\\pm\\delta 2^n$ takes palce at $T^n_{m+2}$ and there are two possibilities:\n",
      "1. the process crossed a new $\\pm\\delta 2^{n+1}$ grid line: $\\big|X_{T^n_{m+2}} - X_{T^n_m}\\big|\\geq 2\\cdot\\delta 2^n$. In this case $T^{n+1}_{k+1}\\leq T^n_{m+2}$ and there are _two_ subcrosings.\n",
      "2. the process moved back to the level $X_{T^n_m}$, which does not incurr a crossing of $\\pm\\delta 2^{n+1}$ grid line, yet is registered by the $\\pm\\delta 2^n$ grid. In this case $T^n_{m+2} < T^{n+1}_{k+1}$ and $X_{T^n_{m+2}} = X_{T^{n+1}_{k+1}}$ -- back to the beginning of this argument.\n",
      "\n",
      "Since the crossing is complete, $T^{n+1}_{k+1}<+\\infty$ implies that sooner that later a crossing of $\\pm\\delta 2^{n+1}$ grid occurs, in which case $T^{n+1}_{k+1}=T^n_{m+2p}$, meaning that there were exactly $2p$ subcrossings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "from synthfbmcircul import synth_fbm\n",
      "from hsssi_processes import *\n",
      "from Weierstrass import synth_Weier\n",
      "\n",
      "from crossing_tree import *\n",
      "from monte_carlo import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Distribution checking\n",
      "Let's test the hypothesis that the number of subcrossings follows a distribution similar to geometric. If $G$ is a geometrically distributed random variable, $G\\sim \\text{Geom}(\\theta)$, then $\\mathbb{P}(G=k) = {(1-\\theta)}^{k-1}\\theta$ for any $k\\geq1$, its expectation is $\\mathbb{E}(G) = \\theta^{-1}$.\n",
      "\n",
      "To reiterate, we test the hypothesis that $2N\\sim \\text{Geom}(\\theta)$. Since it is not knon beforehand how much subcrossings there will be in any realisation of the crossing tree, we estimate a truncated geometric distribution: for $k=1\\ldots\\bar{k}$ $$\\mathbb{P}(N=k) = {(1-\\theta)}^{k-1}\\theta\\,1_{k<\\bar{k}} + {(1-\\theta)}^\\bar{k}\\,1_{\\bar{k}\\leq k}$$\n",
      "with an atypical for geometric distributons concentration of probability at the upper truncation level $\\bar{k}$.\n",
      "\n",
      "Since, as has been shown before, the numbers of subcrossings are always even, the distribution of interest in our case becomes\n",
      "$$\\mathbb{P}(N=k) = {(1-\\theta)}^{\\tfrac{k}{2}-1}\\theta\\,1_{k<\\bar{k}} + {(1-\\theta)}^\\tfrac{\\bar{k}}{2}\\,1_{\\bar{k}\\leq k}$$ for $k=2,4,\\ldots\\bar{k}$.\n",
      "\n",
      "Given a sample ${(g_i)}_{i=1}^N$ of a geometrically distributed random variable truncated by some $\\bar{k}$, it is easy to show that the ML extimator of the probability parameter $\\theta$ is given by $$\\hat{\\theta} = \\frac{N-\\#{\\big\\{\\left.i\\,\\right\\rvert\\,g_i=\\bar{k}\\big\\}} }{\\sum_{i=1}^N g_i}$$\n",
      "\n",
      "Indeed the likelihood function is given by $$\\text{LogLik} = \\sum_{v\\in V,v\\neq \\bar{k}} f_v\\frac{v}{2} \\ln {(1-\\theta)} - \\sum_{v\\in V,v\\neq \\bar{k}} f_v \\ln {(1-\\theta)} + \\sum_{v\\in V,v\\neq \\bar{k}} f_v \\ln \\theta + f_{\\bar{k}}\\frac{\\bar{k}}{2} \\ln {(1-\\theta)}$$ where $V$ is the set of all unique values in the sample and $f_v = \\#{\\left\\{\\left.i\\,\\right\\rvert\\,g_i=v\\right\\}}$ -- the number of observations with taking the value specified. Straight forward differentiation yields the desired estimate.\n",
      "\n",
      "The first order condition on optimal $\\theta$, obtained by equating th first derivative of the log-likelihood with respect to the parameter $\\theta$ to zero, are given by\n",
      "$$\\frac{1}{\\theta}\\sum_{v\\in V,v\\neq \\bar{k}} f_v = \\frac{1}{1-\\theta}\\Big(\\frac{1}{2}\\sum_{v\\in V} v f_v - \\sum_{v\\in V,v\\neq \\bar{k}}f_v\\Big)$$\n",
      "using the fact that the frequencies sum up to the the total number of observations $N$ we get\n",
      "$$\\frac{1}{\\theta}(N-f_{\\bar{k}}) = \\frac{1}{1-\\theta}\\Big(\\frac{1}{2}M - (N-f_{\\bar{k}})\\Big)$$\n",
      "where $M = \\sum_{v\\in V} v f_v$ is the sum of all observed values in the sample.\n",
      "Therefore after minor simplification we get $$\\frac{1}{\\theta} = \\frac{1}{2}\\frac{M}{N-f_{\\bar{k}}}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Given the agregated value-count data in X, compute the MLE of \\theta of the geometric distribution.\n",
      "def mle_theta( x ) :\n",
      "    S = sum( [ i*f for i,f in enumerate( x, 1 ) ] )\n",
      "    N = sum( x )\n",
      "    return ( N - x[ -1 ] ) / ( S + 0.0 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####The Monte Carlo routine\n",
      "The procedure below is performs the specified $M$ number of MonteCralo simulations of the sample paths of the process specified by the generator object. The paratmeters $K$ and $T$ determine the truncation performed while collecting the crossing tree data.  \n",
      "For example $K=4$ menas that subcrossings of size up to and including 4 are recorded in full detail, while every subcrossing of larger size is agregated and stored in the ``tail''.  \n",
      "\n",
      "Similarly for the parameter $T$, which set the threshold level of the crossing tree starting from the leaf level and up, beyond which the data is strored in aggregate manner."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Simulation results\n",
      "Below interspersed with the listings of code are the results obtained so far for the corssing tree monte carlo simulation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import IPython.parallel as mp\n",
      "\n",
      "## Initialize the cluster to run the Monte Carlo experiment\n",
      "def mp_mc_setup( genr, M = 100, K = 4, T = 2 ) :\n",
      "## The parallel computing part: use synchronous computations\n",
      "\tcli = mp.Client()\n",
      "\tcli.clear()\n",
      "\tclu = cli.direct_view()\n",
      "## Make the workers import the necessary dependencies\n",
      "\tclu.execute('import numpy as np', block = True )\n",
      "## The crossing tree toolkit\n",
      "\tclu.execute('from crossing_tree import xtree_build', block = True )\n",
      "# \twith clu.sync_imports( ) :\n",
      "# \t\timport numpy\n",
      "# \t\tfrom monte_carlo import mp_mc_kernel\n",
      "# \t\tfrom crossing_tree import xtree_build\n",
      "## The generators\n",
      "# \t\tfrom Weierstrass import synth_Weier\n",
      "# \t\tfrom synthfbmcircul import synth_fbm, synth_fgn\n",
      "# \t\tfrom hsssi_processes import synth_Rosenblatt, synth_Hermite3, synth_Hermite4\n",
      "# Distribute the workload evenly among the members of the cluster\n",
      "\tclu.scatter( 'local_replications', range( M ) )\n",
      "## Pass the necesary environment to the cluster.\n",
      "\tclu.push({\n",
      "\t\t'generator' : genr, 'K' : K, 'T': T,\n",
      "\t\t'M': M, 'kernel': mp_mc_kernel })\n",
      "\treturn clu\n",
      "\n",
      "## This is a genral procedure to be run on each node of the cluster.\n",
      "def mp_mc_worker( ) :\n",
      "# \treturn ( min(local_replications), max(local_replications) )\n",
      "## Define a local storage for the results of calls to the kernel\n",
      "#     result = []\n",
      "#     for m in local_replications :\n",
      "#         distr = kernel( m, genr, K, T )\n",
      "#         result.append( distr )\n",
      "## Perform the appointed number of monte carlo local_replications\n",
      "\treturn [ kernel( generator, K, T ) for m in local_replications ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## DD = monte_carlo_serial( genr, M=100, K = 16, T = 3 )\n",
      "def mp_monte_carlo( genr, M = 100, K = 16, T = 3 ) :\n",
      "    import time\n",
      "## Setup the cluster for the monte carlo experiment.\n",
      "    cluster = mp_mc_setup( genr, M, K, T )\n",
      "## Run the expriment asynchronously\n",
      "    result = cluster.apply_async( mp_mc_worker )\n",
      "## Wait intil the jobs are complete\n",
      "    while not result.ready( ):\n",
      "        time.sleep( 2 )\n",
      "## Track the porgress\n",
      "        print result.progress\n",
      "    return np.concatenate([r for r in result if r])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 2**15\n",
      "H = .5\n",
      "\n",
      "## First initalize the generator of the desired continuous process\n",
      "genr = synth_fbm( N, H )\n",
      "## Run the monte carlo on the cluster\n",
      "DD = mp_monte_carlo( genr, M = 100, K = 16, T = 3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DD[1,:,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dstack(tuple(DD))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mp_mc_run( cluster ) :\n",
      "    result = cluster.apply_async( mp_mc_worker )\n",
      "    while not result.ready( ):\n",
      "        time.sleep( 2 )\n",
      "        print result.progress\n",
      "    cube = np.empty( ( cluster['K'][0] // 2 + 1, cluster['T'][0] + 1, cluster['M'][0] ), dtype = np.int32 )\n",
      "    for w in result :\n",
      "        for i, s in w :\n",
      "            cube[:,:,i] = s\n",
      "    return cube\n",
      "\n",
      "dd = mp_mc_run( cluster )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta = np.apply_along_axis(mle_theta, 0, DD)\n",
      "mle_theta(DD[:,0,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot( theta[0,:], \"r-\")\n",
      "plt.plot( theta[1,:], \"b-\")\n",
      "plt.plot( theta[2,:], \"k-\")\n",
      "1/(1-np.log(np.mean(theta[2,:])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### Straighforward, dumb linear regression\n",
      "However, for the estimation purposes it is useful to worj with the complementary CDF, instead of the ``density'' itself:\n",
      "$$\\mathbb{P}(N\\geq k) = {(1-\\theta)}^k$$ this way no distinction between $k<\\bar{k}$ of $k=\\bar{k}$ needs to be done.\n",
      "\n",
      "For this purpose, let's estimate the following regression model: $$\\ln \\hat{p}_k \\sim C + \\beta k$$ where $\\hat{p}_k$ is the empirical probability that a crossing had no less than $k$ subcrossings and $\\beta = \\ln \\sqrt{(1-\\theta)}$.\n",
      "\n",
      "It is entirely posible that the distribution's shape depend on the depth. If it does, then there is no self similarity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import chi2_contingency as chisq_test\n",
      "def ctable( x, y ) :\n",
      "## This procedure makes a contingeny table out of x and y (they should be discrete)\n",
      "    lxy = np.append( x, y ).reshape( len( x ),2)\n",
      "    lxy = lxy.view(dtype = np.dtype([('x', x.dtype), ('y', y.dtype)]))\n",
      "    lc, lf = np.unique( lxy, return_counts = True )\n",
      "    print lc\n",
      "    xc = np.unique( x ) ; yc = np.unique( y )\n",
      "    txy = np.zeros( ( len( xc ), len( yc ) ), dtype = np.int )\n",
      "    vix = lc.view( ( np.int, len( lc.dtype ) ) )\n",
      "    ix = np.searchsorted( xc, vix[:,0] )\n",
      "    iy = np.searchsorted( yc, vix[:,1] )\n",
      "    txy[[ix, iy]] = lf\n",
      "    return txy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing on the deterministic cascade"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scaffolding_plot( *genr( ) )\n",
      "def run() :\n",
      "    monte_carlo( genr, M=100, K = 16, T = 3 )\n",
      "%lprun -f xtree_integer_crossings_fast run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plt.plot(d[0,:], \"rx\", d[1,:], \"bx\", d[2,:], \"kx\")\n",
      "# f ~ a^(k-1)(1-a)\n",
      "# l\n",
      "plt.plot( range(2, 13, 2), np.log(np.mean( d, 1 )) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = 0\n",
      "for s in a[1:]:\n",
      "    plt.step( s[0], np.log( s[1] * 2**m ), \"b-\")\n",
      "    m+=1\n",
      "plt.step( a[1][0], np.log( a[1][1] ), \"r-\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 2**10\n",
      "H = .5\n",
      "\n",
      "# draw(synth_fbm( N, H ))\n",
      "# draw(synth_Weier( N, H ))\n",
      "# draw(synth_Rosenblatt( N, H ))\n",
      "# draw(synth_Hermite3( N, H ))\n",
      "# draw(synth_Hermite4( N, H ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Test the process generators\n",
      "def draw( gen, seed = None ) :\n",
      "\tt, u = gen( seed )\n",
      "\tt, v = gen( seed )\n",
      "\tplt.plot( t, u, \"r-\", t, v, \"b-\" )\n",
      "\tplt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Last test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scaffolding_plot( t, x ) :\n",
      "    ht, hp, xt = xtree_build( t, x )\n",
      "    l = len( ht ) - 5\n",
      "    plt.figure( figsize = (15, 8) )\n",
      "    plt.plot( t, x, \"y-\")\n",
      "# for p in tp[l] : plt.axhline( p, linewidth = 1, color = 'k' )\n",
      "    plt.step( ht[l+0], hp[l+0], \"g>\" )\n",
      "    plt.step( ht[l+1], hp[l+1], \"b>\" )\n",
      "    plt.step( ht[l+2], hp[l+2], \"r>\" )\n",
      "    plt.step( ht[l+3], hp[l+3], \"k>\" )\n",
      "    plt.show( )\n",
      "#     return [zip(*np.unique( x, return_counts = True)) for x in xt ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Weierstrass import *\n",
      "N = 2**15\n",
      "# T, X = genr()\n",
      "T, X = synthweier( N, 0.7, 1.2, 1000, deterministic = True, seed = None )\n",
      "T = np.arange(N, dtype=np.float)/ (N-1)\n",
      "\n",
      "delta = np.std( np.diff( X ), ddof = 1 )\n",
      "Z = ( X - X[ 0 ] ) / delta\n",
      "delta\n",
      "lht, lhp, lhx = xtree_integer_crossings_fast( T, Z )\n",
      "\n",
      "print len(lht), len(lhp)\n",
      "lhp[ np.append( [False], lhp[1:]==lhp[:-1]) ]\n",
      "print len(T), len(Z)\n",
      "np.std( np.diff( lht ), ddof = 1 )-5.3e-05\n",
      "scaffolding_plot( T, X )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ht, hp, hx = xtree_integer_crossings( T, Z )\n",
      "print np.max(np.abs(ht-lht))+np.max(np.abs(hp-lhp))\n",
      "lht, lhp, lhx = xtree_super_crossing( lht, lhp, 2 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}