\documentclass[a4paper]{article}
\usepackage{fullpage}
%% Date of the last edit : 2013-11-21
%%\linespread{1.5}

\usepackage{graphicx, url}

\title{A little of linear algebra}
\author{Nazarov Ivan}

\usepackage{amsmath, amsfonts, xfrac}

%% My commands to shorten the text
%% regex: \\left\s*\|\s*([^\|]*?)\s*\\right\s*\|
\newcommand{\abs}[1]{\ensuremath{\left| #1 \right|}}

%% regex: \\left\s*\\\{\s*([^\{\}]*?)\s*\\right\s*\\\}
%% furhter regex: \\left\s*\\\{\s*(.*?)\s*\\right\s*\\\}
\newcommand{\obj}[1]{\ensuremath{\left\{ #1 \right\}}}

\newcommand{\cintc}[1]{\ensuremath{\left[ #1 \right]}}
\newcommand{\cinto}[1]{\ensuremath{\left[ #1 \right)}}
\newcommand{\ointc}[1]{\ensuremath{\left( #1 \right]}}

%% regex: \\left\s*\(\s*([^\(\)]*?)\s*\\right\s*\)
\newcommand{\brac}[1]{\ensuremath{\left( #1 \right)}}

%% regex: \\obj\{([^\{\}]*)\}_\{_\{([^\{\}]*)\}\}
%% and: \\obj\{([^$]+)\}_\{_\{([^$]+)\}\}
\newcommand{\seq}[2]{\ensuremath{\obj{#1}_{_{#2}}}}

%% regex: \\frac\{([^\{\}]+)\}\{2\}
\newcommand{\half}[1]{\ensuremath{\frac{#1}{2}}}

%% regex (bad): \^\{?\\tex\w+\{(.*?)\}\}?
\newcommand{\susc}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\sbsc}[1]{\ensuremath{_{\textrm{#1}}}}

% regex: \\begin\{equation\}\s*(.*)\s*\\end\{equation\}
\newcommand{\eqn}[1]{\begin{equation*} #1 \end{equation*}}
\newcommand{\eqnn}[1]{\begin{equation} #1 \end{equation}}
\newcommand{\eqna}[1]{\begin{eqnarray} #1 \end{eqnarray}}
%% \eqn{\inf_{i \in A}{a_i} + \inf_{i \in A}{b_i} \leq \inf_{i \in A}{\brac{a_i+b_i}}}

% regex: \\epsilon
\newcommand{\eps}{\epsilon}


\newcommand{\lin}[1]{\ensuremath{\text{lin}\obj{ #1 }}}
\newcommand{\spn}[1]{\ensuremath{\left[ #1 \right]}}
\newcommand{\Dim}[1]{\ensuremath{\text{dim}\brac{ #1 }}}
\newcommand{\Ker}[1]{\ensuremath{\text{Ker}\brac{ #1 }}}
\newcommand{\im}[1]{\ensuremath{\text{Im}\brac{ #1 }}}


\begin{document}
\maketitle
\section{Determinants and systems of linear equations} % (fold)
\label{sec:determinants_and_linear_systems}


% section determinants_and_linear_systems (end)

\section{Vector spaces} % (fold)
\label{sec:vector_spaces}

\subsection{Definitions and axioms} % (fold)
\label{sub:definitions_and_axioms}
%%from here
What is a vector space: main axioms. Subspaces: definition, correctness under inherited operations, equivalence of them to sets closed under finite linear combinations.
% subsection definitions_and_axioms (end)

\subsection{The spanning set} % (fold)
\label{sub:the_spanning_set}
Define the span of $S$, $\spn{S}$, as the intersection of all subspaces of $V$ which contain $S$, i.e. $\spn{S} = \bigcap_{W \in \Gamma } W$ where \[\Gamma = \obj{W \vert W \subseteq V,\; W \text{ is a vector space under the inherited operations}}\]

Note that an arbitrary intersection of subspaces of $V$ is itself a subspace of $V$. Indeed if $x_1, x_2 \in V' = \bigcap_\alpha W_\alpha$, then $x_1, x_2 \in W_\alpha$ for every $\alpha$. But $W_\alpha$ is a subset of $V$ which is also a vector space, and so for any arbitrary $\lambda_1, \lambda_2 \in \mathbb{C}$ the pairwise linear combination $x' = \lambda_1 x_1 + \lambda_2 x_2$ is an element of $W_\alpha$ for any $\alpha$. Thus $x' \in V'$, which means that the intersection is closed under pairwise linear combinations. Therefore $V'$ is also a subspace of $V$ by the established equivalence between being a subspace and being closed under pairwise linear combinations (under the inherited operations).

Thus $\spn{S}$ is indeed a subspace of $V$ which also contains $S$. By definition it is also the smallest such subspace, since whenever a subspace $W$ contains $S$, $\spn{S} \subseteq W$.

The linear closure of a non-empty subset $S$ of a vector space $V$ over a field $\mathbb{C}$, $\lin S$, is the set of all finite linear combinations of members of $S$: \[\lin S = \obj{\sum_k \lambda_k s_k \text{; for all } m \geq 1,\; \brac{s_k}_{k=1}^m \in S \text{ and } \brac{\lambda_k}_{k=1}^m \in \mathbb{C}}\] Note that though $S$ might not necessarily be finite, the linear combinations it spawns must be finite ``weighted'' sums (infinite combinations are ruled out be the definition).

By the equivalence of any subspace to a set closed under arbitrary finite linear combinations, $\lin{S}$ is a subspace of $V$. Hence $\spn{S} \subseteq \lin{S}$, since $\lin{S}$ trivially contains all members of $S$. But because $\spn{S}$ is a subspace it is closed under arbitrary finite linear combinations of its members, therefore in particular it contains all finite linear combinations of elements of $S \subseteq \spn{S}$. Thus $\lin{S} \subseteq \spn{S}$.

Therefore $\spn{S} = \lin{S}$ and the linear closure of $S$ is the smallest vector subspace of $V$ that contains $S$. Now by this definition the span of an empty subset of $V$ is the trivial subspace, which consists only of the zero element. Indeed every subspace $W$ of $V$ has to contain the zero element, $\obj{0_V} \subseteq W$, and the set $\obj{0_V}$ is a vector space. Thus one can complete the definition of a linear closure by postulating that $\lin{\emptyset} = \obj{0_V}$.

Now any subspace $W$ of $V$ is trivially a span of itself and therefore there exists a subset of $V$, namely $W$ itself, of which it is the linear closure. Since the linear closure of some subset of $V$ is a subspace of $V$, it follows that $W$ is a subspace of $V$ if and only if it is the linear closure of some subset of $V$. This is just a re-statement of the previously shown equivalence of subspaces to sets, closed under arbitrary finite linear combinations.
% subsection the_spanning_set (end)

\subsection{Linear independence} % (fold)
\label{sub:linear_independence}
A collection $S=\brac{v_i}_{i\in \mathfrak{I}}$ of vectors from some linear space $V$ is called linearly dependent if there exists an element $v_j$ that is a linear combination of $v_i$ for $i\neq j$ with at most \emph{finite} number of non-zero coefficients.

Suppose $\brac{v_i}_{i\in \mathfrak{I}}$ is linearly dependent. Then there is a finite $F\subseteq \mathfrak{I}$ with $j\notin F$ such that $v_j = \sum_{i\in F} \lambda_i v_i$ where $\lambda_i\in \mathbb{C}$. Thus among the elements of the sub-collection $S'=\brac{v_j, \brac{v_i}_{i\in F}}$ of $S$ there is at least one non-trivial linear relationship: setting $\lambda_j=-1\in\mathbb{C}$ results in a finite linear relationship among the elements of $S$: \[\lambda_j v_j + \sum_{i\in F} \lambda_i s_i + 0_V= (-1) v_j + v_j = 0_V\]

Conversely, suppose there is a finite non-empty $F\in \mathfrak{I}$ and a non-trivial $\brac{\lambda_i}_{i\in F}\in \mathbb{C}$, such that $\sum_{i\in F} \lambda_i v_i = 0_V$. When $\abs{F}=1$, there is $j\in \mathfrak{I}$ and a non-zero $\lambda\in\mathbb{C}$ such that $\lambda v_j = 0_V$. But then $v_j$ must be $0_V$, whence for any $i\neq j$ an element $v_i$ can be used to form a finite linear combination, albeit a trivial one, representing $v_j$.

If $\abs{F}\geq 2$ then some element of $S$ can be expressed as a linear combination of others. Indeed, by assumption not all of $\brac{\lambda_i}_{i\in F}$ are zero and so there is $j\in F$ such that $\lambda_j\neq 0$. For this $\lambda_j\neq 0$ there is an inverse value in $\mathbb{C}$ and so \[0_V = \sum_{i\in F} \lambda_i v_i = \sum_{i\in F,\,i\neq j} \lambda_i v_i + \lambda_j v_j\;\Rightarrow v_j = \frac{-1}{\lambda_j}\sum_{i\in F,\,i\neq j} \lambda_i v_i\] Therefore the existence of a finite linear relationship is equivalent to there being an element of $S$, that is a finite linear combination of others.

A collection $S=\brac{v_i}_{i\in\mathfrak{I}}$ is linearly independent if and only if it is not linearly dependent. In particular, for such a collection it is true that for all finite $F\subseteq\mathfrak{I}$ and for every $\brac{\lambda_i}_{i\in F}$ the fact that $\sum_{i\in F}\lambda_i v_i = 0_V$ implies that $\lambda_i = 0$ for every $i\in F$. Note that for any collection a trivial linear combination always produces the zero vector $0_V$. Therefore for a linearly independent collection for any finite $F\subseteq\mathfrak{I}$ the only solution $\brac{\lambda_i}_{i\in F}\in \mathbb{C}$ for the equation $\sum_{i\in F}\lambda_i v_i = 0_V$ is trivial: $\lambda_i=0$ for all $i\in F$.

Since linear independence is the logical negation of linear dependence, it is therefore equivalent to there being no element, which can be expressed as a finite non-trivial linear combination of other members. In particular, a linearly independent collection necessarily does not contain $0_V$, because $0 w=0_V$ for any $w\in V$. Furthermore linear independence guarantees that there is only one way to express $0_V$ with the members of the collection.

If for $w\in\spn{S}$ there are finite $F, G\subseteq \mathfrak{I}$ and $\brac{\alpha_i}_{i\in F}, \brac{\beta_i}_{i\in G}\in \mathbb{C}$ such that $w=\sum_{i\in F} \alpha_i v_i = \sum_{i\in G} \beta_i v_i$ then $\sum_{i\in F\cup G} (\alpha_i - \beta_i) v_i=0_V$, where $\alpha_i$ is zero outside $F$ and $\beta_i = 0$ for $i\in F\setminus G$. As a consequence $\alpha_i$ must be equal to $\beta_i$ over $F\cup G$, by the uniqueness of the representation of $0_V$. Therefore any vector in the span of a linearly independent collection has a unique representation as a finite linear combination of $S$.

Consider a collection $S=\brac{v_i}_{i\in\mathfrak{I}}$ with elements of a vector space $V$. Pick any $j\in\mathfrak{I}$ and suppose that $\spn{S_{-j}} = \spn{S}$, where $S_{-j}$ is the collection $\brac{v_i}_{i\in \mathfrak{I},\,i\neq j}$. Since every vector of $S$ is a finite linear combination involving itself, i.e. $v_j=\sum_{i\in\mathfrak{I},\,i\neq j} 0 v_i + 1 v_j$, it is true that $v_j\in \spn{S}$ and thus $v_j\in \spn{S_{-j}}$.

Pick any $j\in\mathfrak{I}$ then for any $w\in \spn{S_{-j}}$ the addition of $0 v_j = 0_V$ to its linear representation through the members of $S_{-j}$ makes it a linear combination of elements of $S$ but does not change it. Therefore $w\in \spn{S}$.
Now pick any $j\in\mathfrak{I}$ and suppose $v_j\in \spn{S_{-j}}$. For any $w \in \spn{S}$ there is a finite $F\subseteq\mathfrak{I}$ and $\brac{\lambda_i}_{i\in F}\in \mathbb{C}$ such that $w = \lambda_j v_j + \sum_{i\in F,\,i\neq j} \lambda_i v_i$. If $j\notin F$, then $w$ is a linear combination of the elements of $S_{-j}$ only, and thus $w\in \spn{S_{-j}}$.
If however $j\in F$, then, because $v_j$ is in the span of $S_{-j}$, there is another finite $G\subseteq \mathfrak{I}\setminus\obj{j}$ and $\brac{\alpha_i}_{i\in G} \in \mathbb{C}$ such that $v_j = \sum_{i\in G} \alpha_i v_i$. Since both $F$ and $G$ are finite their union is finite as well. Let $\lambda'_i=\lambda_i$ for $i\in F$ and $\lambda'_i=0$ for $i\in H\setminus G$, and $\alpha'_i=\alpha_i$ for $i\in G$ and $\alpha'=0$ for $i\in H\setminus F$. Then it is obvious that \[w = \lambda_j v_j + \sum_{i\in F,\,i\neq j} \lambda_i v_i = \lambda_j \sum_{i\in G} \alpha_i v_i + \sum_{i\in F,\,i\neq j} \lambda_i v_i = \sum_{i\in H,\,i\neq j} (\lambda'_j \alpha'_i + \lambda'_i) v_i\] because $j \notin G$. This means that $w \in \spn{S_{-j}}$, and consequently for any $j\in\mathfrak{I}$ it is true that $\spn{S_{-j}} = \spn{S}$ is necessary and sufficient for $v_j \in \spn{S_{-j}}$.

If the set $S=\brac{v_i}_{i\in\mathfrak{I}}$ is linearly dependent, then there exists $j\in\mathfrak{I}$ such that $v_j$ that is a linear combination of the other members of $S$ with at most finite number of non-zero coefficients. However, this is equivalent to $v_j$ being in the linear closure of $S_{-j}$, whence $\spn{S_{-j}} = \spn{S}$ by the above result. Therefore linear dependence of $S$ implies that there \emph{exists} a member of $S$, the removal of which does not shrink the span. Therefore, if for every $j\in\mathfrak{I}$ the member $v_j$ is such, that $\spn{S_{-j}} \neq \spn{S}$ (i.e. its removal shrinks the span) then the set $S$ must be linearly independent.

Conversely, suppose that that there is $j\in \mathfrak{I}$ such that $\spn{S_{_j}}=\spn{S}$. Clearly $v_j\in\spn{S}$, from where it follows that $v_j \in \spn{S_{-j}}$. This literally implies that there is an element of $S$ which is a finite linear combination of others, whence the collection $S$ is linearly dependent. Therefore for the linear independence of a collection $S$ it is necessary and sufficient that the removal of any member shrinks the span:\[S\text{ is linearly independent } \Leftrightarrow \obj{\forall{j\in \mathfrak{I}}:\;\spn{S_{-j}} \neq \spn{S}}\]

A linearly independent collection can be thought of as the ``minimal'' spanning set in the sense that further elimination of the member vectors necessarily reduces the spanned space. At the same time in a linearly dependent collection there are always elements, which can be removed without affecting the span. For instance, if there are two identical vectors in a collection, a non-trivial linear relationship can be formed from them, or one can be expressed through another and consequently the collection is linearly dependent. Thus, in particular, a linearly independent set has neither zeroes, nor duplicates.

Let $S=\brac{v_i}_{i\in\mathfrak{I}}$ be some collection. Pick any arbitrary $w\in V$ and let $S_{+w}=\brac{v'_i}_{i\in\mathfrak{J}}$, where $\mathfrak{J}=\mathfrak{I}\cup\obj{j'}$ for some $j'\notin \mathfrak{I}$ and $v'_i=v_i$ for $i\in \mathfrak{I}$ and $v'_{j'}=w$. Note that it is fairly obvious that every linear combination of the members of $S$ can be regarded as a linear combination with an extra $0 w=0_V$, and so $\spn{S} \subseteq \spn{S_{+w}}$. Additionally, for an arbitrary collection $S\subseteq \spn{S}$.
If it is true that $\spn{S_{+w}}=\spn{S}$, then $w\in \spn{S}$, since $S_{+w}\subseteq \spn{S_{+w}}$ and $w\in \spn{S_{+w}}$.

Suppose that $w\in \spn{S}$ and pick any $v\in \spn{S_{+w}}$. Then there is $F\subseteq \mathfrak{J}$ and $\brac{\lambda_i}_{i\in F}$, such that $v=\sum_{i\in F}\lambda_i v'_i$. If $j'\notin F$, then $F\subseteq \mathfrak{I}$ and so $v=\sum_{i\in F} \lambda_i v_i\in \spn{S}$. If $j'\in F$, then $v=\sum_{i\in F,\,i\neq j'} \lambda_i v_i + \lambda_{j'} w$, where $w$ can be substituted for its representation as a finite linear combination of members of $S$ only. Therefore a new element cannot expand the span if and only if it is already in the span:\[\forall{w\in V}:\; w\in \spn{S}\; \text{ is equivalent to }\; \spn{S_{+w}} = \spn{S}\]

Now for any linearly independent $S=\brac{v_i}{i\in\mathfrak{I}}$ and $w\in V$, the set $\obj{S_{+w}}$ is linearly independent if and only if $w \notin \spn{S}$. Indeed, if $w\in \spn{S}$ then there is a finite $F\subseteq\mathfrak{I}$ and $\brac{\lambda_i}_{i\in F} \in \mathbb{C}$, such that $w = \sum_{i\in F} \lambda_i v_i$, whence follows the existence of a non-trivial linear relationship among the members of $S_{+w}$, and, consequently, $S_{+w}$ is linearly dependent. Therefore whenever the addition of a new vector to a linearly independent collection preserves linear independence this new element cannot be a member of the span of the original collection.

Conversely, suppose the inclusion of a new $w\in V$ has made $S_{+w}$ linearly dependent, despite $S$ being linearly independent. This means that there is a finite $F\subseteq \mathfrak{J}$ and $\brac{\lambda_i}_{i\in F}\in \mathbb{C}$, not all zero, such that $\sum_{i\in F} \lambda_i v'_i = 0_V$. Suppose for the sake of contradiction that $j'\notin F$, or $\lambda_{j'}=0$. Then the linear relationship involves the members of $S$ only with non-zero coefficients, since $\brac{\lambda_i}_{i\in F}$ is non-trivial. Therefore $S$ is linearly dependent -- a contradiction, which necessarily implies that $j'\in F$.

Thus $\lambda_{j'}\neq 0$, which means that this new $w$ is expressible as a linear combination of members of $S$ exclusively, $w \in \spn{S}$:\[\sum_{i\in F,\,i\neq j'} \lambda_i v_i + \lambda_{j'} w = 0_V \Rightarrow w = \frac{-1}{\lambda_{j'}} \sum_{i\in F,\,i\neq j'} \lambda_i s\] Therefore the addition of an element to a linearly independent set \emph{preserves} linear independence if and only if this new element is \emph{not} a member of the span of the original set. Whenever $S$ is linearly independent then for every $w\in V$ is is true that \[w\notin \spn{S} \Leftrightarrow S_{+w}\text{ is linearly independent}\]

\subsubsection{the Substitution lemma} % (fold)
\label{ssub:the_substitution_lemma}
The main result is that any non-zero vector $d$ in the linear closure of some spanning set $B$ can be seamlessly plugged into the set instead of some special vector without any consequences to the span.

Consider some finite collection $B=\obj{b_i}_{i=1}^m$ of vectors of some linear space $V$ and let $d\in \spn{B}\subseteq V$ be some non-zero element. Then $d=\sum_{i=1}^m \lambda_i b_i$ for some $\obj{\lambda_i}_{i=1}^m\in \mathbb{C}$ and there is $k\in 1\ldots m$ such that $\lambda_k b_k\neq 0_V$, because $d$ would have been $0_V$ otherwise. Furthermore $\lambda_k\neq 0$ and that $\mathbb{C}$ is a field together imply that there exists an inverse $\lambda_k^{-1}$, such that $\lambda_k^{-1} \lambda_k = 1\in \mathbb{C}$. Therefore, the representation of $d$ through $B$ can be manipulated to express $b_k$, even if it is the zero vector:\[d = \lambda_k b_k + \sum_{i\neq k} \lambda_i b_i \Leftrightarrow b_k = \frac{1}{\lambda_k} d - \sum_{i\neq k} \frac{\lambda_i}{\lambda_k} b_i\]

Let $W$ be a collection $\obj{w_i}_{i=1}^m$, where $w_i=b_i$ for $i\neq k$ and $w_k=d$. Then for any $b\in \spn{W}$ there are $\obj{\beta_i}_{i=1}^m\in \mathbb{C}$ such that $b = \sum_{i=1}^m \beta_i w_i = \beta_k d + \sum_{i\neq k} \beta_i b_i$. Therefore, after expanding the vector $d$, one gets \[b = \beta_k \brac{\sum_{i=1}^m \lambda_i b_i} + \sum_{i\neq k} \beta_i b_i = \beta_k \lambda_k b_k + \sum_{i\neq k} \brac{\beta_i + \beta_k \lambda_i} b_i\] whence it is obvious that $b$ is also a linear combination of the elements of the original $B$, and thus $b\in \spn{B}$.

Similarly, if $w\in \spn{B}$ then there are $\obj{\omega_i}_{i=1}^m$ such that $w = \sum_{i=1}^m \omega_i b_i$, which after plugging in the expression of $b_k$ through elements of $W$ yields \[w = \sum_{i\neq k} \omega_i b_i + \omega_k\brac{\frac{1}{\lambda_k} d - \sum_{i\neq k} \frac{\lambda_i}{\lambda_k} b_i} = \sum_{i\neq k} \brac{\omega_i-\lambda_i\frac{\omega_k}{\lambda_k}} w_i + \frac{\omega_k}{\lambda_k} w_k\] This means that $w$ is an element of the span of $W$, and consequently $\spn{B}=\spn{W}$.

If it is additionally assumed that $B$ is linearly independent, then on top of never shrinking the span, exchanging a non-zero element with a special vector in $B$ preserves the linear independence. Indeed, if there are $\obj{\beta_i}_{i=1}^m$ such that $\sum_{i=1}^m \beta_i w_i = 0_V$, then substituting the expression for $d$ yields a linear combination of the elements of $B$:\[\beta_k \lambda_k b_k + \sum_{i\neq k} \brac{\beta_i + \beta_k \lambda_i} b_i = 0_v\] Since $B$ is linearly independent, this combination must be trivial, implying that $\beta_k \lambda_k = 0$ and $\beta_i + \beta_k \lambda_i = 0$ for $i\neq k$. Since $\lambda_k\neq 0$, the first equation means that $\beta_k = 0$, and subsequently that $\beta_i = 0$. Therefore the solution to the linear relationship has to be a trivial one, and hence the $W$ is linearly independent as well.

However, for the case of $d = 0_V$ the linear independence plays a decisive part, since the elimination of one element of $B$ shrinks its span. However if $B$ is linearly dependent, then one has to be careful not to remove some element, which is a part of the maximal linearly independent subset.

In conclusion, the span and linear independence are retained whenever the set is augmented by plugging some non-zero vector from its span instead of some member, which enters non-trivially into that element's linear representation. Concisely, plugging some non-trivial $d\in \spn{B}$ instead of a certain element of the linearly independent collection $B$ preserves both its linear independence and its span.
% subsubsection the_substitution_lemma (end)

\subsubsection{the Spanning Set lemma} % (fold)
\label{ssub:the_spanning_set_lemma}
Even at first the result above might appear quite ordinary, it nevertheless indirectly enables one of the cornerstones of linear algebra -- the definition of a dimensionality of a vector space. In the paragraphs to come the following fact is shown: if $D$ is a collection of linearly independent vectors, such that for some other not necessarily finite collection $B$ it is true that $D\subseteq \spn{B}$, then the size of $D$ never exceeds the number of elements in the spanning set $B$.

First of all, let $V$ be some linear space, and $B_0$ be some possibly infinite collection of elements of $V$. Further, suppose $D=\brac{d_j}_{j=1}^n$ is some finite linearly independent collection, all elements of which lie within the span of $B_0$.
If the collection $B_0$ is infinite then, the claim is trivially true. Otherwise, if it is finite, there must be $m$, such that $B_0 = \brac{b_i}_{i=1}^m$.
In the case when $m$ is zero, $B_0$ is actually an empty set, and therefore its linear closure is $\obj{0_V}$. Since $D\subseteq \spn{B_0}$ and there are $n > 0$ vectors in it, it must be that $0_V\in D$, which contradicts its linear independence. Hence it cannot be that $m<n$ and, consequently, $D$ must itself be empty.

Consider a non-empty $B_0$. If $D$ is empty then $n\leq m$ holds vacuously, because no element of $D$ can be expressed through the rest since there are literally no elements in $D$.
If $D$ is non-empty, then its linear independence implies that $d_i\neq 0_V$ for any $i\leq n$. Thus the application of the ``substitution lemma'' yields the following observation: for some $j_1\in \obj{1\ldots n}$ there is $i_1\in I_0 = \obj{1\ldots m}$ such that the set $B_1 = \brac{w_i}_{i=1}^m$, where $w_i=b_i$ for $i\neq i_1$ and $w_{i_1}=d_{j_1}$, has the same span as $B_0$, but includes an element of $D$, namely $d_{j_1}$ instead of one of its former vectors $b_{i_1}$.

The next step is to prove inductively, in an arrangement such as this ($D$ is linearly independent and $D\subseteq \spn{B_0}$) the span of $B_0$ ultimately coincides with the span of some proper sub-collection of $D$. Suppose that at some step $k\in \obj{2\ldots m}$, the spanning set $B_{k-1}$ has exactly $k-1$ distinct elements of $D$ and it is true that $\spn{B_{k-1}} = \spn{B_0}$. The collection $B_{k-1}$ is $\brac{w_i}_{i=1}^m$, where $w_i=b_i$ for $i\in I_{k-1}$, $w_{i_s}=d_{j_s}$ for all $1\leq s\leq k-1$, $I_{k-1} = I_0\setminus \obj{i_s\vert 1\leq s\leq k-1}$ and $J_{k-1} = \obj{j_s\vert 1\leq s\leq k-1}$.
Note that the set elements of $D$ not in $B_{k-1}$ is non-empty, because $k\leq m < n$. Furthermore, since $D$ is linearly independent and $D\subseteq \spn{B_{k-1}}$, for an arbitrary remaining $j_k\in \obj{1\ldots n}\setminus J_{k-1}$, $d_{j_k}\neq 0_V$ and \[d_{j_k} = \sum_{i\in I_{k-1}} \delta_i b_i + \sum_{j\in J_{k-1}} \delta_j d_j\] for some $\obj{\delta_i}_{i\in I_{k-1}}, \obj{\delta_j}_{j\in J_{k-1}} \in \mathbb{C}$.
If $\delta_i b_i = 0_V$ for all $i\in I_{k-1}$, then the non-zero $d_{j_k}$ is a non-trivial linear combination of some vectors from $D$, namely those, which are in $B_{k-1}$. This, however, contradicts the linear independence of $D$, and therefore there must exist such $i_k\in I_{k-1}$ that $\delta_{i_k} b_{i_k} \neq 0_V$. Then by the ``substitution'' lemma, the set $B_k = \brac{w_i}_{i=1}^m$, where $w_i=b_i$ for $i\in I_k$ and $w_{i_s}=d_{j_s}$ for $1\leq s\leq k$ with $I_k = I_{k-1}\setminus \obj{i_k}$ and $J_k = J_{k-1} \cup \obj{j_k}$, has the same span as $B_{k-1}$, with yet another element of $B_0$ ($b_{i_k}$) substituted for a vector from $D$ ($d_{j_k}$).

The inductive argument therefore implies that $\spn{B_m}=\spn{B_0}$, where $B_m = \brac{\brac{v_i}_{i\in I_m}, \brac{d_j}_{j\in J_m}}$, where $I_m = I_0\setminus \obj{i_s\vert 1\leq s\leq m}$ and $J_m = \obj{j_s\vert 1\leq s\leq m}$.Next, the set $I_m = \emptyset$ and so $B_m$ has not a single element of the original $B_0$ left, since there are exactly $m$ elements in $I_0$ and $\brac{i_k}_{k=1}^m$ is a set of $m$ distinct indices from $I_0$: $B_m = \brac{d_{j_k}}_{k=1}^m$.
And finally, there is at least one more element left in $D$, because $m<n$, and hence there is $j_{m+1}\in \obj{1\ldots n}\setminus J_m$, such that $d_{j_{m+1}}\in \spn{B_0}$ and all $\brac{d_{j_s}}_{s=1}^{m+1}$ are distinct.
Therefore stringing these observations together results a contradiction to the assumed linear independence of $D$:\[d_{j_{m+1}}\in \spn{B_0} = \spn{\brac{d_{j_k}}_{k=1}^m}\]
In other words there is an element which can be expressed as a linear combination of other vectors, implying the set $D$ cannot be linearly independent. Consequently, the contradiction implies that the assumption $n>m$ is fallacious, and so it must be true that any linearly independent collection cannot be larger than a spanning set, in the linear closure of which it resides.
% subsubsection the_spanning_set_lemma (end)

% subsection linear_independence (end)


\subsection{Basis} % (fold)
\label{ssub:basis}
Before proceeding to the definition of a basis, note that since the definition of a linear combination explicitly rules out combinations involving infinite number of elements, the following exposition is restricted to finite collections of vectors only.

A collection of vectors $\brac{v_i}_{i\in \mathfrak{I}} \in V$ is called a basis of $V$ if and only if any element of $V$ can be represented as a linear combination of $S$ in a unique way. So, trivially, any basis spans the entire space of which it is a basis.

Let $B = \brac{v_i}_{i=1}^m$ be some basis of $V$. Thus for any $v\in V$ there is $\brac{\beta_i}_{i=1}^m$ such that $\sum_{i=1}^m \beta_i v_i = v$. Suppose $\brac{\lambda_i}_{i=1}^m \in \mathbb{C}$ are such that $\sum_{i=1}^m \lambda_i v_i = 0_V$. Then \[ v = v + 0_V = \sum_{i=1}^m \brac{ \beta_i + \lambda_i } v_i\] is another way to represent $v$ through $\brac{v_i}$. Had there existed a non-trivial representation of $0_V$, $\lambda_i \neq 0$ for some $1 \leq j\leq m$, then $v$ would have had at least two distinct representations through $B$, altogether contradicting the uniqueness of the representation property. Therefore the only way to get $0_V$ is by a trivial linear combination of $\brac{v_i}_{i=1}^m$, and a basis is necessarily linearly independent.

Suppose $B = \brac{v_i}_{i=1}^m$ is linearly independent and spans the entire space $V$. Since $V = \spn{\brac{v_i}_{i=1}^m}$, suppose for some $v \in V$ there exist two sets of coefficients $\brac{\alpha_i}_{i=1}^m$ and $\brac{\beta_i}_{i=1}^m \in \mathbb{C}$, such that \[v = \sum_{i=1}^m \alpha_i v_i = \sum_{i=1}^m \beta_i v_i\] Due to the linearity of the space and the commutative properties of addition and distributive properties of scalar multiplication, this is equivalent to \[\sum_{i=1}^m \brac{\alpha_i - \beta_i} v_i = 0_V\] If two representations of $v$ are distinct, then $\beta_i \neq \alpha_i$ for some $i$, implying a non-trivial linear relationship among the elements of $B$. Since this contradicts linear independence, one concludes that for no element of $V$ there exist more that one representation with respect to the set $\brac{v_i}_{i=1}^m$. Therefore any linearly independent set, which spans the entire space, is also its basis.

Let a finite collection $B = \brac{v_i}_{i=1}^m$ be a basis of $V$, and $D = \brac{d_j}_{j=1}^n$ be some linearly independent subset of $V$. Since $B$ is a spanning set of the whole space $V$, the ``Spanning Set lemma'' implies that $n\leq m$. Therefore whenever a finite set is a basis of some vector space, any linearly independent subset has at most as many elements as this basis and that any set with strictly more members than this basis must be linearly dependent. A corollary to this observation is that if a vector space has a finite basis, then any other basis must have exactly the same number of elements. Indeed, if a set is a basis, it must be linearly independent, and as such must not have more elements than any other basis, which spans the space, and in particular the one, existence of which has been postulated in the premise.
% subsection basis (end)

\subsection{Dimensionality} % (fold)
\label{sec:dimensionality}
A vector space $V$ is called finite dimensional if and only if it has a finite basis. Since in this case the number of elements in any basis of $V$ is the same, one can correctly define the dimensionality of the space $V$ as the size of any basis of it.

Suppose the dimensionality of $V$ is $d$, then there cannot exist a linearly independent set with strictly more elements without contradicting itself. Therefore in such space there exists a set of $d$ linearly independent elements and any set with $d+1$ or more elements is linearly dependent.

Conversely, suppose $V$ is such a vector space that there is a linearly independent collection with $d\geq 1$ vectors and any set of $d+1$ vectors is linearly dependent. If $B=\brac{b_k}_{k=1}^d \in V$ is the mentioned linearly \emph{independent} collection, then for any $v \in V$ the collection $\brac{\brac{b_k}_{k=1}^d, v}$ is linearly \emph{dependent} by the supposed property of the space $V$. Therefore by the ``extension'' lemma $v\in \spn{B}$ and consequently $B$ spans the space $V$. Thus $B$ is a basis and hence the dimensionality of $V$ is $d$.

Now if the vector space $V$ is such that only the zero-sized, or empty, set of its elements is linearly independent, then, in particular, for any $x \in V$ the singular collection $\brac{x}$ must be linearly dependent. Hence there is a non-zero $\lambda \in \mathbb{C}$ such that $\lambda x = 0_V$, which under this circumstances can only be satisfied if $x = 0_V$ itself. Therefore $V$ is a trivial vector space, which can only have an empty basis.

So an equivalent definition of a finite dimensional space is that there is a number $d\geq 0$ such that there exists a linearly independent set with $d$ elements, while any set with at least $d+1$ vectors is linearly dependent.

% subsection dimensionality (end)

%% and sums of spaces

\subsection{Basis extension and Sums of spaces} % (fold)
\label{sub:basis_extension_and_sums_of_spaces}

Consider some multiset $S$ of the vector space $V$. As it has been noted earlier, a linearly independent set is the minimal spanning set. Indeed, removal of any element causes the span to shrink, while removing specific elements of a linearly dependent set, namely those that are the linear combination of others, preserves the span. It should be noted, that in the process of linear dependency elimination, the removal of an element which is linearly independent of others necessarily shrinks the span. So after this elimination procedure, all that is left is the largest linearly independent subset $S'$ of $S$. Since the span has been preserved, any element of $\spn{S}$ is a linear combination of $S'$. Therefore the set $S'$ constitutes a basis of $\spn{S}$ and thus the dimensionality of the span is the size of the largest linearly independent subset of the spanning set.

Let $W$ be a subspace of a finite dimensional space $V$. Since $V$ is finite dimensional, it has a basis with finite number of elements, with respect to which any other element, and in particular any member of $W$, can be uniquely represented. Furthermore being a subset of $V$, $W$ cannot have a linearly independent set with strictly more elements than $\Dim{V}$. Therefore the subspace $W$ is necessarily finite dimensional, and hence $\Dim{W}$ cannot exceed $\Dim{V}$.

Now consider the case of $d = \Dim{V} = 0$. Then $V$ is the null-space, which makes $W$ the null-space too, since it is a subset of $V$ and a linear space. So in this case $\Dim{W} = \Dim{V} = 0$ if and only if $W = \obj{0_V} = V$. Suppose $\Dim{W} = \Dim{V} = d \geq 1$, then any basis of $W$ has as many elements as any basis of $V$. Let $\brac{w_i}_{i=1}^d\in W$ be a basis of $W$. As a basis it is a linearly independent set, which is also a subset of $V$, due to inclusion $W$ into $V$. Since $\Dim{V} = d$, this set $\brac{w_i}_{i=1}^d$ is also a basis of $V$, because any $d+1$ elements of $V$ are linearly dependent. Indeed, adding any extra element of $V$ to $\brac{w_i}_{i=1}^d$ would create a linear dependency, meaning that this new element is in $\spn{\brac{w_i}_{i=1}^d}$. So every element of $V$ is a linear combination of $\brac{w_i}_{i=1}^d$, which also spans $W$. Hence $V\subseteq W$, which together with $W$ being a subspace of $V$, leads to the conclusion that $W=V$ whenever a subspace $W$ has the same dimensionality as $V$.

Combining the last two statements gives the following corollary: a subspace does not coincide with the entire space if and only if its dimensionality is strictly less than that of the space it is contained in.

Now, any linearly independent set can be extended to a basis. Indeed, let $V$ be a finite dimensional vector space and $S$ be some linearly independent subset of it. Members of $S$ constitute a basis of its span, because $S$ is linearly independent and, trivially, any element of $\spn{S}$ is a linear combination of $S$. As $V$ is finite dimensional, $S$ has got to be a finite set as well, and so $\spn{S}$ is a finite dimensional space too, since the span is closed under linear combinations. Furthermore from $S\subseteq V$ follows that $\spn{S}$ is a subspace of $V$ under the inherited operations. Now if $\Dim{\spn{S}} = \Dim{V}$, then $S$ must be a basis of $V$, as any larger set of vectors in $V$ is linearly dependent. Otherwise, in the case of strictly lower dimensionality the span of $S$ must be a strict subspace of $V$, because $V$ having strictly higher dimensionality would contradict the fact, that $S$ would be a linearly independent spanning set of $W=V$ with less members, while all bases must have the equal size. Hence there exists an element in $V$, that is not in $\spn{S}$. Therefore adding such an element to $S$ would produce a linearly independent set $S'$ with a strictly larger span (because the new element is not in $\spn{S}$ dropping it from $S'$ would produce the original $S$). Bot both the new span and the original one are subspaces of $V$, and $\Dim{\spn{S}} < \Dim{\spn{S'}}$, since $S'$ has one extra element than $S$, while still being a linearly independent spanning set. Therefore repeating this procedure until $\Dim{\spn{S'}}$ reaches $\Dim{V}$, would ultimately produce a linearly independent set $S'$, the span of which coincides with $V$, making it its basis (it would take at most finite number of iterations since the spaces are finite-dimensional).

Now consider the set $F$ of elements, by which the linearly independent set $S$ has been extended to a basis of $V$. Since $S$ is finite, it can be arbitrarily enumerated as $\brac{s_i}_{i=1}^n$. If $F$ is empty, then vacuously the extension procedure above produced an altogether linearly independent set. If $F$ is not empty, then  its elements have been added some order, for instance, $F = \brac{f_j}_{j=1}^m$. Let's proceed with proof by contradiction. If the combined multiset $S\cup F$ is linearly dependent, then there exists such a set of coefficients not all zero, that \[\sum_{i=1}^n \alpha_i s_i + \sum_{j=1}^m \beta_j f_j = 0_V\] For these coefficients at least one of $\beta_j$ must be non-zero, because otherwise the non-trivial linear relationship above would involve the elements of $S$ only thereby contradicting its linear independence. Since there is a fintie number of elements in the extension $F$, there is such $1\leq k\leq m$, that $\beta_k \neq 0$, while $\beta_j=0$ for all $k<j\leq m$. Therefore the element $f_k\in F$, added at the $k\susc{th}$ iteration,  can be expressed as a linear combination of $S \cup \brac{f_i}_{i=1}^{k-1}$ -- elements included into the linearly independent set earlier. Thus such $f_k$ would have been in the current span $\spn{S \cup \brac{f_i}_{i=1}^{k-1}}$ and must not have been included at all. Therefore linear dependence of the final set necessarily contradicts the extension algorithm itself.

Furthermore, for any $x\in \spn{F}\cap \spn{S}$ there are $\brac{\alpha_i}_{i=1}^n$ and $\brac{\beta_j}_{j=1}^m$, such that $\sum_{i=1}^n \alpha_i s_i = x = \sum_{j=1}^m \beta_j f_j$. Hence such coefficients uncover a linear relationship among the final set $S\cup F$, which is linearly independent by construction: briefly, each newly added element lies outside the current span and therefore it cannot enter non-trivially into any linear relationship among the augmented set. Hence $\alpha_i = \beta_j = 0$ for all $i, j$ and so the common element of $\spn{F}$ and $\spn{S}$ cannot be anything but $0_V$.

Let $V$ be a finite-dimensional vector space and $W$ its arbitrary subspace. First, any subspace is a span of some set (at least of itself). Even though it is a fact, that linearly dependent set can be condensed to a linearly independent set, without shrinking its span, the set $W$ might have infinite number of elements, thereby making it impossible to iteratively eliminate the ones causing linear dependency. However, since $W$ is a space of not greater dimensionality than $V$, it must have a maximal linearly independent set $S$. If $S$ is the empty set, then the subspace $W$ must be the null-space, which means any basis of $V$ could be used to expand $S$ to produce a basis of $V$. If $S\neq \emptyset$, then it can be nothing short of a basis of $W$, since otherwise it could not be the maximal linearly independent set. Let $S = \brac{w_i}_{i=1}^r \in W$ be such basis, where $r = \Dim{W}$. Then by the above extension result $S$ can be extended with certain elements of $V$ to a linearly independent set $\brac{w_i}_{i=1}^{r+p}$, which spans the entire $V$. So, in short, the corollary to the above extension result is, that any basis of any subspace of a finite-dimensional space $V$ can be extended to a basis of $V$.

Now consider the extension of the original basis, $\brac{w_{r+i}}_{i=1}^p \in V$, where $p = \Dim{V} - r\geq 0$ is the dimensions $W$ is ``lacking''. This extension is a linearly independent subset of $V$, because the extension procedure adds a new vector if and only if it is not a linear combination of the existing ones, and so new vectors cannot have linear relationships among them. Hence, the dimensionality of $W_1 = \spn{\brac{w_{r+i}}_{i=1}^p}$ is exactly $p$, and it can very well serve as a basis of this subspace (if $p=0$ then the extension is an empty set, meaning that its span is the null-space $\obj{0_V}$, which has the empty set as its basis). Now note that no element of $W_1$, except for the trivial $0_V$, is a linear combination of $\brac{w_i}_{i=1}^r$ -- the spanning set of $W$. Indeed, let $x \in W$ and $x \in W_1$, then there are $\brac{\alpha_i}_{i=1}^r$ and $\brac{\beta_i}_{i=1}^p$, such that \[\sum_{i=1}^r \alpha_i w_i = \sum_{i=1}^p \beta_i w_{r+i} \Leftrightarrow \sum_{i=1}^r \alpha_i w_i - \sum_{i=1}^p \beta_i w_{r+i} = 0_V\] But $\brac{w_i}_{i=1}^{r+p}$ is a basis, so its linear independence implies that $\alpha_i = \beta_j = 0$ for all $i,j$. Therefore the only common element of $W$ and $W_1$ is the zero element $0_V$.\textbf{this actually follows from the extension result}

Let $W_2$ and $W_1$ be two subspaces of a vector space $V$. Then the sum of subspaces $W_1\oplus W_2$ is defined as the set $\obj{x_1+x_2\vert x_1\in W_1, x_2\in W_2}$, where the operations are inherited from the parent space $V$. The sum of subspaces is called a ``direct sum'' if for any $x \in W_1\oplus W_2$ there exist unique $x_1\in W_1$ and $x_2\in W_2$, such that $x = x_1+x_2$. Suppose two subsets have a null-space intersection, then their sum is always a direct sum. Indeed, pick any $v\in W_1\oplus W_2$ and suppose there are two pairs of vectors $x_1,y_1 \in W_1$ and $x_2, y_2 \in W_2$, such that $x_1+x_2 = v = y_1+y_2$. Then re-arranging the equation yields $x_1-y_1 = x_2-y_2$, which is an equality between an element from $W_1$ on the left, and $W_2$ on the right. Therefore both must be elements of the intersection of their parent subspaces. But $W_1\cap W_2 = \obj{0_V}$, which means that $x_1-y_1 = 0_V = x_2-y_2$ and so $x_1=y_1$ and $x_2=y_2$, implying that any element of $W_1\oplus W_2$ is uniquely decomposable into a sum of elements of $W_1$ and $W_2$.

Suppose the subspaces $W_1$ and $W_2$ of a finite dimensional space $V$ share the zero only, and $\Dim{W_1} + \Dim{W_2} = \Dim{V}$. Since each subspace is finite dimensional, each has a finite basis. Let $\brac{w_i}_{i=1}^p$ and $\brac{w_{p+i}}_{i=1}^q$ be the bases of $W_1$ and $W_2$ respectively, where $q=\Dim{W_2}$ and $p = \Dim{W_2}$. Since $W_1$ and $W_2$ are subspaces of $V$, the concatenation of their bases, the set $\brac{w_i}_{i=1}^{p+q}$, is a subset of $V$. Suppose there are $\brac{\lambda_i}_{i=1}^{p+q}\in \mathbb{C}$ such that $\sum_{i=1}^{p+q} \lambda_i w_i = 0_V$. Then re-arranging the elements into groups according the subspace where they belong produces \[\sum_{i=1}^p \lambda_i w_i = - \sum_{i=p+1}^{p+q} \lambda_i w_i\] Since the left hand side is an element of $W_1$, while the right hand side's linear combination uses only the vectors of $W_2$, it must be that both results belong to both subspaces simultaneously. Thus \[\sum_{i=1}^p \lambda_i w_i = 0_V = \sum_{i=p+1}^{p+q} \lambda_i w_i\] which, together with linear independence of each group of $w_i$ separately, implies that $\lambda_i = 0$ for all $1\leq i\leq p+q$. Hence the set $\brac{w_i}_{i=1}^{p+q}\in V$ is linearly independent. Now, as $\Dim{V} = p+q$, it must be that any larger set is linearly dependent, thus by adding any arbitrary $v\in V$ to $\brac{w_i}_{i=1}^{p+q}$, the resulting set becomes linearly dependent. Therefore it must be that $v\in \spn{\brac{w_i}_{i=1}^{p+q}}$, meaning that the concatenated bases make up a basis in $V$. Furthermore, since any vector has a unique representation with respect to a basis, for any $x\in V$ it is true that \[x = \sum_{i=1}^{p+q} \alpha_i w_i = \brac{\sum_{i=1}^p \alpha_i w_i} + \brac{\sum_{i=p+1}^{p+q} \alpha_i w_i} = x_1 + x_2\] where $x_1\in W_1$ and $x_2\in W_2$ are defined by the respective linear combinations in parentheses. Hence any element of $V$ can be represented as a sum of vectors from $W_1$ and $W_2$ in a unique way, since the intersection of the subspaces is the null-space. Therefore the space $V$ is a direct sum of subspaces $W_1$ and $W_2$.

This ``direct sum'' result and the previous ``basis extension'' result are extremely important, since, first, the latter allows one to move from a basis in a subspace $W$, to a proper basis of the enclosing finite dimensional space $V$, which includes the initial basis. Secondly, while extending the basis, the latter also constructs an complementary independent subspace $\hat{W}$ (i.e. no non-zero element of one subspace is a linear combination of vectors from the other) with $\Dim{W}+\Dim{W'} = \Dim{V}$, so by the former result, $V$ is the direct sum of the subspaces $W$ and $\hat{W}$. Thirdly, the former permits one to pick a basis in $V$, slice it into any number of non-overlapping subsets, make a direct sum out of their spans, and thus recover the original $V$. Indeed, all one has to do is pick a basis in $V$, split it in two groups, then continue splitting the group into increasingly finer sets, if necessary. Each group of the original basis spans a subspace and these subspaces have only null-space intersections. Thus they sum directly to the parent space.

Let $V_1$ and $V_2$ be two subspaces of a finite-dimensional vector space $V$. Consider $V_0 = V_1 \cap V_2$ -- the common subspace of the two and pick any basis in it $\brac{e_i}_{i=1}^m$, where $m = \Dim{V_0}$. Such basis exists by the assumption that $V$ is finite-dimensional and $V_0$'s being its subspace. Because $V_0$ is a subspace of $V_1$, $\brac{e_i}$ can be extended to a basis in $V_1$ by some $\brac{f_j}_{j=1}^{p-m}$, where $m\leq p = \Dim{V_1}$. Analogously there is $\brac{g_k}_{k=1}^{q-m}$ with $m\leq q = \Dim{V_2}$ such that $\brac{\brac{e_i}_{i=1}^m, \brac{g_k}_{k=1}^{q-m}}$ is a basis of $V_2$.

First, the combined set $\brac{\brac{f_j}_{j=1}^{p-m}, \brac{e_i}_{i=1}^m, \brac{g_k}_{k=1}^{q-m}}$ is linearly independent. Indeed, suppose there is $\brac{\alpha_j}_{j=1}^{p-m}$, $\brac{\beta_i}_{i=1}^m$ and $\brac{\gamma_k}_{k=1}^{q-m}$ from $\mathbb{C}$ such that\[\sum_{j=1}^{p-m} \alpha_j f_j + \sum_{i=1}^m \beta_i e_i + \sum_{k=1}^{q-m} \gamma_k g_k = 0_V\] This linear relationship can be manipulated to yield\[\sum_{i=1}^m \beta_i e_i + \sum_{k=1}^{q-m} \gamma_k g_k = -\sum_{j=1}^{p-m} \alpha_j f_j\] meaning that the right-hand side, which is an element of $V_1$, is expressible as a linear combination of elements of $V_2$. Hence on the right is an element of $V_0$, which has a basis $\brac{e_i}$. Therefore there are $\brac{\lambda_i}_{i=1}^m\in \mathbb{C}$ such that \[\sum_{j=1}^{p-m} \alpha_j f_j = -\sum_{i=1}^m \lambda_i e_i\] whence the following linear relationship emerges: \[\sum_{i=1}^m \brac{\beta_i+\lambda_i} e_i + \sum_{k=1}^{q-m} \gamma_k g_k = 0_V\] Because the extension procedure is correct, the set $\brac{\brac{e_i}_{i=1}^m, \brac{g_k}_{k=1}^{q-m}}$ is linearly independent, implying that, in particular, $\gamma_k = 0$ for all $1\leq k\leq q-m$. Thus\[\sum_{j=1}^{p-m} \alpha_j f_j + \sum_{i=1}^m \beta_i e_i = 0_V\] which is a linear relationship among the linearly independent elements, produced via the extension procedure of the set $\brac{e_i}$ of $V_0$ into the containing space $V_1$. Hence $\alpha_j = \beta_i = 0$ for all $1\leq i \leq m$ and $1\leq j \leq p-m$. Therefore any linear relationship among $\brac{\brac{f_j}_{j=1}^{p-m}, \brac{e_i}_{i=1}^m, \brac{g_k}_{k=1}^{q-m}}$ must be trivial, whence the set altogether is linearly independent.

Secondly, this set spans the space $\hat{V} = V_1\oplus V_2$ -- the sum of subspaces $V_1$ and $V_2$. For any $x\in \hat{V}$ by the definition of a sum of vector spaces there is $x_1 \in V_1$ and $x_2\in V_2$ such that $x = x_1+x_2$. Note that the extension procedure always produces a linearly independent set, spanning the containing space (the property of being the spanning set is actually the termination condition of the extension algorithm). So both $x$'s can be uniquely represented as linear combinations of the relevant bases ($x_1$ with respect to $\brac{\brac{f_j}_{j=1}^{p-m}, \brac{e_i}_{i=1}^m}$, while $x_2$ w.r.t. $\brac{\brac{e_i}_{i=1}^m, \brac{g_k}_{k=1}^{q-m}}$ ), whence each $x\in \hat{V}$ is a linear combination of $\brac{\brac{f_j}_{j=1}^{p-m}, \brac{e_i}_{i=1}^m, \brac{g_k}_{k=1}^{q-m}}$. Since the dimensionality of the span is the largest number of linearly independent elements from the spanning set, and the set $\brac{\brac{f_j}_{j=1}^{p-m}, \brac{e_i}_{i=1}^m, \brac{g_k}_{k=1}^{q-m}}$ already is linearly independent, the dimensionality of the sum $V_1+V_2$ is $\Dim{\hat{V}} = \brac{p-m} + \brac{q-m} + m$. Therefore, having rearranged the equality, one gets the balance of dimensionality of sums of arbitrary spaces: for every pair of subspaces $V_1, V_2$ of a finite-dimensional space $V$ \[\Dim{V_1\oplus V_2} + \Dim{V_1\cap V_2} = \Dim{V_1} + \Dim{V_2}\]

Now, as a bonus, if the decomposition is unique, then $V_0 = \obj{0_V}$. Suppose $V_0\neq \obj{0_V}$, so there is some non-zero $x\in V_0$. Since $V_0\subseteq V_1, V_2$ it is true that \[x = x\vert_{V_1} + 0_V\vert_{V_2} = 0_V\vert_{V_1} + x\vert_{V_2}\] implying that there are two distinct pairs of elements of $V_1$ and $V_2$ that sum to the same element $x$. Therefore the decomposition is non-unique at least for the non-zero elements of $V_0 \subseteq V_1\oplus V_2$. Thus non-null subspace intersection implies non-unique decomposition, so the necessary condition for the uniqueness of decomposition into elements of $V_1$ and $V_2$is the nullity of the common subspace $V_0$. Together with the prior proof of the converse to this result, $V_1\cap V_2 = \obj{0_V}$ is equivalent to $V_1\oplus V_2$ being a direct sum of subspaces $V$.

Let $V$ be a finite-dimensional space and $W$ its subspace. It is true that any basis of $W$ can be extended to a basis of $V$. However, let's construct a basis of $V$ such that its subset is a basis of $W$. Pick any basis in $V$, for example, $\brac{v_i}_{i=1}^n$. If $W$ is a null-space, then trivially the empty subset of $\brac{v_i}$ is its basis. Suppose there is a non-zero element $w_1\in W$. Then $w_1\in V$ and thus $w_1$ is a linear combination of $\brac{v_i}$ and a non-trivial one, since $w_1\neq 0_V$. Using the ``substitution'' result, one gets another basis $\brac{v'_i}_{i=1}^n$ in $V$ which has $w_1$ instead of one of the original $v_i$'s, with non-zero weight in $w_1$'s representation.

Suppose $\brac{w_i}_{i=1}^{m-1}$ is a linearly independent subset of a basis $\brac{v_i}_{i=1}^n$ with elements in $W$. Analogously to the ``extension'' result, if $\spn{\brac{w_i}_{i=1}^{m-1}} = W$ then the new basis of $V$ has the required property and there is nothing more to be done. If, however, $\brac{w_i}_{i=1}^{m-1}$ does not span $W$, then there must be $w_m\in W$ and not in that span. Therefore $\brac{w_i}_{i=1}^m$ is linearly independent and has a strictly larger span than $\brac{w_i}_{i=1}^{m-1}$. Now, since $w_m\in V$, it can be represented as a linear combination of the current basis of $V$. Note, that there must be a $v_i$, not in $\brac{w_i}_{i=1}^{m-1}$ with a non-zero coefficient in this representation. By the ``substitution'' result this $w_m$ can be switched with this $v_i$ to get a linearly independent $\brac{v'_i}_{i=1}^n$ with the same span as $\brac{v_i}_{i=1}^n$. Therefore $\brac{v'_i}_{i=1}^n$ is yet another basis in $V$, this time with a subset $\brac{w_i}_{i=1}^m\in W$.

Since $W$ is a subspace of finite-dimensional $V$, $\Dim{W}\leq \Dim{V}$ and the iterative procedure above is bound to terminate in finite number of steps. Indeed, the subset $\brac{w_i}_{i=1}^m$ of a basis $\brac{v_i}_{i=1}^n$ in $V$ spans a subspace with $\Dim{\spn{\brac{w_i}_{i=1}^m}} = m$, while also being a linearly independent set of elements of $W$ implying that $m\leq \Dim{W}$ always. Note that $\spn{\brac{w_i}_{i=1}^m} = W$ means that $\brac{w_i}_{i=1}^m$ is a basis of $W$ which is in turn equivalent to $\Dim{W}=m$. This described procedure is identical to the ``extension'' algorithm.

Summarize the relationships between direct sums, extension algorithm, nullity of the common subspace (probably should mention why it cam be called ``the common subspace'' and so on).

So a sum of subspaces of some finite dimensional space is a direct sum if and only if the common subspace of these two is the null-space. 

Note that the similarly named Wikipedia article states that linear combinations involve only finite number of vector by definition. Beside linear combinations there are conical combinations defined as linear combinations of elements of a vector spave $V$ over the field of real numbers $\mathbb{R}$ with non-negative coefficients only. The set of all possible conical combinations is known as the conical hull (by definition $0_V$ is a member of a conical hull).

% subsection basis_extension_and_sums_of_spaces (end)

% section vector_spaces (end)

\section{Maps between spaces} % (fold)
\label{sec:maps_between_spaces}
Let $A:V\to W$ be a map between two vector spaces. The map $A$ is a homomorphism if and only if it preserves linear combinations: for any $x, y\in V$ and $\lambda \in \mathbb{C}$ \[A\brac{x + \lambda y} = A\brac{x} + \lambda A\brac{y}\] Such maps are also known as linear maps.

Let $A:V\to W$ be a homomorphism. Then, since $A$ maps $\lambda x \in V$ to $\lambda A\brac{x} \in W$ for any $x \in V$ and $\lambda \in \mathbb{C}$, it is true that $A\brac{0_V} = A\brac{0 x} = 0 A\brac{x} = 0_W$, where $0_V$ and $0_W$ are the zero elements of the spaces $V$ and $W$ respectively.

Let $A:V\to W$ be a linear map between spaces $V$ and $W$. Suppose $\brac{v_k}_{k=1}^m \in V$ is a collection of vectors (elements) and let $\brac{\alpha_k}_{k=1}^m \in \mathbb{C}$ be a set coefficients such that $\sum_k \alpha_k v_k = 0_V$. Then, since $A$ maps $0_V$ to $0_W$, one can conclude from the linearity of $A$, that this set of coefficients must also satisfy \[A\brac{\sum_{k=1}^m \alpha_k v_k} = \sum_{k=1}^m \alpha A\brac{v_k} = 0_W\] Thus the set of coefficients which nullify a linear combination of some vectors in the domain of $A$ is a subset of solutions, which bring the linear combination of their images to zero. Therefore any homomorphism preserves linear dependency.

The contrapositive of the last statement gives the following corollary: whenever a collection of vectors from the domain $V$ is such that their images by $A$ in $W$ are linearly independent, then the collection is linearly independent as well.

A map $A:V\to W$ is injective if the equality of images by $A$ implies the equality of the corresponding pre-images, i.e. $A\brac{x}=A\brac{y}$ implies $x = y$ for any $x, y \in V$.

Let $A:V \to W$ be an injective homomorphism. Let $\brac{v_k}_{k=1}^m \in V$ and consider $\brac{\lambda_k}_{k=1}^m \in \mathbb{C}$ such that $\sum_k \lambda_k A\brac{v_k} = 0_W$. Then by linearity \[0_W = \sum_{k=1}^m \lambda_k A\brac{v_k} = A\brac{\sum_{k=1}^m \lambda_k v_k}\] As $A$ is injective and $0_V$ is mapped to $0_W$, the fact $A\brac{x} = 0_W$ implies that $x$ must be $0_V$. Hence it cannot be otherwise but $\sum_{k=1}^m \lambda_k v_k = 0_V$. This means that, whenever $A$ is injective, the set of coefficients validating any linear relationship of images of $v_k$ by $A$ also produces a linear relationship among $v_k$ themselves. Therefore if a set of vectors in the domain is linearly independent, then their images by the linear map $A$ stay linearly independent if $A$ is injective.

A corollary of the previous result is that a necessary condition for injectivity of a homomorphism is that the dimensionality of its domain be not greater than its co-domain. Indeed, if $A:V\to W$ be an injective linear map, then by the previous result any linearly independent subset of $V$ is mapped to a linearly independent subset of $W$, thus imposing a lower bound on the dimensionality of $W$. Therefore $\Dim{V} \leq \Dim{W}$.

Let $A:V\to W$ be a linear map. Define $\Ker{A}\subseteq V$ as the set of all elements $x \in V$ such that $A\brac{x} = 0_W$, all elements that are mapped by $A$ to $0_W$. Obviously $0_V\in \Ker{A}$. Let $x_1, x_2 \in \Ker{A}$ and $\alpha_1, \alpha_2\in \mathbb{C}$. Because $A$ is a linear map, the value of $A$ at $\alpha_1 x_1 + \alpha_2 x_2$ is\[A\brac{\alpha_1 x_1}+A\brac{\alpha_2 x_2} = \alpha_1 A\brac{x_1} + \alpha_2 A\brac{x_2} =  \alpha_1 0_W + \alpha_2 0_W = 0_W\] Hence $\Ker{A}$ is closed under pairwise linear combinations, and so $\Ker{A}$ is a subspace of $V$.

Let $A:V\to W$ be a linear map that is also an injective mapping. Then if $x\in V$ is such that $A\brac{x}=0_W$, then from $A\brac{0_V} = 0_W$ follows that $x = 0_V$, which means that $\Ker{A} = \obj{0_V}$. Suppose conversely, that a homomorphism $A$ is such, that $\Ker{A} = \obj{0_V}$. If $x_1, x_2\in V$ are such, that $A\brac{x_1} = A\brac{x_2}$, then the linearity of $A$ implies that $A\brac{x_1-x_2}=0_W$. But then $x_1-x_2 \in \Ker{A}$, whence $x_1-x_2=0_V$. So whenever $\Ker{A}=\obj{0_V}$, $A\brac{x_1} = A\brac{x_2}$ implies $x_1 = x_2$. Therefore such $A$ is also an injective map.

Define $\im{A}$ as the set $A\brac{V} = \obj{A\brac{x}\vert \forall{x\in V} }$. Since $A$ maps $V$ to $W$, obviously $\im{A} \subseteq W$. Let $y_1, y_2\in \im{A}$, then there must exist (by the definition of $\im{A}$) such $x_1, x_2\in V$, that $y_1 = A\brac{x_1}$ and $y_2 = A\brac{x_2}$. Then first of all note that since $V$ is a vector space, $x_1 + x_2\in V$ and as such is mapped by $A$ to \[A\brac{x_1+x_2} = A\brac{x_1} + A\brac{x_2} = y_1 + y_2\] which means that there is $x\in V$, such that $A\brac{x} = y_1+y_2$. Therefore $y_1+y_2\in \im{A}$. Now pick any $y\in \im{A}$ and any $\lambda\in \mathbb{C}$. Then for such $y$ there is $x\in A$ such, that $A\brac{x} = y$. Therefore at least $\lambda x$ is mapped by $A$ to $\lambda y$, due to linearity of $A$. Now since $V$ is a vector space $\lambda x \in V$, and so its image $\lambda y$ must be in $\im{A}$. Hence $\im{A}$ is a subspace of a linear space $W$.

Let $V$ be a finite-dimensional space with $\Dim{V} = n$ and $\brac{v_i}_{i=1}^n$ be its arbitrary basis. Let $A:V\to W$ be a homomorphism. For $y \in \im{A}$, then there is $x \in V$ such that $A\brac{x}=y$. Then\[y = A\brac{x} = A\brac{\sum_{i=1}^n \lambda_i v_i} = \sum_{i=1}^n \lambda_i A\brac{v_i}\] because any $x\in V$ is a linear combination of the basis $\brac{v_i}_{i=1}^n$ in $V$. Thus $\brac{A\brac{v_i}}_{i=1}^n$, possibly linearly dependent, is a spanning set of $\im{A}$. As the basis set has been picked arbitrarily, it is indeed true, that $A$'s image space is spanned by a set of images of any basis in the domain $V$.

A final note is, that for any homomorphism $A:V\to W$ it is true, that $\Dim{\im{A}}\leq \Dim{V}$, because for any linearly independent subset of $\im{A}$ the necessarily existing subset of corresponding pre-images in $V$ is also linearly independent.

A map $A:V\to W$ is surjective if for any element in the co-domain there is an element in the domain mapped back to the former by $A$, which images by $A$ implies the equality of the corresponding pre-images, i.e. for any $y\in W$ there is at least one $x\in V$ such that $y = A\brac{x}$. Therefore a map $A$ is surjective if and only if $\im{A} = W$.

Suppose a homomorphism $A:V\to W$ is also a surjective map and $W$ is a finite dimensional space. Let $\brac{w_i}_{i=1}^p$ be any basis of $W$, where $p = \Dim{W}$. Then because $A$ is surjective, for every $w_i\in W$, there is such $v_i\in V$, that $A\brac{v_i} = w_i$. Consider any linear relationship between the pre-images: $\sum_{i=1}^p \lambda_i v_i = 0_V$ for some $\brac{\lambda_i}\in \mathbb{C}$. As $A$ is a linear map \[\sum_{i=1}^p \lambda_i w_i = \sum_{i=1}^p \lambda_i A\brac{v_i} = A\brac{\sum_{i=1}^p \lambda_i v_i} = A\brac{0_V} = 0_W\] whence each $\lambda_i$ must be zero, since $\brac{w_i}_{i=1}^p$ is linearly independent. Therefore there exists a linearly independent set in $V$ with at least $p$ elements, which necessarily implies that $\Dim{W}\leq \Dim{V}$.

``Equivalence of having a linear inverse to having $\Ker{A} = \obj{0_V}$.'' Existence of an inverse requires a surjective map, which imposes a restriction on the dimensionality of the co-domain. For example, an injective map $A:V\to W$ is one-to-one from its domain to $\im{A}$, for which it is surjective by definition. However its image space may very well be a strict subspace of $W$. Take a linear map form a line to a line in a plane (all passing through the origin). Thus the following should be proven: ``Equivalence of having a linear inverse to having $\Ker{A} = \obj{0_V}$ and $\Dim{V} = \Dim{W}$.''

A result very specific to linear maps is that whenever the dimensionality of the space $W$ is the same as of $V$, a homomorphism $A:V\to W$ which is an injective map, is also a surjective map. Indeed, injectivity implies that any set of linearly independent elements $\brac{v_i} \in V$ is mapped by $A$ to a linearly independent set $\brac{A\brac{v_i}} \in W$. Thus a basis of $V$ is mapped to a set linearly independent images, which in turn constitute a basis of $W$, because $\Dim{V}=\Dim{W}$ by the premise. Let $\brac{v_i}$ be any basis of $V$. Then for any arbitrary $y \in W$, there exist $\brac{\lambda_i}\in \mathbb{C}$ such that \[y = \sum_i \lambda_i A\brac{v_i} = A\brac{\sum_i \lambda_i v_i}\] whence at least $x = \sum_i \lambda_i v_i \in V$ is mapped by $A$ back to this $y$. Therefore any injective homomorphism between vector spaces of equal dimension is also surjective.

A map is called one-to-one if and only if it is both an injective and a surjective map. Now if a linear map $A:V\to W$ is one-to-one, then injectivity implies that $\Dim{V}\leq \Dim{W}$ while surjectivity implies that $\Dim{W} \leq \Dim{V}$. Hence in this case it must be that $\Dim{V}=\Dim{W}$.

Finally suppose the finite-dimensional vector spaces are such, that $\Dim{V}=\Dim{W}$, and a linear map $A:V\to W$ is surjective. Suppose $\brac{w_i}$ is a basis of $W$ and $\brac{v_i}\in V$ are such that $A\brac{v_i}=w_i$, which must necessarily exist due to surjectivity of $A$. Since any basis is linearly independent and for any homomorphism the linear independence of images implies linear independence of pre-images, the set $\brac{v_i}\in V$ must be a linearly independent set with the number of elements equal to the dimensionality of $V$. Thus these pre-images constitute a basis of $V$. Now suppose $x_1, x_2 \in V$ are such, that $A\brac{x_1}=A\brac{x_2}$. But then there exist $\lambda_i^1, \lambda_i^2 \in \mathbb{C}$, such that\[x_1 = \sum_i \lambda_i^1 v_i \text{ and } x_2 = \sum_i \lambda_i^2 v_i \] which, together with the linearity of $A$, implies that \[0_W = A\brac{x_1-x_2} = A\brac{\sum_i \brac{\lambda_i^1 - \lambda_i^2} v_i} = \sum_i \brac{\lambda_i^1 - \lambda_i^2} A\brac{v_i} = \sum_i \brac{\lambda_i^1 - \lambda_i^2} w_i\] Hence such $\brac{\lambda_i^1 - \lambda_i^2}$ produce a linear relationship among the basis elements $\brac{w_i}\in W$, which can only be achieved with all-zero coefficients. Thus $\lambda_i^1 = \lambda_i^2$, whence $x_1 = x_2$. Therefore, very surprisingly, surjectivity of a homomorphism $A:V\to W$ implies its injectivity, whenever $V$ and $W$ are vector spaces of equal dimensionality.

To sum up, given two finitely-dimensional spaces $V$ and $W$ with $\Dim{V}=\Dim{W}$, a homomorphism $A:V\to W$ is injective if and only if it is surjective. Therefore whenever $\Dim{V}=\Dim{W}$, $\Ker{A} = \obj{0_V}$ is equivalent to $\im{A} = W$. And finally, $\Ker{A} = \obj{0_V}$ is equivalent to $A$ being one-to-one (bijective) which is in turn equivalent to $\im{A} = W$.

Whenever a map has an inverse, it has to be injective and surjective, since otherwise it would contradict the fact that the inverse is a map. Therefore suppose a homomorphism $A:V\to W$ is one-to-one. Then there exists a map $A^{-1}:W\to V$ such that $A\brac{A^{-1}\brac{y}}=y$ and $A^{-1}\brac{A\brac{x}}=x$ for all $x\in V$ and $y\in W$. Thus for any $y_1, y_2\in W$ it is true that \[y_1 + y_2 = A\brac{A^{-1}\brac{y_1}} + A\brac{A^{-1}\brac{y_2}} = A\brac{A^{-1}\brac{y_1} + A^{-1}\brac{y_2}}\] because $A$ is linear. But then \[ A^{-1}\brac{y_1 + y_2} = A^{-1}\brac{A\brac{A^{-1}\brac{y_1} + A^{-1}\brac{y_2}}} = A^{-1}\brac{y_1} + A^{-1}\brac{y_2}\] Now if $y\in W$ and $\lambda \in \mathbb{C}$, then the linearity of $A$ implies that \[A^{-1}\brac{\lambda y} = A^{-1}\brac{\lambda A\brac{A^{-1}\brac{y}}} = A^{-1}\brac{A\brac{\lambda A^{-1}\brac{y}}} = \lambda A^{-1}\brac{y}\] Hence the inverse map must be linear.

Let $A:V\to W$ be a linear map between finite-dimensional spaces. Pick any basis $\brac{v_i}_{i=1}^m$ in $\Ker{A}$, where $m = \Dim{\Ker{A}}$. Being a linearly independent subset of $V$, this basis can be extended to a basis of $V$. Let $\brac{v_{m+i}}_{i=1}^q$ be the expansion and $V_1$ its span. If the set $\brac{v_i}_{i=1}^m$ was already a basis of $V$, then the extension is an empty set and $V_1 = \obj{0_V}$.

First, the restriction of $A$ to $V_1$, $A\vert_{V_1}:V_1\to W$, is a linear map. Since $V_1$ is a subspace of $V$, for any $x_1, x_2\in V_1$, the linearity of $A$ implies that\[A\vert_{V_1}\brac{x_1+x_2} = A\brac{x_1+x_2} = A\brac{x_1}+A\brac{x_2} = A\vert_{V_1}\brac{x_1}+A\vert_{V_1}\brac{x_2}\] and $A\vert_{V_1}\brac{\lambda x} = A\brac{\lambda x} = \lambda A\brac{x} = \lambda A\vert_{V_1}\brac{x}$ for any $\lambda \in \mathbb{C}$ and $x \in V_1$.

Secondly, the map $A\vert_{V_1}:V_1\to W$ is injective. Indeed, let $x_1, x_2\in V_1$ be such that $A\vert_{V_1}\brac{x_1}=A\vert_{V_1}\brac{x_2}$. Then $A\brac{x_1}=A\brac{x_2}$ and by linearity of the map $A\brac{x_1-x_2} = 0_W$, which implies that the difference $x_1-x_2\in \Ker{A}$. At the same time, however, the fact that $V_1$ is a vector space implies that $x_1-x_2\in V_1$ as well. But the extension procedure guarantees that the only common element is the zero, so it must be true that $x_1 = x_2$, which proves injectivity of $A\vert_{V_1}$.

Thirdly, for this restriction it is true that $\im{A\vert_{V_1}} = \im{A}$. Indeed, for any $x\in V_1\subseteq V$ \[A\vert_{V_1}\brac{x} = A\brac{x}\in \im{A}\] implies that $\im{A\vert_{V_1}} \subseteq \im{A}$. Conversely, for any $y\in \im{A}$ there must be $x\in V$, which $A$ maps back to $y$. Now if $y = 0_W$, then $y\in \im{A\vert_{V_1}}$, since the latter is a vector space. If however $y\neq 0_W$, then $x\not\in\Ker{A}$, which means that it must be a member of $V_1$, as the latter complements $\Ker{A}$ to the entire space $V$. Hence $A\brac{x}=A\vert_{V_1}\brac{x} = y \in \im{A\vert_{V_1}}$. Therefore the restriction is an injective map $A\vert:V_1\to \im{A}$. Since any map $A$ is surjective with respect to the co-domain $\im{A}$ -- the set of all possible images of $A$, the restriction is a one-to-one map between $V_1$ and $\im{A\vert_{V_1}}$.

Finally, in general for any homomorphism $\Dim{\im{A\vert_{V_1}}}\leq \Dim{V_1}$, while for an injective one $\Dim{V_1}\leq \Dim{\im{A\vert_{V_1}}}$. Indeed, the first follows form the definition of the image set and linearity (linear independence in images implies linear independence in pre-images for every linear map) and the last from injectvity of $A\vert_{V_1}$ (linear independence is preserved by any injective map). Thus it must be that $\Dim{\im{A}} = \Dim{\im{A\vert_{V_1}}} = \Dim{V_1}$, whence \[\Dim{\Ker{A}} + \Dim{\im{A}} = \Dim{\Ker{A}} + \Dim{V_1} = \Dim{V}\] Therefore for any map $A:V\to W$ there is the balance of dimensionality (note that when $W = V$, there are linear maps, for which the kernel and the image coincide).

Proof of existence of a linear map with the given kernel and image as subsets of $V$ and $W$. Consider two finite-dimensional vector spaces $V$ and $W$. Suppose there are subspaces $V_1$ and $W_2$ of those two, which satisfy the dimensionality balance:\[\Dim{V_1}+\Dim{W_1} = \Dim{V}\]The following argument provides an answer to a question, which is in some sense inverse to the result above.
Pick a basis $\brac{v_i}_{i=1}^n$ in $V$ such that $\brac{v_i}_{i=1}^p$ is a basis of $V_1$, where $n = \Dim{V}$ and $p = \Dim{V_1}$ (this is possible due to the linear combination ``substitution'' result above), and a basis in $W_1$, $\brac{w_i}_{i=1}^{n-p}$, since the $\Dim{W_1} = \Dim{V}-\Dim{V_1} = n-p$. Denote by $V_2$ the subspace spanned by $\brac{v_i}_{i=p+1}^n$. Let $A:V\to W$ be a linear map that acts upon the chosen basis vectors of $V$ in the following manner: $A\brac{v_i} = 0_W$ for $1\leq i \leq p$ and $A\brac{v_i} = w_{i-p}$ for $p+1\leq i \leq n$.

Let $x \in V$ be represented as $\sum_{i=1}^n \lambda_i v_i$ and be such that $A\brac{x} = 0_W$. From the linearity of $A$ and given its effect on the basis of $V$ one gets $A\brac{x} = \sum_{i=p+1}^n \lambda_i w_i = 0_W$, which can only be true when $\lambda_i = 0$, since $\brac{w_i}$ is a basis. Therefore if $A\brac{x} = 0_W$ then $x = \sum_{i=1}^p \lambda_i v_i \in V_1$. Hence $\Ker{A} \subseteq V_1$. On the other hand, for any $x\in V_1$, \[A\brac{x} = A\brac{\sum_{i=1}^p \lambda_i v_i} = \sum_{i=1}^p \lambda_i A\brac{v_i} = 0_W\] since $\brac{v_i}_{i=1}^p$ is a basis of $V_1$. Thus $V_1 \subseteq \Ker{A}$.

The restriction of $A$ to the subspace $V_2$, $A\vert_{V_2}:V_2 \to W_1$, is injective. Indeed if $x_1, x_2\in V_2$ are such that $A\brac{x_1}=A\brac{x_2}$, then $A\brac{x_1-x_2}=0_W$, implying that $x_1-x_2\in V_1$. But $V_2$ and $V_1$ are complementary subspaces of $V$ in a sense that $V$ is the direct sum of them. Therefore $x_1-x_2=0_V$, implying the stated property. This, and $\Dim{V_2} = n-p = \Dim{W_1}$ are sufficient to infer that $A\vert_{V_2}$ is one-to-one, and therefore a surjective map.

Now, if $y\in \im{A}$, then there is $x\in V$, such that $A\brac{x}=y$. Then $y\in W_1$, since \[A\brac{x} = A\brac{\sum_{i=1}^n \lambda_i v_i} = \sum_{i=1}^n \lambda_i A\brac{v_i} = \sum_{i=p+1}^n \lambda_i w_i\] where $x = \sum_{i=1}^n \lambda_i v_i\in V$. Hence $\im{A}\subseteq W_1$. Conversely, if $y\in W_1$, then there must be $x\in V_2$ such that $A\brac{x} = A\vert_{V_2}\brac{x} = y$, which implies that $y\in \im{A}$. Alternatively, any $x=\sum_{i=1}^n \lambda_i v_i$, such that $\sum_{i=p+1}^n \lambda_i w_i = y$ is a pre-image of any $y\in W_1$ and therefore $W_1\subseteq \im{A}$.

Therefore for such $V_1$ and $W_1$ with balanced dimensionality, there is a linear map $A:V\to W$ suc that $\im{A} = W_1$ and $\Ker{A}=V_1$. 

Composition of maps $A:V\to L$ and $B:L\to W$ is the map $B\circ A:V \to W$ is defined as $\brac{B\circ A}\brac{x} = B\brac{A\brac{x}}$ for any $x\in V$. The composition is itself a homomorphism: indeed $B\brac{A\brac{\lambda_1 x_1+\lambda_1 x_2}} = B\brac{\lambda_1 A\brac{x_1}+\lambda_1 A\brac{x_2}} = \lambda_1 B\brac{A\brac{x_1}}+\lambda_1 B\brac{A\brac{x_2}}$, implying that the linearity of $B\circ A$.

Let $A:V\to E$ and $B:E\to W$ be linear maps between finite-dimensional vector spaces, and consider the kernel of their composition. First of all for any $x\in \Ker{A}$ it is true that $ \brac{B\circ A}\brac{x} = B\brac{A\brac{x}} = B\brac{0_E} = 0_W$, and so $x\in \Ker{\brac{B\circ A}}$. Since $\Ker{A}$ is a subspace any its basis $\brac{x_i}_{i=1}^p$ can be extended to a basis of $\Ker{\brac{B\circ A}}$, where $p = \Dim{\Ker{A}}$. If the extension turns out to be the empty set, then, obviously, \[\Dim{\Ker{\brac{B\circ A}}} = \Dim{\Ker{A}} \leq \Dim{\Ker{A}} + \Dim{\Ker{B}}\] Consider the case when the extension $\brac{x_i}_{i=p+1}^q$ is non-empty, where $q = \Dim{\brac{B\circ A}}$. Let $v_1, v_2\in \spn{\brac{x_i}_{i=p+1}^q}$ be such that $A\brac{v_1} = A\brac{v_2}$. By the linearity $A\brac{v_1-v_2} = 0_E$, implying that $v_1-v_2\in \Ker{A}$. But the extension algorithm ensures the result $\brac{x_i}_{i=1}^q$ is linearly independent and that the spans of $\brac{x_i}_{i=1}^p$ and $\brac{x_i}_{i=p+1}^q$ have only the zero element of $V$ in common. Therefore $v_1-v_2 = 0_V$ and thus for such $v_1, v_2$, $A\brac{v_1} = A\brac{v_2}$ implies $v_1=v_2$. Therefore on the span of $\brac{x_i}_{i=p+1}^q$ the map $A$ (more precisely, its restriction) is one-to-one. Therefore the set $\brac{A\brac{x_i}}_{i=p+1}^q$ is linearly independent (indeed a solution to $\sum_{i=p+1}^q \lambda_i A\brac{x_i} = 0_E$ is also a solution to $\sum_{i=p+1}^q \lambda_i x_i = 0_V$ because $0_V$ is the only element mapped to $0_E$). Now note, that the set $\brac{A\brac{x_i}}_{i=p+1}^q$ also belongs to $\Ker{B}$ because $\brac{x_i}_{i=1}^q$ are such that $\brac{B\circ A}\brac{x_i} = 0_W$. Therefore there is a linearly independent set with $q-p$ elements in $\Ker{B}$, implying that $q-p\leq \Dim{\Ker{B}}$. Hence \[\Dim{\Ker{B\circ A}} = \brac{q-p}+p\leq \Dim{\Ker{B}} + \Dim{\Ker{A}}\]

Vector spaces are said to be isomorphic if and only if there is a one-to-one homomorphism between them.

Consider an arbitrary finite-dimensional vector space $V$ and suppose that $\Dim{V} = n$. The real vector space $\mathbb{R}^n$ has dimensionality $n$, since it it at least the set $\brac{e_i}_{i=1}^n$ is trivially linearly independent and spans the entire space, where $\brac{e_i}_j = 0$, unless $j=i$, in which case $\brac{e_i}_i=1$. In complete analogy to the ``inverse dimensionality balance result'' pick a basis $\brac{v_i}_{i=1}^n$ in $V$ and let $\brac{e_i}_{i=1}^n$ be the standard basis in the space $\mathbb{R}^n$. Define $A\brac{x} = \sum_{i=1}^n \lambda_i e_i$ for any $x = \sum_{i=1}^n \lambda_i v_i \in V$. The map $A$ is injective, since being otherwise would imply linear dependence among the elements of $\brac{e_i}$. Now any $y\in \mathbb{R}^n$ can be represented as $y = \sum_{i=1}^n y_i e_i$, which implies that $x=\sum_{i=1}^n y_i v_i$ is mapped back to $y$ by the map $A$. Thus $A:V\to \mathbb{R}^n$ is one-to-one. The linearity of $A$ is straightforward.

Therefore any $n$-dimensional vector space is isomorphic to the real vector space $\mathbb{R}^n$. Suppose $\phi:V\to \mathbb{R}^n$ and $\psi:W\to \mathbb{R}^n$ are isomorphisms identifying $V, W$ with $\mathbb{R}^n$. Consider a map $\theta:V\to W$, defined as $\theta = \brac{\psi^{-1}\circ \phi}$. For any $v_1, v_2\in V$ such that $\theta\brac{v_1} = \theta\brac{v_2}$, the nature of $\psi$ implies that $\phi\brac{v_1} = \phi\brac{v_2}$, .

$\theta\brac{v} = \brac{\psi^{-1}\circ \phi}\brac{v} = \psi^{-1}\brac{\phi\brac{x}}$
two finite-dimensional spaces with the same dimensionality are isomorphic, and thus can be identified.

The definition of the map's rank

%Rank AB \leq Rank A, Rank AB \leq Rank B

%Rank A + Rank B - n \leq Rank AB

the matrix of the linear map in some basis

existence of a matrix representation for every homomorphism in any basis of V

existence of a homomorphism for any given matrix in any given basis

the equality of the map's matrix's rank to the map's rank itself

the relation of a homomorphism's matrix representations in different bases

matrix representation of linear combination of homomorphisms

the determinant of a linear map (invariant under the selection of basis)

The characteristic polynomial of a homomorphism (invariant under basis transformations)

% section maps_between_spaces (end)

\section{Other stuff} % (fold)
\label{sec:unimportant}

$y_t = \mu_t + \epsilon_t$

$\epsilon_t = V_t^{\frac{1}{2}} \xi_t$

$\xi_t \in \mathbb{R}^d$ -- $d\times 1$ vector, and $\xi_t \sim G_d\brac{0; R}$

$\mu_t = \mathbb{E}\brac{y_t \vert \mathbb{F}_{t-1}}$

$V_t = \mathbb{E}\brac{\epsilon_t \vert \mathbb{F}_{t-1}}$


Let $f$ be a strictly increasing function continuous on the interval $\cintc{ a; b}$, with $a<b$. Let $\alpha = f\brac{a}$ and $\beta = f\brac{b}$. The strict monotonicity implies that $\alpha < \beta$. Consider some $\gamma \in \brac{\alpha, \beta}$. Let $\Gamma^- = \obj{x\in \cintc{a ;b}\vert f\brac{x}\leq \gamma}$ and note that $a\in \Gamma^-$. Now for any $x\in \Gamma^-$ it is true that $f\brac{x}\leq\gamma<\beta = f\brac{b}$ and as $f$ is strictly increasing it cannot be otherwise but $x<b$. Therefore the set $\Gamma^-\subseteq \mathbb{R}$ has a finite upper bound and $c = \sup \Gamma^- \leq b$ is finite. Now if $c=b$ then $f\brac{c} = \beta$ and at the same time $f\brac{x}\leq \gamma$ for all $x\in \Gamma^-$. Since $f$ is left continuous at the point $b$ and strictly increasing, it is true that for any $\epsilon>0$ there is $\delta > 0$ such that $f\brac{c} - \epsilon < f\brac{x} < f\brac{c}$ for any $x$ with $c-\delta<x<c$. 




% section unimportant (end)

\end{document}